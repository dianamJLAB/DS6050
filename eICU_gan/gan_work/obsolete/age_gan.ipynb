{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "#plt.switch_backend('agg')\n",
    "from tensorflow.keras import regularizers\n",
    "#from tensorflow.keras.utils import multi_gpu_model\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pandas.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = \"data/eICU_age.npy\" #hapt format input file\n",
    "latent_size = 600 #size of noise input\n",
    "alph = 0.01 #alpha value for LeakyReLU\n",
    "g_learn = 1e-3 #generator learning rate\n",
    "d_learn = 1e-5 #discriminator learning rate\n",
    "epochs = 50\n",
    "batch_size = 280 # just making it easier for length of 2520\n",
    "ag_size = 2520 #number of  ages to return to show a meaningful distribution\n",
    "save_that = 25 #epoch interval for saving outputs\n",
    "\n",
    "mean_age = 38.5 # mean US age\n",
    "stdv_age = 3.52 # standard deviation of US age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('inpt: ', inpt)\n",
    "print('latent_size: ', latent_size)\n",
    "print('alph: ', alph)\n",
    "print('g_learn: ', g_learn)\n",
    "print('d_learn: ', d_learn)\n",
    "print('epochs: ', epochs)\n",
    "print('batch_size: ', batch_size)\n",
    "print('ag_size: ', ag_size)\n",
    "print('save_that: ', save_that)\n",
    "print('mean_age: ', mean_age)\n",
    "print('stdv_age: ', stdv_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read input\n",
    "ages_np = np.load(inpt)\n",
    "print('length: ', len(ages_np))\n",
    "print(ages_np[0:5])\n",
    "ages_np_ary = np.asarray(ages_np.flatten().tolist()).flatten()\n",
    "print('length: ', len(ages_np_ary))\n",
    "print(ages_np_ary[0:5])\n",
    "\n",
    "df_ages = pd.DataFrame(ages_np_ary, columns=['age'])\n",
    "print(df_ages.shape)\n",
    "df_ages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ages_describe = df_ages.copy(deep=True)\n",
    "df_ages_describe['Y'] = 'Y'\n",
    "df_ages_describe.groupby('age').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.histplot(ages_np_ary, bins=90, kde=True)\n",
    "\n",
    "US_population_ages = np.random.normal(loc=mean_age, scale=stdv_age, size=latent_size) \n",
    "sns.histplot(US_population_ages, bins=90, kde=True, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory used by models\n",
    "K.clear_session()\n",
    "\n",
    "# Make generator - simple sequential network\n",
    "generator = Sequential()\n",
    "generator.add(Dense(1024, input_shape=(latent_size,), kernel_regularizer=regularizers.l2(0.0001)))\n",
    "generator.add(LeakyReLU(alpha=alph))\n",
    "generator.add(Dense(512, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "generator.add(LeakyReLU(alpha=alph))\n",
    "generator.add(Dense(1, activation = 'relu'))\n",
    "\n",
    "print('==' * 30)\n",
    "print('Generator:')\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make discriminator\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Dense(256, input_shape=(1,), kernel_regularizer=regularizers.l2(0.0001)))\n",
    "discriminator.add(LeakyReLU(alpha=alph))\n",
    "discriminator.add(Dense(128, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "discriminator.add(LeakyReLU(alpha=alph))\n",
    "discriminator.add(Dense(1, activation = 'sigmoid'))\n",
    "discriminator.compile(optimizer=Adam(learning_rate=d_learn), loss='binary_crossentropy')\n",
    "#Set discriminator to non-trainable\n",
    "discriminator.trainable = False\n",
    "\n",
    "print('==' * 30)\n",
    "print('Discriminator:')\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine G and D to make the GAN\n",
    "gan = Sequential()\n",
    "gan.add(generator)\n",
    "gan.add(discriminator)\n",
    "gan.compile(optimizer=Adam(learning_rate=g_learn), loss='binary_crossentropy')\n",
    "\n",
    "print('==' * 30)\n",
    "print('GAN:')\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create discrimnator targets: indicating Real ==1 and Fake == 0\n",
    "y_real, y_fake = np.ones([batch_size, 1]), np.zeros([batch_size, 1])\n",
    "X_real = ages_np_ary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array to save loss tuples to\n",
    "losses = []\n",
    "\n",
    "# number of batches\n",
    "batches = len(X_real)//batch_size\n",
    "\n",
    "for e in range(0,epochs):\n",
    "    print('Started Epoch: ', e)\n",
    "    \n",
    "    for b in range(batches):\n",
    "        # TODO: get the current batch - randomly select a batch-size number of samples from the data here\n",
    "        X_batch_real = X_real[b*batch_size:(b+1)*batch_size] \n",
    "\n",
    "        # randomly selected indices for the samples for the batch\n",
    "        indices = np.random.randint(0, len(X_real), batch_size)\n",
    "\n",
    "        X_batch_real = X_real[indices]\n",
    "        \n",
    "\n",
    "        # create the noisy data to be sent to the generator of size: latent_size\n",
    "        latent_samples = np.random.normal(loc=mean_age, scale=stdv_age, size=(batch_size, latent_size)) \n",
    "        \n",
    "        \n",
    "        # create the generated data from the latent samples\n",
    "        X_batch_fake = generator.predict_on_batch(latent_samples).flatten()\n",
    "\n",
    "        \n",
    "\n",
    "        # train the discriminator on both the real (y == 1) and the fake (y == 0) data\n",
    "        # set the discriminator to trainable\n",
    "        # TODO: see whether this is actually setting the discriminator is getting set to True and False\n",
    "        discriminator.trainable = True\n",
    "        # get the loss for both discriminating real and discriminating fake\n",
    "        d_loss_real = discriminator.train_on_batch(X_batch_real, y_real)\n",
    "        d_loss_fake = discriminator.train_on_batch(X_batch_fake, y_fake)\n",
    "        # total discriminator loss\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # make discriminator non-trainable and train gan to get the gan loss\n",
    "        # TODO: see whether this is actually setting the discriminator is getting set to True and False\n",
    "        discriminator.trainable = False\n",
    "        g_loss = gan.train_on_batch(latent_samples, y_real)\n",
    "\n",
    "    losses.append((d_loss, d_loss_real, d_loss_fake, g_loss))\n",
    "    print(\"Epoch:\\t%d/%d Discriminator loss: %6.4f Generator loss: %6.4f\"%(e+1, epochs, d_loss, g_loss))\n",
    "    if e%save_that == 0 or e == (epochs-1):\n",
    "\n",
    "        # TO DO: Save the Model\n",
    "\n",
    "        print(\"=========\" * 30)\n",
    "        print(\"Epoch:\\t%d/%d Real, Latent, and Generated Ages:\"%(e+1, epochs))\n",
    "        print(\"First 5 Real Ages: \", X_batch_real[0:5])\n",
    "        print(\"Noisy Data Sent to Generator: \", latent_samples[1,1:5])\n",
    "        print(\"First 5 Generated Ages: \", X_batch_fake[0:5])\n",
    "\n",
    "        # create the generated ages\n",
    "        # create noise to feed to generator\n",
    "        latent_samples = np.random.normal(loc=mean_age, scale=stdv_age, size=(ag_size, latent_size))\n",
    "        # generate some data\n",
    "        generated_ages = generator.predict(latent_samples)\n",
    "        # round any elements to the nearest integer as we know we want age integers\n",
    "        generated_ages = np.rint(generated_ages)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.histplot(ages_np_ary, bins=90, kde=True)\n",
    "        sns.histplot(generated_ages.flatten(), bins=90, kde=True, color='orange', alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(np.array([losses]).T[0], label='Discriminator', color='crimson')\n",
    "        plt.legend()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(np.array([losses]).T[1], label='Discriminator: Real', color='crimson', alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(np.array([losses]).T[2], label='Discriminator: Fake', color='crimson', alpha=0.7)\n",
    "        plt.legend()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(np.array([losses]).T[3], label='Generator', color='xkcd:grass green')\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34b722ad9db4b3bab2fc22ea7ad4e4dafa3a71a0a38143cc4f1aba484bf1fe4b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
