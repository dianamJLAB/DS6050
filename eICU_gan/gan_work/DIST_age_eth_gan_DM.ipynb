{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "c_5e8_VcuC7r"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-td63pu6r because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, LeakyReLU, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mName: tensorflow\n",
      "Version: 2.6.2\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: /home/dianam/.local/lib/python3.8/site-packages\n",
      "Requires: absl-py, astunparse, clang, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, numpy, opt-einsum, protobuf, six, tensorboard, tensorflow-estimator, termcolor, typing-extensions, wheel, wrapt\n",
      "Required-by: gpflow\n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python\n",
      "3.8.6 | packaged by conda-forge | (default, Dec 26 2020, 05:05:16) \n",
      "[GCC 9.3.0]\n",
      "sys.version_info(major=3, minor=8, micro=6, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc5PSNlfuJ3w",
    "outputId": "465f558f-ba2c-49df-d868-911343f86e5b"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0qzXBK53uC7t"
   },
   "outputs": [],
   "source": [
    "#https://keras.io/examples/generative/conditional_gan/#interpolating-between-classes-with-the-trained-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "K0Kj1O1juC7u"
   },
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, noise_dim=50, \n",
    "                 data_shape=1,\n",
    "                 num_classes=3, \n",
    "                 d_learning_rate=1e-5, \n",
    "                 g_learning_rate=1e-6, \n",
    "                 batch_size=64, \n",
    "                 start_epoch=0,\n",
    "                 verbose = False, \n",
    "                 distribute = False):\n",
    "\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.data_shape = data_shape # output shape of the generator and goes to discriminator\n",
    "        self.num_classes = num_classes\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(d_learning_rate)\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(g_learning_rate)\n",
    "        self.batch_size = batch_size\n",
    "        self.start_epoch = start_epoch\n",
    "        self.verbose = verbose\n",
    "        self.distribute = distribute\n",
    "\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "        # add number of class labels to the input channels for generator\n",
    "        self.g_dim = self.noise_dim + self.num_classes\n",
    "\n",
    "        # add the number of class labels to the input to the discriminator\n",
    "        self.d_dim = self.data_shape + self.num_classes\n",
    "\n",
    "        if (self.verbose):\n",
    "            print(\"Generator input dim: \", self.g_dim)\n",
    "            print(\"Dicrimination input dim: \", self.d_dim)\n",
    "\n",
    "\n",
    "        # build generator and discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "    \n",
    "    def build_generator(self):\n",
    "        \"Build the generator model\"\n",
    "        inputs = Input(shape=(self.g_dim,))\n",
    "        hidden = Dense(64)(inputs) # 128\n",
    "        hidden = LeakyReLU(alpha=0.2)(hidden)\n",
    "        hidden = BatchNormalization()(hidden)\n",
    "        hidden = Dense(64)(hidden)\n",
    "        hidden = LeakyReLU(alpha=0.2)(hidden)\n",
    "\n",
    "        output = Dense(self.data_shape, activation=\"sigmoid\")(hidden)\n",
    "        #output = Dense(self.data_shape, activation=\"linear\")(hidden)\n",
    "\n",
    "        if self.distribute:\n",
    "          mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() #tf.distribute.MirroredStrategy()\n",
    "          with mirrored_strategy.scope():\n",
    "\n",
    "            generator = Model(inputs=inputs, outputs=output, name=\"generator\")\n",
    "            print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))\n",
    "        \n",
    "        else: \n",
    "          generator = Model(inputs=inputs, outputs=output, name=\"generator\")\n",
    "\n",
    "        generator.summary()\n",
    "        return generator\n",
    "\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        \"build the discriminator model\"\n",
    "        d_inputs = Input(shape=(self.d_dim,))\n",
    "        h = Dense(64, input_shape=(self.g_dim,))(d_inputs) \n",
    "        h = LeakyReLU(alpha=0.2)(h)\n",
    "        h = Dropout(0.1)(h)\n",
    "        h = Dense(64)(h) #32\n",
    "        h = LeakyReLU(alpha=0.2)(h)\n",
    "        h = Dropout(0.1)(h)\n",
    "        h = Dense(1, activation=\"sigmoid\")(h)\n",
    "\n",
    "        if self.distribute:\n",
    "          mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() #tf.distribute.MirroredStrategy()\n",
    "          with mirrored_strategy.scope():\n",
    "            discriminator = Model(d_inputs, h, name=\"discriminator\")\n",
    "            print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))\n",
    "        else:\n",
    "          discriminator = Model(d_inputs, h, name=\"discriminator\")\n",
    "\n",
    "        discriminator.summary()\n",
    "        return discriminator\n",
    "\n",
    "    def compile(self, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_ages, one_hot_labels = data\n",
    "        real_ages = tf.cast(real_ages, tf.float32)\n",
    "        #print(\"real_ages:\", real_ages[0:2])\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        age_one_hot_labels = one_hot_labels[:, None]\n",
    "        print(\"age_one_hot_labels1:\", age_one_hot_labels[0:2])\n",
    "\n",
    "        age_one_hot_labels = tf.repeat(age_one_hot_labels, repeats=[1])\n",
    "        print(\"age_one_hot_labels2:\", age_one_hot_labels[0:2])\n",
    "\n",
    "        age_one_hot_labels = tf.reshape(age_one_hot_labels, (-1, self.num_classes))\n",
    "        age_one_hot_labels = tf.cast(age_one_hot_labels, tf.float32)\n",
    "        print(\"age_one_hot_labels3:\", age_one_hot_labels[0:2])\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_ages)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.noise_dim))\n",
    "        random_vector_labels = tf.concat([random_latent_vectors, one_hot_labels], axis=1)\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake ages.\n",
    "        generated_ages = self.generator(random_vector_labels)\n",
    "        generated_ages= tf.cast(generated_ages, tf.float32)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here. and tf.concat is on the last dimension (-1)\n",
    "        fake_ages_and_labels = tf.concat([generated_ages, age_one_hot_labels], -1)\n",
    "        real_ages_and_labels = tf.concat([real_ages, age_one_hot_labels], -1) \n",
    "        combined_ages = tf.concat([fake_ages_and_labels, real_ages_and_labels], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake ages. 1 == fake, 0 == real\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # TODO: concerned that observations are ordered fake then real - do we want to concat, then shuffle, then separate?\n",
    "        \n",
    "        #labels = tf.random.shuffle(labels, seed = 24)\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_ages)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.t\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.noise_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_ages = self.generator(random_vector_labels)\n",
    "            fake_ages_and_labels = tf.concat([fake_ages, age_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_ages_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def generate(self, n=1000, one_hot_label=[1., 0., 0.]):\n",
    "        \"\"\"Generate n ages for a class\"\"\"\n",
    "        print(\"Generating: \", n, \" ages for unit type: \", one_hot_label)\n",
    "        input_noise = tf.random.normal((n, self.noise_dim), 0, 1)\n",
    "        random_vector_labels = tf.concat([input_noise, one_hot_label], axis=1)\n",
    "\n",
    "        ages = self.generator(random_vector_labels)\n",
    "\n",
    "        return ages.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dianam/Documents/data_science/PlayGround/DS6050/eICU_gan/gan_work\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "path_prefix = ''\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    #path_prefix = '/content/drive/My Drive/eICU_gan/eICU_gan_data.db'\n",
    "    path_prefix = '/sfs/qumulo/qhome/dmf4ns/DS6050/'\n",
    "\n",
    "else:\n",
    "    path_prefix = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4wxEV4vouC7x"
   },
   "outputs": [],
   "source": [
    "ages_ethnicity_np = np.load(path_prefix + \"eICU_age_ethnicity.npy\", allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZmKz22HuC7y",
    "outputId": "5ac478bd-32e3-4a7f-f00e-a83cf9e7ef57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(87,), ('Caucasian',)],\n",
       "       [(87,), ('Caucasian',)],\n",
       "       [(76,), ('Caucasian',)],\n",
       "       ...,\n",
       "       [(41,), ('African American',)],\n",
       "       [(72,), ('Caucasian',)],\n",
       "       [(50,), ('Caucasian',)]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages_ethnicity_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIWt5ZbvuC7y",
    "outputId": "5eac12d1-75ee-4e31-b3c6-5d0d45998da2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethnicity length:  2253\n"
     ]
    }
   ],
   "source": [
    "ethnicity_np = np.asarray(ages_ethnicity_np[:,1].flatten().tolist()).flatten()\n",
    "print('ethnicity length: ', len(ethnicity_np))\n",
    "#print(ethnicity_np[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhTefOGQuC7y",
    "outputId": "5919582e-0545-4620-a778-e47ed83f2c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  2253\n",
      "[[(87,) ('Caucasian',)]\n",
      " [(87,) ('Caucasian',)]\n",
      " [(76,) ('Caucasian',)]\n",
      " [(34,) ('Caucasian',)]\n",
      " [(61,) ('Caucasian',)]]\n",
      "ages length:  2253\n",
      "ethnicity length:  2253\n",
      "(2253, 2)\n",
      "                   age\n",
      "ethnicity             \n",
      "African American   231\n",
      "Caucasian         2010\n",
      "Native American     12\n",
      "============================================================\n",
      "FILTERED:\n",
      "                   age  ethnicity_code\n",
      "ethnicity                             \n",
      "African American   230             230\n",
      "Caucasian         2010            2010\n",
      "Native American     12              12\n",
      "mean age:  63.507548845470694\n",
      "std age:  17.573\n",
      "min age:  15\n",
      "max age:  89\n",
      "[0.972972972972973, 0.972972972972973, 0.8243243243243243, 0.25675675675675674, 0.6216216216216216]\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Shape of ages: (2252, 1)\n",
      "Shape of labels: (2252, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-8a7ece02a874>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ages_filtered['ethnicity_code'] = df_ages_filtered['ethnicity'].astype('category').cat.codes\n"
     ]
    }
   ],
   "source": [
    "ages_ethnicity_np = np.load(path_prefix + \"eICU_age_ethnicity.npy\", allow_pickle=True)\n",
    "print('length: ', len(ages_ethnicity_np))\n",
    "print(ages_ethnicity_np[0:5])\n",
    "\n",
    "ages_np = np.asarray(ages_ethnicity_np[:,0].flatten().tolist()).flatten()\n",
    "print('ages length: ', len(ages_np))\n",
    "#print(ages_np[0:5])\n",
    "\n",
    "ethnicity_np = np.asarray(ages_ethnicity_np[:,1].flatten().tolist()).flatten()\n",
    "print('ethnicity length: ', len(ethnicity_np))\n",
    "#print(ethnicity_np[0:5])\n",
    "\n",
    "df_ages = pd.DataFrame(zip(ages_np, ethnicity_np), columns=['age','ethnicity'])\n",
    "print(df_ages.shape)\n",
    "print(df_ages.groupby('ethnicity').count())\n",
    "\n",
    "# create data set without 90 or greater since that was a category flattened\n",
    "print(\"==\" * 30)\n",
    "print(\"FILTERED:\")\n",
    "df_ages_filtered = df_ages.query(\"age < 90\")\n",
    "df_ages_filtered['ethnicity_code'] = df_ages_filtered['ethnicity'].astype('category').cat.codes\n",
    "print(df_ages_filtered.groupby('ethnicity').count())\n",
    "\n",
    "\n",
    "mean_age_filtered = df_ages_filtered.age.mean()\n",
    "std_age_filtered =  df_ages_filtered.age.std()\n",
    "min_age_filtered =  df_ages_filtered.age.min()\n",
    "max_age_filtered =  df_ages_filtered.age.max()\n",
    "\n",
    "print(\"mean age: \", mean_age_filtered)\n",
    "print(\"std age: \", np.round(std_age_filtered,3))\n",
    "print(\"min age: \", min_age_filtered)\n",
    "print(\"max age: \", max_age_filtered)\n",
    "\n",
    "scaled_ages_filtered = [(x - min_age_filtered)/(max_age_filtered - min_age_filtered) for x in df_ages_filtered['age']]\n",
    "all_ages = np.reshape(scaled_ages_filtered, (-1, 1))\n",
    "\n",
    "all_labels = keras.utils.to_categorical(df_ages_filtered['ethnicity_code'], 3)\n",
    "\n",
    "\n",
    "print(scaled_ages_filtered[0:5])\n",
    "print(np.array(all_labels[0:5]))\n",
    "print(f\"Shape of ages: {all_ages.shape}\")\n",
    "\n",
    "print(f\"Shape of labels: {all_labels.shape}\")\n",
    "\n",
    "# Create tf.data.Dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_ages, all_labels))\n",
    "dataset = dataset.shuffle(buffer_size=200).batch(64)\n",
    "#list(dataset.as_numpy_iterator())\n",
    "#for element in dataset:\n",
    "#    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "RJ80Cu_fuC7z",
    "outputId": "32a173a8-5f3e-4ed5-d2c9-b16dda72a4d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(data=df_ages, x='age', hue='ethnicity', bins=70, color='orange', kde=True, alpha=0.6)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GWKp3RHXuC7z",
    "outputId": "75bedaf3-e90c-4ff1-c463-15437044694a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>African American</th>\n",
       "      <th>Caucasian</th>\n",
       "      <th>Native American</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>62</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>41</td>\n",
       "      <td>African American</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>41</td>\n",
       "      <td>African American</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>72</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>50</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age         ethnicity  African American  Caucasian  Native American\n",
       "2248   62         Caucasian               0.0        1.0              0.0\n",
       "2249   41  African American               1.0        0.0              0.0\n",
       "2250   41  African American               1.0        0.0              0.0\n",
       "2251   72         Caucasian               0.0        1.0              0.0\n",
       "2252   50         Caucasian               0.0        1.0              0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode the ethnicity\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder_df = pd.DataFrame(encoder.fit_transform(df_ages[['ethnicity']]).toarray())\n",
    "\n",
    "#merge one-hot encoded columns back with original DataFrame\n",
    "final_df = df_ages.join(encoder_df)\n",
    "final_df.columns = ['age', 'ethnicity', 'African American', 'Caucasian', 'Native American']\n",
    "final_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "x4TWdUNGuC70",
    "outputId": "5253d321-2c7a-41ff-8de5-8aa7d50971f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethnicity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>African American</th>\n",
       "      <td>231.0</td>\n",
       "      <td>56.151515</td>\n",
       "      <td>16.861546</td>\n",
       "      <td>19.0</td>\n",
       "      <td>46.50</td>\n",
       "      <td>58.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caucasian</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>64.443781</td>\n",
       "      <td>17.419489</td>\n",
       "      <td>15.0</td>\n",
       "      <td>55.00</td>\n",
       "      <td>67.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Native American</th>\n",
       "      <td>12.0</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>20.331346</td>\n",
       "      <td>19.0</td>\n",
       "      <td>39.25</td>\n",
       "      <td>48.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     age                                                     \n",
       "                   count       mean        std   min    25%   50%   75%   max\n",
       "ethnicity                                                                    \n",
       "African American   231.0  56.151515  16.861546  19.0  46.50  58.0  69.0  90.0\n",
       "Caucasian         2010.0  64.443781  17.419489  15.0  55.00  67.0  78.0  89.0\n",
       "Native American     12.0  50.500000  20.331346  19.0  39.25  48.0  66.0  88.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ages.groupby('ethnicity').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7jflWgEuC70"
   },
   "source": [
    "## Create the ConditionalGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYoRh28vuC71",
    "outputId": "33d9638c-ec0a-49f9-986f-f362a53a03a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator input dim:  53\n",
      "Dicrimination input dim:  4\n",
      "WARNING:tensorflow:From <ipython-input-4-20adeaf39045>:83: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CommunicationImplementation.AUTO\n",
      "Number of devices: 2\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,545\n",
      "Trainable params: 4,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CommunicationImplementation.AUTO\n",
      "Number of devices: 2\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 53)]              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                3456      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 7,937\n",
      "Trainable params: 7,809\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cgan = ConditionalGAN(noise_dim=50,\n",
    "                 data_shape=1,\n",
    "                 num_classes=3, \n",
    "                 d_learning_rate=1e-6, \n",
    "                 g_learning_rate=1e-6, \n",
    "                 batch_size=32, \n",
    "                 start_epoch=0,\n",
    "                 verbose = True,\n",
    "                 distribute= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "g4SId-gauC72"
   },
   "outputs": [],
   "source": [
    "cgan.compile(loss_fn=keras.losses.BinaryCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lO40wQe_uC72",
    "outputId": "29bc98e9-a8e2-464c-a647-42bdee64f7ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "age_one_hot_labels1: Tensor(\"strided_slice_1:0\", shape=(None, 1, 3), dtype=float32)\n",
      "age_one_hot_labels2: Tensor(\"strided_slice_2:0\", shape=(None,), dtype=float32)\n",
      "age_one_hot_labels3: Tensor(\"strided_slice_3:0\", shape=(None, 3), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dianam/.local/lib/python3.8/site-packages/keras/backend.py:4993: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_one_hot_labels1: Tensor(\"strided_slice_1:0\", shape=(None, 1, 3), dtype=float32)\n",
      "age_one_hot_labels2: Tensor(\"strided_slice_2:0\", shape=(None,), dtype=float32)\n",
      "age_one_hot_labels3: Tensor(\"strided_slice_3:0\", shape=(None, 3), dtype=float32)\n",
      "36/36 [==============================] - 1s 4ms/step - g_loss: 0.6988 - d_loss: 0.6939\n",
      "Epoch 2/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6985 - d_loss: 0.6938\n",
      "Epoch 3/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6984 - d_loss: 0.6938\n",
      "Epoch 4/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6981 - d_loss: 0.6937\n",
      "Epoch 5/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6980 - d_loss: 0.6936\n",
      "Epoch 6/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6978 - d_loss: 0.6936\n",
      "Epoch 7/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6935\n",
      "Epoch 8/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6934\n",
      "Epoch 9/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6973 - d_loss: 0.6934\n",
      "Epoch 10/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6933\n",
      "Epoch 11/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6933\n",
      "Epoch 12/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6932\n",
      "Epoch 13/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6931\n",
      "Epoch 14/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6931\n",
      "Epoch 15/1000\n",
      "36/36 [==============================] - 0s 6ms/step - g_loss: 0.6965 - d_loss: 0.6930\n",
      "Epoch 16/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6929\n",
      "Epoch 17/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6929\n",
      "Epoch 18/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6961 - d_loss: 0.6928\n",
      "Epoch 19/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6927\n",
      "Epoch 20/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6927\n",
      "Epoch 21/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6926\n",
      "Epoch 22/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6925\n",
      "Epoch 23/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6925\n",
      "Epoch 24/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6924\n",
      "Epoch 25/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6924\n",
      "Epoch 26/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6923\n",
      "Epoch 27/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6922\n",
      "Epoch 28/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6922\n",
      "Epoch 29/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6922\n",
      "Epoch 30/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6949 - d_loss: 0.6921\n",
      "Epoch 31/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6948 - d_loss: 0.6920\n",
      "Epoch 32/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6947 - d_loss: 0.6920\n",
      "Epoch 33/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6947 - d_loss: 0.6919\n",
      "Epoch 34/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6919\n",
      "Epoch 35/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6945 - d_loss: 0.6918\n",
      "Epoch 36/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6945 - d_loss: 0.6918\n",
      "Epoch 37/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6918\n",
      "Epoch 38/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6917\n",
      "Epoch 39/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6916\n",
      "Epoch 40/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6916\n",
      "Epoch 41/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6917\n",
      "Epoch 42/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6916\n",
      "Epoch 43/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6915\n",
      "Epoch 44/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6915\n",
      "Epoch 45/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6915\n",
      "Epoch 46/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6915\n",
      "Epoch 47/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6915\n",
      "Epoch 48/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6914\n",
      "Epoch 49/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6914\n",
      "Epoch 50/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6914\n",
      "Epoch 51/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6914\n",
      "Epoch 52/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6914\n",
      "Epoch 53/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6913\n",
      "Epoch 54/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6913\n",
      "Epoch 55/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6913\n",
      "Epoch 56/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6912\n",
      "Epoch 57/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6911\n",
      "Epoch 58/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6913\n",
      "Epoch 59/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6912\n",
      "Epoch 60/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6912\n",
      "Epoch 61/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6912\n",
      "Epoch 62/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6912\n",
      "Epoch 63/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6913\n",
      "Epoch 64/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6912\n",
      "Epoch 65/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6912\n",
      "Epoch 66/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6911\n",
      "Epoch 67/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6912\n",
      "Epoch 68/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6912\n",
      "Epoch 69/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6912\n",
      "Epoch 70/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6911: 0s - g_loss: 0.6938 - d_loss: 0.\n",
      "Epoch 71/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6912\n",
      "Epoch 72/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6913\n",
      "Epoch 73/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6911\n",
      "Epoch 74/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6912\n",
      "Epoch 75/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6912\n",
      "Epoch 76/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6912\n",
      "Epoch 77/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6914\n",
      "Epoch 78/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6912\n",
      "Epoch 79/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6914\n",
      "Epoch 80/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6912\n",
      "Epoch 81/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6913\n",
      "Epoch 82/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6912\n",
      "Epoch 83/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6913\n",
      "Epoch 84/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6914\n",
      "Epoch 85/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6912\n",
      "Epoch 86/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6912\n",
      "Epoch 87/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6912\n",
      "Epoch 88/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6911\n",
      "Epoch 89/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6913\n",
      "Epoch 90/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6912\n",
      "Epoch 91/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6912\n",
      "Epoch 92/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6913\n",
      "Epoch 93/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6914\n",
      "Epoch 94/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6914\n",
      "Epoch 95/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6913\n",
      "Epoch 96/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6913\n",
      "Epoch 97/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6915\n",
      "Epoch 98/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6914\n",
      "Epoch 99/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6916\n",
      "Epoch 100/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6914\n",
      "Epoch 101/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6914\n",
      "Epoch 102/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6914\n",
      "Epoch 103/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6916\n",
      "Epoch 104/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6917\n",
      "Epoch 105/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6915\n",
      "Epoch 106/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6917\n",
      "Epoch 107/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6917\n",
      "Epoch 108/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6916\n",
      "Epoch 109/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6916\n",
      "Epoch 110/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6916\n",
      "Epoch 111/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6916\n",
      "Epoch 112/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6916\n",
      "Epoch 113/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6918\n",
      "Epoch 114/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6916\n",
      "Epoch 115/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6918\n",
      "Epoch 116/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6918\n",
      "Epoch 117/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6917\n",
      "Epoch 118/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6918\n",
      "Epoch 119/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6917\n",
      "Epoch 120/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6918\n",
      "Epoch 121/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6920\n",
      "Epoch 122/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6917\n",
      "Epoch 123/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6917\n",
      "Epoch 124/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6919\n",
      "Epoch 125/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6918\n",
      "Epoch 126/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6922\n",
      "Epoch 127/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6919\n",
      "Epoch 128/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6919\n",
      "Epoch 129/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6918\n",
      "Epoch 130/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6917\n",
      "Epoch 131/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6919\n",
      "Epoch 132/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6942 - d_loss: 0.6921\n",
      "Epoch 133/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6921\n",
      "Epoch 134/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6920\n",
      "Epoch 135/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6921\n",
      "Epoch 136/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6922\n",
      "Epoch 137/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6922\n",
      "Epoch 138/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6921\n",
      "Epoch 139/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6921\n",
      "Epoch 140/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6922\n",
      "Epoch 141/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6922\n",
      "Epoch 142/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6923\n",
      "Epoch 143/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6919\n",
      "Epoch 144/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6922\n",
      "Epoch 145/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6922\n",
      "Epoch 146/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6923\n",
      "Epoch 147/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6922\n",
      "Epoch 148/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6923\n",
      "Epoch 149/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6923\n",
      "Epoch 150/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6923\n",
      "Epoch 151/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6923\n",
      "Epoch 152/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6923\n",
      "Epoch 153/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6923\n",
      "Epoch 154/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6922\n",
      "Epoch 155/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6925\n",
      "Epoch 156/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6923\n",
      "Epoch 157/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6923\n",
      "Epoch 158/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6923\n",
      "Epoch 159/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6925\n",
      "Epoch 160/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6922\n",
      "Epoch 161/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6925\n",
      "Epoch 162/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6926\n",
      "Epoch 163/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6923\n",
      "Epoch 164/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6924\n",
      "Epoch 165/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6924\n",
      "Epoch 166/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6924\n",
      "Epoch 167/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6924\n",
      "Epoch 168/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6925\n",
      "Epoch 169/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6924\n",
      "Epoch 170/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6924\n",
      "Epoch 171/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6922\n",
      "Epoch 172/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6955 - d_loss: 0.6925\n",
      "Epoch 173/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6955 - d_loss: 0.6924\n",
      "Epoch 174/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6959 - d_loss: 0.6924\n",
      "Epoch 175/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6958 - d_loss: 0.6924\n",
      "Epoch 176/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6924\n",
      "Epoch 177/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6923\n",
      "Epoch 178/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6961 - d_loss: 0.6922\n",
      "Epoch 179/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6924\n",
      "Epoch 180/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6959 - d_loss: 0.6923\n",
      "Epoch 181/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6961 - d_loss: 0.6923\n",
      "Epoch 182/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6963 - d_loss: 0.6923\n",
      "Epoch 183/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6962 - d_loss: 0.6924\n",
      "Epoch 184/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6962 - d_loss: 0.6924\n",
      "Epoch 185/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6963 - d_loss: 0.6923\n",
      "Epoch 186/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6964 - d_loss: 0.6923\n",
      "Epoch 187/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6922\n",
      "Epoch 188/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6964 - d_loss: 0.6922\n",
      "Epoch 189/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6922\n",
      "Epoch 190/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6923\n",
      "Epoch 191/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6923\n",
      "Epoch 192/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6922\n",
      "Epoch 193/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6923\n",
      "Epoch 194/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6970 - d_loss: 0.6922\n",
      "Epoch 195/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6920\n",
      "Epoch 196/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6970 - d_loss: 0.6922\n",
      "Epoch 197/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6970 - d_loss: 0.6921\n",
      "Epoch 198/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6973 - d_loss: 0.6920\n",
      "Epoch 199/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6921\n",
      "Epoch 200/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6920\n",
      "Epoch 201/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6976 - d_loss: 0.6921\n",
      "Epoch 202/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6974 - d_loss: 0.6921\n",
      "Epoch 203/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6921\n",
      "Epoch 204/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6977 - d_loss: 0.6920\n",
      "Epoch 205/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6920\n",
      "Epoch 206/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6979 - d_loss: 0.6919\n",
      "Epoch 207/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6980 - d_loss: 0.6919\n",
      "Epoch 208/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6919\n",
      "Epoch 209/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6981 - d_loss: 0.6918\n",
      "Epoch 210/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6982 - d_loss: 0.6918\n",
      "Epoch 211/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6982 - d_loss: 0.6918\n",
      "Epoch 212/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6983 - d_loss: 0.6918\n",
      "Epoch 213/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6983 - d_loss: 0.6918\n",
      "Epoch 214/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6985 - d_loss: 0.6917\n",
      "Epoch 215/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6985 - d_loss: 0.6917\n",
      "Epoch 216/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6985 - d_loss: 0.6917\n",
      "Epoch 217/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6987 - d_loss: 0.6917\n",
      "Epoch 218/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6917\n",
      "Epoch 219/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6988 - d_loss: 0.6916\n",
      "Epoch 220/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6991 - d_loss: 0.6916\n",
      "Epoch 221/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6990 - d_loss: 0.6915\n",
      "Epoch 222/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6990 - d_loss: 0.6916\n",
      "Epoch 223/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6992 - d_loss: 0.6915\n",
      "Epoch 224/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6992 - d_loss: 0.6915\n",
      "Epoch 225/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6995 - d_loss: 0.6914\n",
      "Epoch 226/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6994 - d_loss: 0.6914\n",
      "Epoch 227/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6996 - d_loss: 0.6915\n",
      "Epoch 228/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6995 - d_loss: 0.6915\n",
      "Epoch 229/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6998 - d_loss: 0.6914\n",
      "Epoch 230/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6998 - d_loss: 0.6913\n",
      "Epoch 231/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6998 - d_loss: 0.6913\n",
      "Epoch 232/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7000 - d_loss: 0.6913\n",
      "Epoch 233/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6999 - d_loss: 0.6912\n",
      "Epoch 234/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7000 - d_loss: 0.6913\n",
      "Epoch 235/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7001 - d_loss: 0.6912\n",
      "Epoch 236/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7000 - d_loss: 0.6911\n",
      "Epoch 237/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7003 - d_loss: 0.6911\n",
      "Epoch 238/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7004 - d_loss: 0.6912\n",
      "Epoch 239/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7005 - d_loss: 0.6911\n",
      "Epoch 240/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7005 - d_loss: 0.6911\n",
      "Epoch 241/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7007 - d_loss: 0.6909\n",
      "Epoch 242/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7007 - d_loss: 0.6910\n",
      "Epoch 243/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7007 - d_loss: 0.6910\n",
      "Epoch 244/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7009 - d_loss: 0.6909\n",
      "Epoch 245/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7009 - d_loss: 0.6909\n",
      "Epoch 246/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7009 - d_loss: 0.6909\n",
      "Epoch 247/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7008 - d_loss: 0.6909\n",
      "Epoch 248/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7010 - d_loss: 0.6908\n",
      "Epoch 249/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7012 - d_loss: 0.6908\n",
      "Epoch 250/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7011 - d_loss: 0.6908\n",
      "Epoch 251/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7013 - d_loss: 0.6908\n",
      "Epoch 252/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7013 - d_loss: 0.6907\n",
      "Epoch 253/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7014 - d_loss: 0.6907\n",
      "Epoch 254/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7014 - d_loss: 0.6907\n",
      "Epoch 255/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7015 - d_loss: 0.6906\n",
      "Epoch 256/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7015 - d_loss: 0.6906\n",
      "Epoch 257/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6905\n",
      "Epoch 258/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6906\n",
      "Epoch 259/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6905\n",
      "Epoch 260/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7015 - d_loss: 0.6905\n",
      "Epoch 261/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6905\n",
      "Epoch 262/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7017 - d_loss: 0.6904\n",
      "Epoch 263/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7017 - d_loss: 0.6904\n",
      "Epoch 264/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7021 - d_loss: 0.6904\n",
      "Epoch 265/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7019 - d_loss: 0.6904\n",
      "Epoch 266/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7019 - d_loss: 0.6903\n",
      "Epoch 267/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6904\n",
      "Epoch 268/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7019 - d_loss: 0.6904\n",
      "Epoch 269/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7019 - d_loss: 0.6903\n",
      "Epoch 270/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7017 - d_loss: 0.6904\n",
      "Epoch 271/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7020 - d_loss: 0.6903\n",
      "Epoch 272/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7018 - d_loss: 0.6904\n",
      "Epoch 273/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7019 - d_loss: 0.6903\n",
      "Epoch 274/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7017 - d_loss: 0.6903\n",
      "Epoch 275/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7020 - d_loss: 0.6902\n",
      "Epoch 276/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7019 - d_loss: 0.6904\n",
      "Epoch 277/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7015 - d_loss: 0.6903\n",
      "Epoch 278/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7017 - d_loss: 0.6902\n",
      "Epoch 279/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7017 - d_loss: 0.6903\n",
      "Epoch 280/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7015 - d_loss: 0.6902\n",
      "Epoch 281/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7015 - d_loss: 0.6903\n",
      "Epoch 282/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7015 - d_loss: 0.6902\n",
      "Epoch 283/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7016 - d_loss: 0.6903\n",
      "Epoch 284/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7015 - d_loss: 0.6903\n",
      "Epoch 285/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7018 - d_loss: 0.6903\n",
      "Epoch 286/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6900\n",
      "Epoch 287/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7015 - d_loss: 0.6903\n",
      "Epoch 288/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6903\n",
      "Epoch 289/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7011 - d_loss: 0.6902\n",
      "Epoch 290/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7014 - d_loss: 0.6903\n",
      "Epoch 291/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7013 - d_loss: 0.6903\n",
      "Epoch 292/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7015 - d_loss: 0.6901\n",
      "Epoch 293/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7009 - d_loss: 0.6901\n",
      "Epoch 294/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7011 - d_loss: 0.6904\n",
      "Epoch 295/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7010 - d_loss: 0.6903\n",
      "Epoch 296/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7007 - d_loss: 0.6902\n",
      "Epoch 297/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7009 - d_loss: 0.6903\n",
      "Epoch 298/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7009 - d_loss: 0.6903\n",
      "Epoch 299/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7008 - d_loss: 0.6903\n",
      "Epoch 300/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7004 - d_loss: 0.6903\n",
      "Epoch 301/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7003 - d_loss: 0.6903\n",
      "Epoch 302/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7003 - d_loss: 0.6905\n",
      "Epoch 303/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7002 - d_loss: 0.6902\n",
      "Epoch 304/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7000 - d_loss: 0.6903\n",
      "Epoch 305/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7001 - d_loss: 0.6905\n",
      "Epoch 306/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6998 - d_loss: 0.6903\n",
      "Epoch 307/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7001 - d_loss: 0.6903\n",
      "Epoch 308/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6997 - d_loss: 0.6902\n",
      "Epoch 309/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6999 - d_loss: 0.6903\n",
      "Epoch 310/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6996 - d_loss: 0.6903\n",
      "Epoch 311/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6998 - d_loss: 0.6905\n",
      "Epoch 312/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6999 - d_loss: 0.6904\n",
      "Epoch 313/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6997 - d_loss: 0.6904\n",
      "Epoch 314/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6995 - d_loss: 0.6904\n",
      "Epoch 315/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6993 - d_loss: 0.6905\n",
      "Epoch 316/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6991 - d_loss: 0.6904\n",
      "Epoch 317/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6904\n",
      "Epoch 318/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6906\n",
      "Epoch 319/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6904\n",
      "Epoch 320/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6987 - d_loss: 0.6905\n",
      "Epoch 321/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6906\n",
      "Epoch 322/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6905\n",
      "Epoch 323/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6906\n",
      "Epoch 324/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6982 - d_loss: 0.6906\n",
      "Epoch 325/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6987 - d_loss: 0.6906\n",
      "Epoch 326/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6981 - d_loss: 0.6905\n",
      "Epoch 327/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6981 - d_loss: 0.6906\n",
      "Epoch 328/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6907\n",
      "Epoch 329/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6906\n",
      "Epoch 330/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6906\n",
      "Epoch 331/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6976 - d_loss: 0.6905\n",
      "Epoch 332/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6973 - d_loss: 0.6905\n",
      "Epoch 333/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6973 - d_loss: 0.6907\n",
      "Epoch 334/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6906\n",
      "Epoch 335/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6908\n",
      "Epoch 336/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6906\n",
      "Epoch 337/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6970 - d_loss: 0.6908\n",
      "Epoch 338/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6907\n",
      "Epoch 339/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6907\n",
      "Epoch 340/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6908\n",
      "Epoch 341/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6909\n",
      "Epoch 342/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6909\n",
      "Epoch 343/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6910\n",
      "Epoch 344/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6907\n",
      "Epoch 345/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6909\n",
      "Epoch 346/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6907\n",
      "Epoch 347/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6908\n",
      "Epoch 348/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6961 - d_loss: 0.6907\n",
      "Epoch 349/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6961 - d_loss: 0.6907\n",
      "Epoch 350/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6962 - d_loss: 0.6908\n",
      "Epoch 351/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6960 - d_loss: 0.6907\n",
      "Epoch 352/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6907\n",
      "Epoch 353/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6909\n",
      "Epoch 354/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6907\n",
      "Epoch 355/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6954 - d_loss: 0.6908\n",
      "Epoch 356/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6957 - d_loss: 0.6907\n",
      "Epoch 357/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6956 - d_loss: 0.6908\n",
      "Epoch 358/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6954 - d_loss: 0.6909\n",
      "Epoch 359/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6953 - d_loss: 0.6908\n",
      "Epoch 360/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6908\n",
      "Epoch 361/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6908\n",
      "Epoch 362/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6906\n",
      "Epoch 363/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6909\n",
      "Epoch 364/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6908\n",
      "Epoch 365/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6910\n",
      "Epoch 366/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6909\n",
      "Epoch 367/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6909\n",
      "Epoch 368/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6908\n",
      "Epoch 369/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6909\n",
      "Epoch 370/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6908\n",
      "Epoch 371/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6907\n",
      "Epoch 372/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6908\n",
      "Epoch 373/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6907\n",
      "Epoch 374/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6908\n",
      "Epoch 375/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6906\n",
      "Epoch 376/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6908\n",
      "Epoch 377/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6907\n",
      "Epoch 378/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6907\n",
      "Epoch 379/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6909\n",
      "Epoch 380/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6907\n",
      "Epoch 381/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6907\n",
      "Epoch 382/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6945 - d_loss: 0.6906\n",
      "Epoch 383/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6907\n",
      "Epoch 384/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6942 - d_loss: 0.6907\n",
      "Epoch 385/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6944 - d_loss: 0.6907\n",
      "Epoch 386/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6907\n",
      "Epoch 387/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6907\n",
      "Epoch 388/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6905\n",
      "Epoch 389/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6905\n",
      "Epoch 390/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6906\n",
      "Epoch 391/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6905\n",
      "Epoch 392/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6906\n",
      "Epoch 393/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6942 - d_loss: 0.6906\n",
      "Epoch 394/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6906\n",
      "Epoch 395/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6904\n",
      "Epoch 396/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6905\n",
      "Epoch 397/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6905\n",
      "Epoch 398/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6906\n",
      "Epoch 399/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6904\n",
      "Epoch 400/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6905\n",
      "Epoch 401/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6905\n",
      "Epoch 402/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6904\n",
      "Epoch 403/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6906\n",
      "Epoch 404/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6903\n",
      "Epoch 405/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6903\n",
      "Epoch 406/1000\n",
      "36/36 [==============================] - 0s 6ms/step - g_loss: 0.6937 - d_loss: 0.6905\n",
      "Epoch 407/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6903\n",
      "Epoch 408/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6904\n",
      "Epoch 409/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6903\n",
      "Epoch 410/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6903\n",
      "Epoch 411/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6903\n",
      "Epoch 412/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6904\n",
      "Epoch 413/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6902\n",
      "Epoch 414/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6904\n",
      "Epoch 415/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6904\n",
      "Epoch 416/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6903\n",
      "Epoch 417/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6903\n",
      "Epoch 418/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6939 - d_loss: 0.6903\n",
      "Epoch 419/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6903\n",
      "Epoch 420/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6903\n",
      "Epoch 421/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6939 - d_loss: 0.6903\n",
      "Epoch 422/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6902\n",
      "Epoch 423/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6902\n",
      "Epoch 424/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6904\n",
      "Epoch 425/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6902\n",
      "Epoch 426/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6901\n",
      "Epoch 427/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6900\n",
      "Epoch 428/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6903\n",
      "Epoch 429/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6942 - d_loss: 0.6902\n",
      "Epoch 430/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6902\n",
      "Epoch 431/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6902\n",
      "Epoch 432/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6939 - d_loss: 0.6903\n",
      "Epoch 433/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6901\n",
      "Epoch 434/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6902\n",
      "Epoch 435/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6902\n",
      "Epoch 436/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6901\n",
      "Epoch 437/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6902\n",
      "Epoch 438/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6901\n",
      "Epoch 439/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6902\n",
      "Epoch 440/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6901\n",
      "Epoch 441/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6901\n",
      "Epoch 442/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6900\n",
      "Epoch 443/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6900\n",
      "Epoch 444/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6901\n",
      "Epoch 445/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6902\n",
      "Epoch 446/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6902\n",
      "Epoch 447/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6904\n",
      "Epoch 448/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6901\n",
      "Epoch 449/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6903\n",
      "Epoch 450/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6904\n",
      "Epoch 451/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6903\n",
      "Epoch 452/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6942 - d_loss: 0.6904\n",
      "Epoch 453/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6903\n",
      "Epoch 454/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6902\n",
      "Epoch 455/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6902\n",
      "Epoch 456/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6903\n",
      "Epoch 457/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6905\n",
      "Epoch 458/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6901\n",
      "Epoch 459/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6904\n",
      "Epoch 460/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6942 - d_loss: 0.6901\n",
      "Epoch 461/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6906\n",
      "Epoch 462/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6904\n",
      "Epoch 463/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6905\n",
      "Epoch 464/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6904\n",
      "Epoch 465/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6904\n",
      "Epoch 466/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6950 - d_loss: 0.6904\n",
      "Epoch 467/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6951 - d_loss: 0.6906\n",
      "Epoch 468/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6950 - d_loss: 0.6905\n",
      "Epoch 469/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6904\n",
      "Epoch 470/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6952 - d_loss: 0.6905\n",
      "Epoch 471/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6951 - d_loss: 0.6904\n",
      "Epoch 472/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6905\n",
      "Epoch 473/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6908: 0s - g_loss: 0.6955 - d_loss: \n",
      "Epoch 474/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6901\n",
      "Epoch 475/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6906\n",
      "Epoch 476/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6905\n",
      "Epoch 477/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6905\n",
      "Epoch 478/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6906\n",
      "Epoch 479/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6904\n",
      "Epoch 480/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6959 - d_loss: 0.6906\n",
      "Epoch 481/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6908\n",
      "Epoch 482/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6907\n",
      "Epoch 483/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6903\n",
      "Epoch 484/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6907\n",
      "Epoch 485/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6905\n",
      "Epoch 486/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6906\n",
      "Epoch 487/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6907\n",
      "Epoch 488/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6966 - d_loss: 0.6907\n",
      "Epoch 489/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6966 - d_loss: 0.6905\n",
      "Epoch 490/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6906\n",
      "Epoch 491/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6906\n",
      "Epoch 492/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6907\n",
      "Epoch 493/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6907\n",
      "Epoch 494/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6906\n",
      "Epoch 495/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6908\n",
      "Epoch 496/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6906\n",
      "Epoch 497/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6906\n",
      "Epoch 498/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6908\n",
      "Epoch 499/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6965 - d_loss: 0.6907\n",
      "Epoch 500/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6960 - d_loss: 0.6907\n",
      "Epoch 501/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6909\n",
      "Epoch 502/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6909\n",
      "Epoch 503/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6906\n",
      "Epoch 504/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6909\n",
      "Epoch 505/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6908\n",
      "Epoch 506/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6907\n",
      "Epoch 507/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6907\n",
      "Epoch 508/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6909\n",
      "Epoch 509/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6907\n",
      "Epoch 510/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6906\n",
      "Epoch 511/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6909\n",
      "Epoch 512/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6907\n",
      "Epoch 513/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6908\n",
      "Epoch 514/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6907\n",
      "Epoch 515/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6973 - d_loss: 0.6910\n",
      "Epoch 516/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6908\n",
      "Epoch 517/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6908\n",
      "Epoch 518/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6970 - d_loss: 0.6909\n",
      "Epoch 519/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6909\n",
      "Epoch 520/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6907\n",
      "Epoch 521/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6909\n",
      "Epoch 522/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6909\n",
      "Epoch 523/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6907\n",
      "Epoch 524/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6907\n",
      "Epoch 525/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6908\n",
      "Epoch 526/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6906\n",
      "Epoch 527/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6907\n",
      "Epoch 528/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6979 - d_loss: 0.6907\n",
      "Epoch 529/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6979 - d_loss: 0.6908\n",
      "Epoch 530/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6904\n",
      "Epoch 531/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6905\n",
      "Epoch 532/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6907\n",
      "Epoch 533/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6905\n",
      "Epoch 534/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6908\n",
      "Epoch 535/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6977 - d_loss: 0.6907\n",
      "Epoch 536/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6980 - d_loss: 0.6907\n",
      "Epoch 537/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6981 - d_loss: 0.6907\n",
      "Epoch 538/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6977 - d_loss: 0.6904\n",
      "Epoch 539/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6981 - d_loss: 0.6906\n",
      "Epoch 540/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6982 - d_loss: 0.6904\n",
      "Epoch 541/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6977 - d_loss: 0.6907\n",
      "Epoch 542/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6978 - d_loss: 0.6905\n",
      "Epoch 543/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6979 - d_loss: 0.6902\n",
      "Epoch 544/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6982 - d_loss: 0.6906\n",
      "Epoch 545/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6980 - d_loss: 0.6906\n",
      "Epoch 546/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6983 - d_loss: 0.6906\n",
      "Epoch 547/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6979 - d_loss: 0.6906\n",
      "Epoch 548/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6983 - d_loss: 0.6904\n",
      "Epoch 549/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6987 - d_loss: 0.6905\n",
      "Epoch 550/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6981 - d_loss: 0.6907\n",
      "Epoch 551/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6984 - d_loss: 0.6903\n",
      "Epoch 552/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6985 - d_loss: 0.6904\n",
      "Epoch 553/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6984 - d_loss: 0.6902\n",
      "Epoch 554/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6985 - d_loss: 0.6902\n",
      "Epoch 555/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6989 - d_loss: 0.6904\n",
      "Epoch 556/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6903\n",
      "Epoch 557/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6903\n",
      "Epoch 558/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6903\n",
      "Epoch 559/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6984 - d_loss: 0.6904\n",
      "Epoch 560/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6983 - d_loss: 0.6901\n",
      "Epoch 561/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6987 - d_loss: 0.6903\n",
      "Epoch 562/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6985 - d_loss: 0.6900\n",
      "Epoch 563/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6985 - d_loss: 0.6903\n",
      "Epoch 564/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6987 - d_loss: 0.6901\n",
      "Epoch 565/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6903\n",
      "Epoch 566/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6902\n",
      "Epoch 567/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6904\n",
      "Epoch 568/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6989 - d_loss: 0.6902\n",
      "Epoch 569/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6990 - d_loss: 0.6903\n",
      "Epoch 570/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6986 - d_loss: 0.6900\n",
      "Epoch 571/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6988 - d_loss: 0.6901\n",
      "Epoch 572/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6990 - d_loss: 0.6901\n",
      "Epoch 573/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6990 - d_loss: 0.6900\n",
      "Epoch 574/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6900\n",
      "Epoch 575/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6901\n",
      "Epoch 576/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6901\n",
      "Epoch 577/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6902\n",
      "Epoch 578/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6987 - d_loss: 0.6903\n",
      "Epoch 579/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6900\n",
      "Epoch 580/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6897\n",
      "Epoch 581/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6900\n",
      "Epoch 582/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6900\n",
      "Epoch 583/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6901\n",
      "Epoch 584/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6899\n",
      "Epoch 585/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6901\n",
      "Epoch 586/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6898\n",
      "Epoch 587/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6992 - d_loss: 0.6899\n",
      "Epoch 588/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6898\n",
      "Epoch 589/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6992 - d_loss: 0.6899\n",
      "Epoch 590/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6987 - d_loss: 0.6900\n",
      "Epoch 591/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6899\n",
      "Epoch 592/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6898\n",
      "Epoch 593/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6899\n",
      "Epoch 594/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6900\n",
      "Epoch 595/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6991 - d_loss: 0.6900\n",
      "Epoch 596/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6899\n",
      "Epoch 597/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6901\n",
      "Epoch 598/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6984 - d_loss: 0.6901\n",
      "Epoch 599/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6898\n",
      "Epoch 600/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6898\n",
      "Epoch 601/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6896\n",
      "Epoch 602/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6896\n",
      "Epoch 603/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6899\n",
      "Epoch 604/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6899\n",
      "Epoch 605/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6983 - d_loss: 0.6899\n",
      "Epoch 606/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6987 - d_loss: 0.6898\n",
      "Epoch 607/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6982 - d_loss: 0.6898\n",
      "Epoch 608/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6982 - d_loss: 0.6900\n",
      "Epoch 609/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6981 - d_loss: 0.6898\n",
      "Epoch 610/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6986 - d_loss: 0.6898\n",
      "Epoch 611/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6982 - d_loss: 0.6900\n",
      "Epoch 612/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6981 - d_loss: 0.6902\n",
      "Epoch 613/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6984 - d_loss: 0.6901\n",
      "Epoch 614/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6986 - d_loss: 0.6901\n",
      "Epoch 615/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6977 - d_loss: 0.6904\n",
      "Epoch 616/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6982 - d_loss: 0.6903\n",
      "Epoch 617/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6982 - d_loss: 0.6901: 0s - g_loss: 0.6979 - d_loss: 0.\n",
      "Epoch 618/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6978 - d_loss: 0.6901\n",
      "Epoch 619/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6979 - d_loss: 0.6905\n",
      "Epoch 620/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6980 - d_loss: 0.6900\n",
      "Epoch 621/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6903\n",
      "Epoch 622/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6986 - d_loss: 0.6901\n",
      "Epoch 623/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6973 - d_loss: 0.6904\n",
      "Epoch 624/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6903\n",
      "Epoch 625/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6981 - d_loss: 0.6900: 0s - g_loss: 0.6981 - d_loss: \n",
      "Epoch 626/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6977 - d_loss: 0.6903\n",
      "Epoch 627/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6981 - d_loss: 0.6901\n",
      "Epoch 628/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6904\n",
      "Epoch 629/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6902\n",
      "Epoch 630/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6966 - d_loss: 0.6903\n",
      "Epoch 631/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6970 - d_loss: 0.6900\n",
      "Epoch 632/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6905\n",
      "Epoch 633/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6974 - d_loss: 0.6901\n",
      "Epoch 634/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6971 - d_loss: 0.6904\n",
      "Epoch 635/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6904\n",
      "Epoch 636/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6902\n",
      "Epoch 637/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6966 - d_loss: 0.6902\n",
      "Epoch 638/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6965 - d_loss: 0.6905\n",
      "Epoch 639/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6902\n",
      "Epoch 640/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6977 - d_loss: 0.6904\n",
      "Epoch 641/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6903\n",
      "Epoch 642/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6903\n",
      "Epoch 643/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6904\n",
      "Epoch 644/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6906\n",
      "Epoch 645/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6970 - d_loss: 0.6904\n",
      "Epoch 646/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6905\n",
      "Epoch 647/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6964 - d_loss: 0.6905\n",
      "Epoch 648/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6962 - d_loss: 0.6903\n",
      "Epoch 649/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6903\n",
      "Epoch 650/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6906\n",
      "Epoch 651/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6956 - d_loss: 0.6907\n",
      "Epoch 652/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6960 - d_loss: 0.6906\n",
      "Epoch 653/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6965 - d_loss: 0.6905\n",
      "Epoch 654/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6904\n",
      "Epoch 655/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6906\n",
      "Epoch 656/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6906\n",
      "Epoch 657/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6905\n",
      "Epoch 658/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6955 - d_loss: 0.6907\n",
      "Epoch 659/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6952 - d_loss: 0.6906\n",
      "Epoch 660/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6950 - d_loss: 0.6909\n",
      "Epoch 661/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6959 - d_loss: 0.6911\n",
      "Epoch 662/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6956 - d_loss: 0.6907\n",
      "Epoch 663/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6954 - d_loss: 0.6906\n",
      "Epoch 664/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6951 - d_loss: 0.6905\n",
      "Epoch 665/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6953 - d_loss: 0.6909\n",
      "Epoch 666/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6958 - d_loss: 0.6909\n",
      "Epoch 667/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6909\n",
      "Epoch 668/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6952 - d_loss: 0.6908\n",
      "Epoch 669/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6910\n",
      "Epoch 670/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6947 - d_loss: 0.6908\n",
      "Epoch 671/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6908\n",
      "Epoch 672/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6906\n",
      "Epoch 673/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6905\n",
      "Epoch 674/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6910\n",
      "Epoch 675/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6906\n",
      "Epoch 676/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6908\n",
      "Epoch 677/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6912\n",
      "Epoch 678/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6909\n",
      "Epoch 679/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6942 - d_loss: 0.6906\n",
      "Epoch 680/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6908\n",
      "Epoch 681/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6910\n",
      "Epoch 682/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6907\n",
      "Epoch 683/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6911\n",
      "Epoch 684/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6950 - d_loss: 0.6910\n",
      "Epoch 685/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6910\n",
      "Epoch 686/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6907\n",
      "Epoch 687/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6911\n",
      "Epoch 688/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6949 - d_loss: 0.6906\n",
      "Epoch 689/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6909\n",
      "Epoch 690/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6942 - d_loss: 0.6909\n",
      "Epoch 691/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6909\n",
      "Epoch 692/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6909\n",
      "Epoch 693/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6908\n",
      "Epoch 694/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6908\n",
      "Epoch 695/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6905\n",
      "Epoch 696/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6909\n",
      "Epoch 697/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6909\n",
      "Epoch 698/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6909\n",
      "Epoch 699/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6913\n",
      "Epoch 700/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6908\n",
      "Epoch 701/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6910\n",
      "Epoch 702/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6910\n",
      "Epoch 703/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6908\n",
      "Epoch 704/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6910\n",
      "Epoch 705/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6905\n",
      "Epoch 706/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6907\n",
      "Epoch 707/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6907\n",
      "Epoch 708/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6908\n",
      "Epoch 709/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6912\n",
      "Epoch 710/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6909\n",
      "Epoch 711/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6911\n",
      "Epoch 712/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6909\n",
      "Epoch 713/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6906\n",
      "Epoch 714/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6909\n",
      "Epoch 715/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6911\n",
      "Epoch 716/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6911\n",
      "Epoch 717/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6909\n",
      "Epoch 718/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6909\n",
      "Epoch 719/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6908\n",
      "Epoch 720/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6908\n",
      "Epoch 721/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6908\n",
      "Epoch 722/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6909\n",
      "Epoch 723/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6910\n",
      "Epoch 724/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6909\n",
      "Epoch 725/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6942 - d_loss: 0.6911\n",
      "Epoch 726/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6909\n",
      "Epoch 727/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6906\n",
      "Epoch 728/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6907\n",
      "Epoch 729/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6909\n",
      "Epoch 730/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6912\n",
      "Epoch 731/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6904\n",
      "Epoch 732/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6908\n",
      "Epoch 733/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6907\n",
      "Epoch 734/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6908\n",
      "Epoch 735/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6910\n",
      "Epoch 736/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6906\n",
      "Epoch 737/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6908\n",
      "Epoch 738/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6909\n",
      "Epoch 739/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6910\n",
      "Epoch 740/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6906\n",
      "Epoch 741/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6906\n",
      "Epoch 742/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6905\n",
      "Epoch 743/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6905\n",
      "Epoch 744/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6906\n",
      "Epoch 745/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6905\n",
      "Epoch 746/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6907\n",
      "Epoch 747/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6909\n",
      "Epoch 748/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6911\n",
      "Epoch 749/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6908\n",
      "Epoch 750/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6908\n",
      "Epoch 751/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6907\n",
      "Epoch 752/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6910\n",
      "Epoch 753/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6908\n",
      "Epoch 754/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6911\n",
      "Epoch 755/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6907\n",
      "Epoch 756/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6906\n",
      "Epoch 757/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6910\n",
      "Epoch 758/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6908\n",
      "Epoch 759/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6907\n",
      "Epoch 760/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6910\n",
      "Epoch 761/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6907\n",
      "Epoch 762/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6906\n",
      "Epoch 763/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6909\n",
      "Epoch 764/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6948 - d_loss: 0.6907\n",
      "Epoch 765/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6908\n",
      "Epoch 766/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6909\n",
      "Epoch 767/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6910\n",
      "Epoch 768/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6948 - d_loss: 0.6908\n",
      "Epoch 769/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6912\n",
      "Epoch 770/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6946 - d_loss: 0.6907\n",
      "Epoch 771/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6951 - d_loss: 0.6905\n",
      "Epoch 772/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6906\n",
      "Epoch 773/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6908\n",
      "Epoch 774/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6948 - d_loss: 0.6908\n",
      "Epoch 775/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6947 - d_loss: 0.6912\n",
      "Epoch 776/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6909\n",
      "Epoch 777/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6911\n",
      "Epoch 778/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6945 - d_loss: 0.6910\n",
      "Epoch 779/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6911\n",
      "Epoch 780/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6945 - d_loss: 0.6911\n",
      "Epoch 781/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6911\n",
      "Epoch 782/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6942 - d_loss: 0.6912\n",
      "Epoch 783/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6909\n",
      "Epoch 784/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6944 - d_loss: 0.6907\n",
      "Epoch 785/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6946 - d_loss: 0.6909\n",
      "Epoch 786/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6913\n",
      "Epoch 787/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6948 - d_loss: 0.6909\n",
      "Epoch 788/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6942 - d_loss: 0.6911\n",
      "Epoch 789/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6913\n",
      "Epoch 790/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6913\n",
      "Epoch 791/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6910\n",
      "Epoch 792/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6911\n",
      "Epoch 793/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6954 - d_loss: 0.6906\n",
      "Epoch 794/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6914\n",
      "Epoch 795/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6945 - d_loss: 0.6911\n",
      "Epoch 796/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6913\n",
      "Epoch 797/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6913\n",
      "Epoch 798/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6947 - d_loss: 0.6916\n",
      "Epoch 799/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6945 - d_loss: 0.6909\n",
      "Epoch 800/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6950 - d_loss: 0.6910\n",
      "Epoch 801/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6945 - d_loss: 0.6913\n",
      "Epoch 802/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6917\n",
      "Epoch 803/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6915\n",
      "Epoch 804/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6951 - d_loss: 0.6913\n",
      "Epoch 805/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6914\n",
      "Epoch 806/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6950 - d_loss: 0.6912\n",
      "Epoch 807/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6911\n",
      "Epoch 808/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6914\n",
      "Epoch 809/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6914\n",
      "Epoch 810/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6915\n",
      "Epoch 811/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6914\n",
      "Epoch 812/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6917\n",
      "Epoch 813/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6917\n",
      "Epoch 814/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6920\n",
      "Epoch 815/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6917\n",
      "Epoch 816/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6917\n",
      "Epoch 817/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6920\n",
      "Epoch 818/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6918\n",
      "Epoch 819/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6918\n",
      "Epoch 820/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6918\n",
      "Epoch 821/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6912\n",
      "Epoch 822/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6913\n",
      "Epoch 823/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6916\n",
      "Epoch 824/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6917\n",
      "Epoch 825/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6917\n",
      "Epoch 826/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6918\n",
      "Epoch 827/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6959 - d_loss: 0.6920\n",
      "Epoch 828/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6956 - d_loss: 0.6918\n",
      "Epoch 829/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6958 - d_loss: 0.6916\n",
      "Epoch 830/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6951 - d_loss: 0.6917\n",
      "Epoch 831/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6920\n",
      "Epoch 832/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6916\n",
      "Epoch 833/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6959 - d_loss: 0.6924\n",
      "Epoch 834/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6921\n",
      "Epoch 835/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6919\n",
      "Epoch 836/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6916\n",
      "Epoch 837/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6919\n",
      "Epoch 838/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6923\n",
      "Epoch 839/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6952 - d_loss: 0.6916\n",
      "Epoch 840/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6917\n",
      "Epoch 841/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6923\n",
      "Epoch 842/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6921\n",
      "Epoch 843/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6916\n",
      "Epoch 844/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6922\n",
      "Epoch 845/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6916\n",
      "Epoch 846/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6922\n",
      "Epoch 847/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6922\n",
      "Epoch 848/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6920\n",
      "Epoch 849/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6921\n",
      "Epoch 850/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6923\n",
      "Epoch 851/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6918\n",
      "Epoch 852/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6920\n",
      "Epoch 853/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6961 - d_loss: 0.6921\n",
      "Epoch 854/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6919\n",
      "Epoch 855/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6961 - d_loss: 0.6920\n",
      "Epoch 856/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6915\n",
      "Epoch 857/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6920\n",
      "Epoch 858/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6959 - d_loss: 0.6920\n",
      "Epoch 859/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6921\n",
      "Epoch 860/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6918\n",
      "Epoch 861/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6918\n",
      "Epoch 862/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6921\n",
      "Epoch 863/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6922\n",
      "Epoch 864/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6918\n",
      "Epoch 865/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6922\n",
      "Epoch 866/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6919\n",
      "Epoch 867/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6916\n",
      "Epoch 868/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6919\n",
      "Epoch 869/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6921\n",
      "Epoch 870/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6918\n",
      "Epoch 871/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6923\n",
      "Epoch 872/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6972 - d_loss: 0.6920\n",
      "Epoch 873/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6964 - d_loss: 0.6922\n",
      "Epoch 874/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6919\n",
      "Epoch 875/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6921\n",
      "Epoch 876/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6972 - d_loss: 0.6919\n",
      "Epoch 877/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6920\n",
      "Epoch 878/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6920\n",
      "Epoch 879/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6971 - d_loss: 0.6921\n",
      "Epoch 880/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6964 - d_loss: 0.6921\n",
      "Epoch 881/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6971 - d_loss: 0.6916\n",
      "Epoch 882/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6916\n",
      "Epoch 883/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6919\n",
      "Epoch 884/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6971 - d_loss: 0.6921\n",
      "Epoch 885/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6973 - d_loss: 0.6920\n",
      "Epoch 886/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6919\n",
      "Epoch 887/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6919\n",
      "Epoch 888/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6965 - d_loss: 0.6920\n",
      "Epoch 889/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6969 - d_loss: 0.6916\n",
      "Epoch 890/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6974 - d_loss: 0.6917\n",
      "Epoch 891/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6966 - d_loss: 0.6919\n",
      "Epoch 892/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6974 - d_loss: 0.6921\n",
      "Epoch 893/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6921\n",
      "Epoch 894/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6977 - d_loss: 0.6915\n",
      "Epoch 895/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6917\n",
      "Epoch 896/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6970 - d_loss: 0.6918\n",
      "Epoch 897/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6973 - d_loss: 0.6920\n",
      "Epoch 898/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6972 - d_loss: 0.6917\n",
      "Epoch 899/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6975 - d_loss: 0.6917\n",
      "Epoch 900/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6971 - d_loss: 0.6917\n",
      "Epoch 901/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6915\n",
      "Epoch 902/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6917\n",
      "Epoch 903/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6977 - d_loss: 0.6920\n",
      "Epoch 904/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6916\n",
      "Epoch 905/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6983 - d_loss: 0.6917\n",
      "Epoch 906/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6973 - d_loss: 0.6916\n",
      "Epoch 907/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6916\n",
      "Epoch 908/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6919\n",
      "Epoch 909/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6916\n",
      "Epoch 910/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6920\n",
      "Epoch 911/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6921\n",
      "Epoch 912/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6919\n",
      "Epoch 913/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6913\n",
      "Epoch 914/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6916\n",
      "Epoch 915/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6914\n",
      "Epoch 916/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6918\n",
      "Epoch 917/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6915\n",
      "Epoch 918/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6979 - d_loss: 0.6919\n",
      "Epoch 919/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6970 - d_loss: 0.6919\n",
      "Epoch 920/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6913\n",
      "Epoch 921/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6914\n",
      "Epoch 922/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6971 - d_loss: 0.6915\n",
      "Epoch 923/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6919\n",
      "Epoch 924/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6979 - d_loss: 0.6916\n",
      "Epoch 925/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6973 - d_loss: 0.6917\n",
      "Epoch 926/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6977 - d_loss: 0.6914\n",
      "Epoch 927/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6914\n",
      "Epoch 928/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6979 - d_loss: 0.6918\n",
      "Epoch 929/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6976 - d_loss: 0.6919\n",
      "Epoch 930/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6917\n",
      "Epoch 931/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6917\n",
      "Epoch 932/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6911\n",
      "Epoch 933/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6981 - d_loss: 0.6915\n",
      "Epoch 934/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6913\n",
      "Epoch 935/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6916\n",
      "Epoch 936/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6916\n",
      "Epoch 937/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6912\n",
      "Epoch 938/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6973 - d_loss: 0.6915\n",
      "Epoch 939/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6977 - d_loss: 0.6915\n",
      "Epoch 940/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6983 - d_loss: 0.6915\n",
      "Epoch 941/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6916\n",
      "Epoch 942/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6984 - d_loss: 0.6914\n",
      "Epoch 943/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6914\n",
      "Epoch 944/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6978 - d_loss: 0.6916\n",
      "Epoch 945/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6983 - d_loss: 0.6912\n",
      "Epoch 946/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6915\n",
      "Epoch 947/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6980 - d_loss: 0.6917\n",
      "Epoch 948/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6916\n",
      "Epoch 949/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6978 - d_loss: 0.6914\n",
      "Epoch 950/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6917\n",
      "Epoch 951/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6917\n",
      "Epoch 952/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6914\n",
      "Epoch 953/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6972 - d_loss: 0.6914\n",
      "Epoch 954/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6983 - d_loss: 0.6918\n",
      "Epoch 955/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6917\n",
      "Epoch 956/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6973 - d_loss: 0.6915\n",
      "Epoch 957/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6970 - d_loss: 0.6918\n",
      "Epoch 958/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6972 - d_loss: 0.6915\n",
      "Epoch 959/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6972 - d_loss: 0.6916\n",
      "Epoch 960/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6971 - d_loss: 0.6915\n",
      "Epoch 961/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6966 - d_loss: 0.6912\n",
      "Epoch 962/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6917\n",
      "Epoch 963/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6917\n",
      "Epoch 964/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6977 - d_loss: 0.6913\n",
      "Epoch 965/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6915\n",
      "Epoch 966/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6913\n",
      "Epoch 967/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6970 - d_loss: 0.6915\n",
      "Epoch 968/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6915\n",
      "Epoch 969/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6914\n",
      "Epoch 970/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6908\n",
      "Epoch 971/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6914\n",
      "Epoch 972/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6915\n",
      "Epoch 973/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6915\n",
      "Epoch 974/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6913\n",
      "Epoch 975/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6913\n",
      "Epoch 976/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6970 - d_loss: 0.6916\n",
      "Epoch 977/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6915\n",
      "Epoch 978/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6917\n",
      "Epoch 979/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6917\n",
      "Epoch 980/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6916\n",
      "Epoch 981/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6913\n",
      "Epoch 982/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6914\n",
      "Epoch 983/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6916\n",
      "Epoch 984/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6961 - d_loss: 0.6917\n",
      "Epoch 985/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6914\n",
      "Epoch 986/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6918\n",
      "Epoch 987/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6914\n",
      "Epoch 988/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6920\n",
      "Epoch 989/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6919\n",
      "Epoch 990/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6915\n",
      "Epoch 991/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6919\n",
      "Epoch 992/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6915\n",
      "Epoch 993/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6916\n",
      "Epoch 994/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6920\n",
      "Epoch 995/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6959 - d_loss: 0.6919\n",
      "Epoch 996/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6921\n",
      "Epoch 997/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6915\n",
      "Epoch 998/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6921\n",
      "Epoch 999/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6921\n",
      "Epoch 1000/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f88303880a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cgan.fit(dataset, epochs=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NnUkQDJ3uC72"
   },
   "outputs": [],
   "source": [
    "df_age_eth = pd.DataFrame(columns = ['age', 'ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "mZC-txU3NOGN",
    "outputId": "4cd8191e-329f-4040-8187-811b114a60fd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [age, ethnicity]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_age_eth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "QWYYAtVAuC72",
    "outputId": "5797fa5c-c1b5-49cc-c16d-49743bdee189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating:  231  ages for unit type: [1., 0., 0.]\n",
      "Generated Ages:\n",
      "min:  16.754235614091158\n",
      "mean:  65.89910831662031\n",
      "max:  88.32518267631531\n",
      "stdv:  17.3657610103784\n",
      "True Ages:\n",
      "min:  19\n",
      "mean:  56.15151515151515\n",
      "max:  90\n",
      "stdv:  16.825009339158104\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Generate n ages for a class\"\"\"\n",
    "print(\"Generating: \", 231, \" ages for unit type: [1., 0., 0.]\")\n",
    "\n",
    "age_one_hot_labels = tf.repeat([[1., 0., 0.]],231, axis=0)\n",
    "\n",
    "input_noise = tf.random.normal((231, cgan.noise_dim), 0, 1)\n",
    "random_vector_labels = tf.concat([input_noise, age_one_hot_labels], axis=1)\n",
    "\n",
    "ages = cgan.generator(random_vector_labels)\n",
    "\n",
    "inv_gen_ages = [(val * (max_age_filtered-min_age_filtered)) + min_age_filtered for val in ages.numpy().flatten()]\n",
    "\n",
    "print(\"Generated Ages:\")\n",
    "print(\"min: \", np.min(inv_gen_ages))\n",
    "print(\"mean: \", np.mean(inv_gen_ages))\n",
    "print(\"max: \", np.max(inv_gen_ages))\n",
    "print(\"stdv: \", np.std(inv_gen_ages))\n",
    "\n",
    "df_ages_class = final_df.query(\"ethnicity == 'African American'\")\n",
    "\n",
    "print(\"True Ages:\")\n",
    "print(\"min: \", np.min(df_ages_class.age))\n",
    "print(\"mean: \", np.mean(df_ages_class.age))\n",
    "print(\"max: \", np.max(df_ages_class.age))\n",
    "print(\"stdv: \", np.std(df_ages_class.age))\n",
    "\n",
    "\n",
    "sns.histplot(inv_gen_ages, bins=70, label='GAN', kde=True,)\n",
    "sns.histplot(df_ages_class.age, bins=70, color='orange', label='Truth', alpha=0.3, kde=True,)\n",
    "plt.title('African American Ages')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "df_temp = pd.DataFrame(columns = ['age', 'ethnicity'])\n",
    "\n",
    "df_temp['age'] = inv_gen_ages\n",
    "df_temp['ethnicity'] = 'African American'\n",
    "\n",
    "df_age_eth = df_age_eth.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "drwqfXpEuC73",
    "outputId": "8cb645a5-8072-42c3-d524-864c6ab6acb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating:  2010  ages for unit type: [0., 1., 0.]\n",
      "Generated Ages:\n",
      "min:  17.947638273239136\n",
      "mean:  64.27453704972692\n",
      "max:  88.85329866409302\n",
      "stdv:  17.289540586291043\n",
      "True Ages:\n",
      "min:  15\n",
      "mean:  64.44378109452737\n",
      "max:  89\n",
      "stdv:  17.41515533822681\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Generate n ages for a class\"\"\"\n",
    "print(\"Generating: \", 2010, \" ages for unit type: [0., 1., 0.]\")\n",
    "\n",
    "age_one_hot_labels = tf.repeat([[0., 1., 0.]],2010, axis=0)\n",
    "\n",
    "input_noise = tf.random.normal((2010, cgan.noise_dim), 0, 1)\n",
    "random_vector_labels = tf.concat([input_noise, age_one_hot_labels], axis=1)\n",
    "\n",
    "ages = cgan.generator(random_vector_labels)\n",
    "\n",
    "inv_gen_ages = [(val * (max_age_filtered-min_age_filtered)) + min_age_filtered for val in ages.numpy().flatten()]\n",
    "\n",
    "print(\"Generated Ages:\")\n",
    "print(\"min: \", np.min(inv_gen_ages))\n",
    "print(\"mean: \", np.mean(inv_gen_ages))\n",
    "print(\"max: \", np.max(inv_gen_ages))\n",
    "print(\"stdv: \", np.std(inv_gen_ages))\n",
    "\n",
    "df_ages_class = final_df.query(\"ethnicity == 'Caucasian'\")\n",
    "\n",
    "print(\"True Ages:\")\n",
    "print(\"min: \", np.min(df_ages_class.age))\n",
    "print(\"mean: \", np.mean(df_ages_class.age))\n",
    "print(\"max: \", np.max(df_ages_class.age))\n",
    "print(\"stdv: \", np.std(df_ages_class.age))\n",
    "\n",
    "\n",
    "sns.histplot(inv_gen_ages, bins=70, label='GAN', kde=True,)\n",
    "sns.histplot(df_ages_class.age, bins=70, color='orange', label='Truth', alpha=0.3, kde=True,)\n",
    "plt.title('Caucasian Ages')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "df_temp = pd.DataFrame(columns = ['age', 'ethnicity'])\n",
    "\n",
    "df_temp['age'] = inv_gen_ages\n",
    "df_temp['ethnicity'] = 'Caucasian'\n",
    "\n",
    "df_age_eth = df_age_eth.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "S1WvBkzMuC73",
    "outputId": "cdd165f8-af2a-4f5a-813e-f1b21a6b0d63"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethnicity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>African American</th>\n",
       "      <td>231.0</td>\n",
       "      <td>56.151515</td>\n",
       "      <td>16.861546</td>\n",
       "      <td>19.0</td>\n",
       "      <td>46.50</td>\n",
       "      <td>58.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caucasian</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>64.443781</td>\n",
       "      <td>17.419489</td>\n",
       "      <td>15.0</td>\n",
       "      <td>55.00</td>\n",
       "      <td>67.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Native American</th>\n",
       "      <td>12.0</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>20.331346</td>\n",
       "      <td>19.0</td>\n",
       "      <td>39.25</td>\n",
       "      <td>48.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     age                                                     \n",
       "                   count       mean        std   min    25%   50%   75%   max\n",
       "ethnicity                                                                    \n",
       "African American   231.0  56.151515  16.861546  19.0  46.50  58.0  69.0  90.0\n",
       "Caucasian         2010.0  64.443781  17.419489  15.0  55.00  67.0  78.0  89.0\n",
       "Native American     12.0  50.500000  20.331346  19.0  39.25  48.0  66.0  88.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ages.groupby('ethnicity').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "Z6odZ8N7uC73",
    "outputId": "dc8769b7-527f-4c12-fa25-51e05db3d3a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating:  12  ages for unit type: [0., 0., 1]\n",
      "Generated Ages:\n",
      "min:  32.07838091254234\n",
      "mean:  56.794130059580006\n",
      "max:  88.15487504005432\n",
      "stdv:  18.61921223570077\n",
      "True Ages:\n",
      "min:  19\n",
      "mean:  50.5\n",
      "max:  88\n",
      "stdv:  19.465781943365815\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Generate n ages for a class\"\"\"\n",
    "print(\"Generating: \", 12, \" ages for unit type: [0., 0., 1]\")\n",
    "\n",
    "age_one_hot_labels = tf.repeat([[0., 0., 1.]],12, axis=0)\n",
    "\n",
    "input_noise = tf.random.normal((12, cgan.noise_dim), 0, 1)\n",
    "random_vector_labels = tf.concat([input_noise, age_one_hot_labels], axis=1)\n",
    "\n",
    "ages = cgan.generator(random_vector_labels)\n",
    "\n",
    "inv_gen_ages = [(val * (max_age_filtered-min_age_filtered)) + min_age_filtered for val in ages.numpy().flatten()]\n",
    "\n",
    "print(\"Generated Ages:\")\n",
    "print(\"min: \", np.min(inv_gen_ages))\n",
    "print(\"mean: \", np.mean(inv_gen_ages))\n",
    "print(\"max: \", np.max(inv_gen_ages))\n",
    "print(\"stdv: \", np.std(inv_gen_ages))\n",
    "\n",
    "df_ages_class = final_df.query(\"ethnicity == 'Native American'\")\n",
    "\n",
    "print(\"True Ages:\")\n",
    "print(\"min: \", np.min(df_ages_class.age))\n",
    "print(\"mean: \", np.mean(df_ages_class.age))\n",
    "print(\"max: \", np.max(df_ages_class.age))\n",
    "print(\"stdv: \", np.std(df_ages_class.age))\n",
    "\n",
    "\n",
    "sns.histplot(inv_gen_ages, bins=70, label='GAN', kde=True,)\n",
    "sns.histplot(df_ages_class.age, bins=70, color='orange', label='Truth', alpha=0.3, kde=True,)\n",
    "plt.title('Native American Ages')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "df_temp = pd.DataFrame(columns = ['age', 'ethnicity'])\n",
    "\n",
    "df_temp['age'] = inv_gen_ages\n",
    "df_temp['ethnicity'] = 'Native American'\n",
    "\n",
    "df_age_eth = df_age_eth.append(df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQZFsKT8Azeb"
   },
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "o_RCa2NLuC73"
   },
   "outputs": [],
   "source": [
    "df_age_eth['data'] = 'GAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ORAQsVqANUe",
    "outputId": "a6a6ef0b-9083-4734-a4fa-41e9e633d012"
   },
   "outputs": [],
   "source": [
    "#!pip install table_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1MPPPuqfBtr-"
   },
   "outputs": [],
   "source": [
    "#https://pypi.org/project/table-evaluator/\n",
    "from table_evaluator import load_data, TableEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QV_bsxVsLrZ7",
    "outputId": "c08e8855-6c9c-4b0b-fc16-76dbd8af43fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>African American</th>\n",
       "      <th>Caucasian</th>\n",
       "      <th>Native American</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  ethnicity  African American  Caucasian  Native American\n",
       "0   87  Caucasian               0.0        1.0              0.0\n",
       "1   87  Caucasian               0.0        1.0              0.0\n",
       "2   76  Caucasian               0.0        1.0              0.0\n",
       "3   34  Caucasian               0.0        1.0              0.0\n",
       "4   61  Caucasian               0.0        1.0              0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "FLYDOPL0OfjT"
   },
   "outputs": [],
   "source": [
    "final_df['data'] = 'Truth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9nCeC_lDCXVD"
   },
   "outputs": [],
   "source": [
    "df_true = final_df[['age', 'ethnicity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "A2WD47BeCf4X"
   },
   "outputs": [],
   "source": [
    "table_evaluator = TableEvaluator(df_true, df_age_eth[['age', 'ethnicity']], cat_cols=['ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_rJ_8-0Dzbo",
    "outputId": "0fdc65de-92b4-40fc-f13a-4fee6bbe98a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dianam/.local/lib/python3.8/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier F1-scores and their Jaccard similarities::\n",
      "                             f1_real  f1_fake  jaccard_similarity\n",
      "index                                                            \n",
      "DecisionTreeClassifier_fake   0.8780   0.7938              0.7686\n",
      "DecisionTreeClassifier_real   0.8958   0.7738              0.7346\n",
      "LogisticRegression_fake       0.8847   0.8847              1.0000\n",
      "LogisticRegression_real       0.9047   0.9047              1.0000\n",
      "MLPClassifier_fake            0.8847   0.8847              1.0000\n",
      "MLPClassifier_real            0.9047   0.9047              1.0000\n",
      "RandomForestClassifier_fake   0.8780   0.7916              0.7721\n",
      "RandomForestClassifier_real   0.8980   0.7583              0.7019\n",
      "\n",
      "Privacy results:\n",
      "                                            result\n",
      "Duplicate rows between sets (real/fake)  (2232, 0)\n",
      "nearest neighbor mean                       0.0051\n",
      "nearest neighbor std                        0.0222\n",
      "\n",
      "Miscellaneous results:\n",
      "                                  Result\n",
      "Column Correlation Distance RMSE  0.0778\n",
      "Column Correlation distance MAE   0.0550\n",
      "\n",
      "Results:\n",
      "                                                result\n",
      "Basic statistics                                1.0000\n",
      "Correlation column correlations                    NaN\n",
      "Mean Correlation between fake and real columns  0.9986\n",
      "1 - MAPE Estimator results                      0.9392\n",
      "Similarity Score                                   NaN\n"
     ]
    }
   ],
   "source": [
    "table_evaluator.evaluate(target_col='ethnicity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHeuWOl2WK3Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "kR4ckeSWGmsI"
   },
   "outputs": [],
   "source": [
    "df_true.append(df_age_eth[['age', 'ethnicity']]).to_csv('age_eth_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcvPuMDXQD1r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MU4X0gtSDre7",
    "outputId": "1682fbda-5267-4834-d492-8cb33493b3a1"
   },
   "outputs": [],
   "source": [
    "#table_evaluator.visual_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qinl6TVMXvco"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_YE-sHpX932"
   },
   "source": [
    "**Distribute**\n",
    "https://www.tensorflow.org/tutorials/distribute/custom_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3R_tLpGX_SS",
    "outputId": "fcf1e70f-f0d5-4c8c-9f12-02c8faf0f59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "8HRYGzS9YrfI"
   },
   "outputs": [],
   "source": [
    "# Create a checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrswI7AVX_og",
    "outputId": "818555a7-79b7-40ed-e022-c2be86dc3916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator input dim:  53\n",
      "Dicrimination input dim:  4\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CommunicationImplementation.AUTO\n",
      "Number of devices: 2\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,545\n",
      "Trainable params: 4,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CommunicationImplementation.AUTO\n",
      "Number of devices: 2\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 53)]              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                3456      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 7,937\n",
      "Trainable params: 7,809\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#with mirrored_strategy.scope():\n",
    "dist_gan = ConditionalGAN(noise_dim=50,\n",
    "                 data_shape=1,\n",
    "                 num_classes=3, \n",
    "                 d_learning_rate=1e-6, \n",
    "                 g_learning_rate=1e-6, \n",
    "                 batch_size=32, \n",
    "                 start_epoch=0,\n",
    "                 verbose = True, distribute = True)\n",
    "\n",
    "  #loss_obj = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "  #loss = tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)\n",
    "\n",
    "dist_gan.compile(loss_fn=keras.losses.BinaryCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dnE59ztTX_og",
    "outputId": "fdff160f-1a91-4d0a-938f-02855df680b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 3/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 4/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 5/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 6/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6927\n",
      "Epoch 7/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 8/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 9/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 10/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 11/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 12/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 13/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 14/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 15/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 16/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 17/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 18/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 19/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 20/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 21/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 22/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 23/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 24/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 25/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 26/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 27/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 28/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 29/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 30/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 31/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 32/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 33/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 34/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 35/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 36/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 37/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 38/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 39/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 40/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 41/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 42/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 43/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 44/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 45/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 46/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6932\n",
      "Epoch 47/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 48/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 49/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 50/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 51/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 52/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 53/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6927\n",
      "Epoch 54/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 55/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 56/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 57/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 58/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 59/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 60/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 61/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 62/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 63/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 64/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 65/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 66/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 67/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 68/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 69/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 70/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 71/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 72/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 73/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 74/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 75/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 76/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 77/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 78/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 79/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 80/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6929\n",
      "Epoch 81/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 82/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 83/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 84/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 85/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 86/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 87/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 88/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 89/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 90/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 91/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 92/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 93/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 94/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 95/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 96/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 97/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6927\n",
      "Epoch 98/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 99/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 100/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6927\n",
      "Epoch 101/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 102/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 103/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 104/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 105/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 106/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 107/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 108/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6928\n",
      "Epoch 109/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 110/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 111/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6927\n",
      "Epoch 112/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 113/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 114/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 115/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 116/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 117/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 118/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 119/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 120/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 121/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6926\n",
      "Epoch 122/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 123/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 124/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 125/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 126/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 127/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 128/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 129/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 130/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 131/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 132/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 133/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 134/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6928\n",
      "Epoch 135/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 136/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6927\n",
      "Epoch 137/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 138/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 139/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 140/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 141/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 142/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 143/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 144/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 145/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 146/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 147/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 148/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 149/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6927\n",
      "Epoch 150/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 151/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 152/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 153/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 154/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 155/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 156/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 157/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 158/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 159/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6929\n",
      "Epoch 160/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 161/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 162/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 163/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 164/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 165/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 166/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 167/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 168/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 169/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 170/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 171/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6928 - d_loss: 0.6930\n",
      "Epoch 172/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 173/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 174/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 175/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 176/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 177/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 178/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 179/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 180/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 181/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 182/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 183/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 184/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6926\n",
      "Epoch 185/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6926\n",
      "Epoch 186/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 187/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 188/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 189/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 190/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 191/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 192/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 193/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6927\n",
      "Epoch 194/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 195/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 196/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6927\n",
      "Epoch 197/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 198/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 199/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 200/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 201/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 202/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 203/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 204/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 205/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 206/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 207/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 208/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 209/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 210/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 211/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 212/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 213/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 214/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 215/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 216/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 217/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 218/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 219/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 220/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 221/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 222/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6927\n",
      "Epoch 223/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 224/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 225/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 226/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 227/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 228/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 229/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 230/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 231/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 232/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 233/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6927\n",
      "Epoch 234/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 235/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 236/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 237/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 238/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 239/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 240/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 241/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 242/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 243/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 244/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 245/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 246/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 247/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 248/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 249/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 250/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6931\n",
      "Epoch 251/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 252/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 253/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 254/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 255/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 256/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 257/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 258/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 259/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 260/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 261/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 262/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 263/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 264/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 265/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 266/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 267/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 268/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 269/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 270/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 271/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 272/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 273/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 274/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 275/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 276/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 277/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 278/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6931\n",
      "Epoch 279/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6927\n",
      "Epoch 280/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 281/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 282/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6929\n",
      "Epoch 283/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 284/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 285/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6929\n",
      "Epoch 286/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 287/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 288/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 289/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 290/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 291/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 292/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 293/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 294/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 295/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 296/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 297/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 298/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 299/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 300/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 301/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 302/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 303/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6927\n",
      "Epoch 304/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 305/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 306/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 307/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 308/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6927\n",
      "Epoch 309/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 310/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 311/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6927\n",
      "Epoch 312/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 313/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 314/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 315/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 316/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 317/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 318/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 319/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 320/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 321/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 322/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 323/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6927\n",
      "Epoch 324/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 325/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 326/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 327/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 328/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 329/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 330/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6927\n",
      "Epoch 331/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 332/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 333/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 334/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 335/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 336/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 337/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 338/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 339/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 340/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 341/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 342/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 343/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 344/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 345/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 346/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 347/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 348/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 349/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 350/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 351/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6927\n",
      "Epoch 352/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 353/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 354/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 355/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 356/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 357/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 358/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 359/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 360/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6928\n",
      "Epoch 361/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 362/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 363/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6928\n",
      "Epoch 364/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 365/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 366/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 367/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 368/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6931\n",
      "Epoch 369/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 370/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 371/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 372/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 373/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 374/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 375/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 376/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 377/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 378/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6927\n",
      "Epoch 379/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 380/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6927 - d_loss: 0.6929\n",
      "Epoch 381/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 382/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6928\n",
      "Epoch 383/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 384/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 385/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 386/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 387/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 388/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 389/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 390/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 391/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 392/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 393/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 394/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 395/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 396/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 397/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 398/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 399/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6927\n",
      "Epoch 400/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 401/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 402/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 403/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 404/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 405/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 406/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 407/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6930\n",
      "Epoch 408/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 409/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6927\n",
      "Epoch 410/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 411/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 412/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 413/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6929\n",
      "Epoch 414/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 415/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 416/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 417/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 418/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 419/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 420/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 421/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 422/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 423/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 424/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 425/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 426/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 427/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 428/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 429/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 430/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 431/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 432/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6931\n",
      "Epoch 433/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6931\n",
      "Epoch 434/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 435/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 436/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 437/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 438/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 439/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6927\n",
      "Epoch 440/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 441/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 442/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 443/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 444/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 445/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 446/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 447/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 448/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 449/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 450/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 451/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 452/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6927\n",
      "Epoch 453/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 454/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 455/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 456/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 457/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 458/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 459/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 460/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 461/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 462/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 463/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 464/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 465/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 466/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 467/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 468/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 469/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 470/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 471/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928: 0s - g_loss: 0.6939 - d_loss: \n",
      "Epoch 472/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 473/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 474/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 475/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 476/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 477/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 478/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 479/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 480/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 481/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 482/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 483/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 484/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 485/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 486/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 487/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 488/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 489/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 490/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 491/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 492/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 493/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 494/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 495/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 496/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6927\n",
      "Epoch 497/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 498/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 499/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 500/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 501/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 502/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 503/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 504/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6929\n",
      "Epoch 505/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 506/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 507/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 508/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6927\n",
      "Epoch 509/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 510/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 511/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 512/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 513/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 514/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 515/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 516/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 517/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 518/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 519/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 520/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 521/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6929\n",
      "Epoch 522/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 523/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 524/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 525/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 526/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 527/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 528/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 529/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 530/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6927\n",
      "Epoch 531/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 532/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 533/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 534/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 535/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 536/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 537/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 538/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 539/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6927\n",
      "Epoch 540/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 541/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 542/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 543/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 544/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 545/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 546/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 547/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 548/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 549/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 550/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 551/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 552/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 553/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 554/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 555/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 556/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 557/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6927\n",
      "Epoch 558/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 559/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 560/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 561/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 562/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 563/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 564/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 565/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 566/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 567/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 568/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 569/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 570/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 571/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 572/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 573/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 574/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 575/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 576/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 577/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 578/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 579/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 580/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 581/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 582/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 583/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6932\n",
      "Epoch 584/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 585/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 586/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 587/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 588/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 589/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 590/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6927\n",
      "Epoch 591/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6928\n",
      "Epoch 592/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 593/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 594/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 595/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 596/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 597/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 598/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 599/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 600/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 601/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 602/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 603/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 604/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 605/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 606/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 607/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6931\n",
      "Epoch 608/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 609/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 610/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 611/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 612/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 613/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 614/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 615/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 616/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 617/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 618/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 619/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 620/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 621/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6928\n",
      "Epoch 622/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 623/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 624/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 625/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 626/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 627/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 628/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 629/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 630/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 631/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 632/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 633/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 634/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 635/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 636/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 637/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 638/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6928\n",
      "Epoch 639/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 640/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 641/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 642/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 643/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6927\n",
      "Epoch 644/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 645/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 646/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 647/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 648/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 649/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 650/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 651/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 652/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 653/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 654/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 655/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 656/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 657/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 658/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 659/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 660/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 661/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 662/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 663/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 664/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6927\n",
      "Epoch 665/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 666/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 667/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 668/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 669/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 670/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 671/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 672/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 673/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 674/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 675/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 676/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 677/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6927\n",
      "Epoch 678/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 679/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 680/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 681/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 682/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 683/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 684/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 685/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 686/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 687/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 688/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 689/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 690/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 691/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 692/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 693/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 694/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 695/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 696/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 697/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 698/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 699/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 700/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6930\n",
      "Epoch 701/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 702/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 703/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 704/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 705/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 706/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 707/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 708/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 709/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 710/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 711/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 712/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 713/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 714/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 715/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 716/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 717/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 718/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 719/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 720/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 721/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 722/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 723/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 724/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 725/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 726/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 727/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 728/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 729/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 730/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 731/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 732/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 733/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6928\n",
      "Epoch 734/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 735/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 736/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 737/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 738/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 739/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 740/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 741/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 742/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 743/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 744/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 745/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 746/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 747/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 748/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 749/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 750/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 751/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 752/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 753/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 754/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 755/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 756/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 757/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 758/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 759/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 760/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 761/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6928 - d_loss: 0.6930\n",
      "Epoch 762/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 763/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 764/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 765/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 766/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 767/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 768/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 769/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 770/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 771/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 772/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 773/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 774/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 775/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 776/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 777/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 778/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 779/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6932\n",
      "Epoch 780/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 781/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 782/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 783/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 784/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 785/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 786/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 787/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 788/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 789/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 790/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 791/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 792/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 793/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 794/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 795/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 796/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 797/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 798/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 799/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 800/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 801/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 802/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 803/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 804/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 805/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 806/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 807/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 808/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 809/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 810/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 811/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 812/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 813/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 814/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 815/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 816/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 817/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 818/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 819/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 820/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 821/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 822/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6928\n",
      "Epoch 823/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 824/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 825/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6928\n",
      "Epoch 826/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 827/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 828/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 829/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6927\n",
      "Epoch 830/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 831/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 832/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 833/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 834/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 835/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 836/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 837/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 838/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 839/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 840/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 841/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 842/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 843/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 844/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 845/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 846/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 847/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6929\n",
      "Epoch 848/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6928\n",
      "Epoch 849/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6928\n",
      "Epoch 850/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6927\n",
      "Epoch 851/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6928\n",
      "Epoch 852/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 853/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 854/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 855/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 856/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 857/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6928\n",
      "Epoch 858/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 859/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 860/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6928\n",
      "Epoch 861/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 862/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 863/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 864/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6931\n",
      "Epoch 865/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 866/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6929\n",
      "Epoch 867/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 868/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 869/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 870/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 871/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 872/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 873/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 874/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6927\n",
      "Epoch 875/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 876/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 877/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 878/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 879/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 880/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 881/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 882/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 883/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 884/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 885/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 886/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 887/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 888/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 889/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 890/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 891/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 892/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 893/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 894/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 895/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 896/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 897/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 898/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 899/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 900/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 901/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 902/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 903/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 904/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 905/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 906/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 907/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 908/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6927 - d_loss: 0.6929\n",
      "Epoch 909/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 910/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 911/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 912/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 913/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 914/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 915/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 916/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 917/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 918/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 919/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 920/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 921/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 922/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 923/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 924/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 925/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 926/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 927/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 928/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 929/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6928 - d_loss: 0.6928\n",
      "Epoch 930/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 931/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6928 - d_loss: 0.6930\n",
      "Epoch 932/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 933/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6928 - d_loss: 0.6929\n",
      "Epoch 934/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 935/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 936/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6927 - d_loss: 0.6928\n",
      "Epoch 937/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 938/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 939/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 940/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 941/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 942/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 943/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 944/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 945/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 946/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 947/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 948/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 949/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 950/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 951/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 952/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 953/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 954/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 955/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 956/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 957/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 958/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 959/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 960/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 961/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 962/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 963/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 964/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 965/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 966/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 967/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6929\n",
      "Epoch 968/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 969/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 970/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6927\n",
      "Epoch 971/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 972/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 973/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 974/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 975/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 976/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 977/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 978/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 979/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 980/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 981/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 982/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 983/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 984/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 985/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 986/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 987/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 988/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 989/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6927\n",
      "Epoch 990/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 991/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 992/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 993/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 994/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 995/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 996/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 997/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 998/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 999/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1000/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1001/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 1002/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1003/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6927\n",
      "Epoch 1004/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1005/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1006/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1007/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1008/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1009/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1010/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1011/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1012/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1013/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1014/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6927\n",
      "Epoch 1015/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1016/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1017/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1018/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 1019/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1020/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1021/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 1022/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1023/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 1024/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1025/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1026/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1027/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1028/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1029/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1030/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1031/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1032/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 1033/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 1034/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1035/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1036/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1037/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1038/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 1039/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1040/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1041/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1042/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1043/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1044/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1045/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1046/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1047/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1048/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1049/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1050/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1051/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1052/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 1053/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1054/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1055/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 1056/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1057/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1058/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1059/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6929\n",
      "Epoch 1060/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6929\n",
      "Epoch 1061/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6927\n",
      "Epoch 1062/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1063/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1064/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1065/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1066/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 1067/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1068/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1069/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1070/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1071/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1072/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1073/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1074/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 1075/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1076/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1077/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1078/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6929\n",
      "Epoch 1079/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 1080/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1081/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 1082/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1083/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1084/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1085/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1086/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1087/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1088/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1089/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 1090/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1091/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1092/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1093/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1094/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1095/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1096/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1097/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1098/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1099/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1100/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1101/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1102/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 1103/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1104/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1105/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1106/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1107/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1108/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6928\n",
      "Epoch 1109/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1110/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 1111/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1112/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1113/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1114/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1115/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1116/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1117/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1118/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1119/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1120/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1121/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1122/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1123/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1124/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1125/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1126/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1127/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1128/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1129/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 1130/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 1131/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1132/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 1133/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 1134/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1135/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1136/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1137/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1138/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 1139/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6928 - d_loss: 0.6929\n",
      "Epoch 1140/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1141/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 1142/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 1143/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1144/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 1145/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1146/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6928\n",
      "Epoch 1147/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6932\n",
      "Epoch 1148/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 1149/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1150/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1151/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 1152/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1153/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1154/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 1155/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1156/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1157/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1158/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1159/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 1160/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1161/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1162/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1163/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 1164/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1165/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 1166/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 1167/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 1168/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1169/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1170/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 1171/3000\n",
      "36/36 [==============================] - 0s 6ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1172/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6929\n",
      "Epoch 1173/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 1174/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1175/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1176/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1177/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1178/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1179/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1180/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1181/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 1182/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1183/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1184/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1185/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1186/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1187/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 1188/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1189/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1190/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1191/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1192/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1193/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1194/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1195/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1196/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1197/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1198/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1199/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6927\n",
      "Epoch 1200/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1201/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1202/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6931\n",
      "Epoch 1203/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1204/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1205/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1206/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1207/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1208/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1209/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1210/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1211/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1212/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1213/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1214/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1215/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1216/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 1217/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1218/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1219/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 1220/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 1221/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1222/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6931\n",
      "Epoch 1223/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1224/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1225/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1226/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1227/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1228/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1229/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1230/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1231/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1232/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1233/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1234/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1235/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1236/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1237/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1238/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1239/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1240/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1241/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1242/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1243/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1244/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1245/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1246/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 1247/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1248/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 1249/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1250/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 1251/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6928\n",
      "Epoch 1252/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1253/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1254/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 1255/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1256/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1257/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1258/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 1259/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1260/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1261/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 1262/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1263/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1264/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1265/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1266/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1267/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 1268/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1269/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1270/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1271/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1272/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1273/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1274/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1275/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6931\n",
      "Epoch 1276/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 1277/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 1278/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1279/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6930\n",
      "Epoch 1280/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1281/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1282/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1283/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1284/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1285/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1286/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1287/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1288/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1289/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1290/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1291/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1292/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1293/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 1294/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1295/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1296/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1297/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1298/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1299/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1300/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1301/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6927\n",
      "Epoch 1302/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1303/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1304/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1305/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1306/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 1307/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1308/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1309/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 1310/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1311/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 1312/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 1313/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1314/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1315/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6929\n",
      "Epoch 1316/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6927\n",
      "Epoch 1317/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 1318/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1319/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1320/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1321/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 1322/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1323/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1324/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1325/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 1326/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1327/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6928 - d_loss: 0.6930\n",
      "Epoch 1328/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 1329/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1330/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1331/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 1332/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1333/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1334/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1335/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6928 - d_loss: 0.6930\n",
      "Epoch 1336/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1337/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1338/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1339/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 1340/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1341/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1342/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1343/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 1344/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1345/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1346/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1347/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1348/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 1349/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1350/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 1351/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1352/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1353/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1354/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1355/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1356/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1357/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1358/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1359/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6927\n",
      "Epoch 1360/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1361/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1362/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1363/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1364/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1365/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1366/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1367/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1368/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1369/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1370/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1371/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1372/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6933\n",
      "Epoch 1373/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1374/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1375/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1376/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 1377/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1378/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1379/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1380/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1381/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1382/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1383/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1384/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1385/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1386/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 1387/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1388/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1389/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1390/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1391/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1392/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1393/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1394/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1395/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1396/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6931\n",
      "Epoch 1397/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6931\n",
      "Epoch 1398/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 1399/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1400/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1401/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1402/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1403/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1404/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1405/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1406/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1407/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1408/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 1409/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1410/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1411/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6929\n",
      "Epoch 1412/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 1413/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 1414/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1415/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1416/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1417/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1418/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1419/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1420/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1421/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1422/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1423/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 1424/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1425/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6930\n",
      "Epoch 1426/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1427/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1428/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1429/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1430/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6930\n",
      "Epoch 1431/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1432/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1433/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1434/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 1435/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1436/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1437/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1438/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1439/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1440/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1441/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1442/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1443/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1444/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1445/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1446/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1447/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1448/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1449/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1450/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1451/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1452/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1453/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1454/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1455/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1456/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1457/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1458/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1459/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 1460/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 1461/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1462/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1463/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1464/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1465/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1466/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1467/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1468/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1469/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1470/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 1471/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1472/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1473/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1474/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1475/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1476/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1477/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1478/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1479/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1480/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1481/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6929\n",
      "Epoch 1482/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1483/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1484/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1485/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1486/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1487/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1488/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1489/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1490/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1491/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1492/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6932\n",
      "Epoch 1493/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1494/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1495/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1496/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1497/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1498/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1499/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1500/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1501/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1502/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1503/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1504/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1505/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1506/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1507/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1508/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1509/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1510/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1511/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1512/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1513/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1514/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1515/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1516/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1517/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1518/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1519/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1520/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1521/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1522/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1523/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1524/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1525/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1526/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1527/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1528/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1529/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1530/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1531/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1532/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1533/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1534/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1535/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1536/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1537/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1538/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1539/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1540/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1541/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1542/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1543/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 1544/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1545/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1546/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1547/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1548/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1549/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1550/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1551/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1552/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1553/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1554/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 1555/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1556/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1557/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1558/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1559/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1560/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1561/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1562/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1563/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1564/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1565/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1566/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1567/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1568/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1569/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1570/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1571/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1572/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1573/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1574/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1575/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1576/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1577/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1578/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1579/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1580/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1581/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1582/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1583/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1584/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1585/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1586/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1587/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1588/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 1589/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1590/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 1591/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1592/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1593/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1594/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 1595/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 1596/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1597/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1598/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1599/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1600/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1601/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 1602/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1603/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1604/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1605/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1606/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 1607/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1608/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1609/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1610/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1611/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 1612/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1613/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1614/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1615/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6929\n",
      "Epoch 1616/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1617/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1618/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1619/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1620/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1621/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1622/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1623/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1624/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1625/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1626/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 1627/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1628/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1629/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1630/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1631/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1632/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1633/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1634/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1635/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6931\n",
      "Epoch 1636/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1637/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1638/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1639/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1640/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1641/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1642/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1643/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1644/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1645/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1646/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1647/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1648/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1649/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1650/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6931\n",
      "Epoch 1651/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1652/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1653/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1654/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1655/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1656/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1657/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1658/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1659/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1660/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 1661/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1662/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1663/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1664/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1665/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1666/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6927\n",
      "Epoch 1667/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1668/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1669/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1670/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6932\n",
      "Epoch 1671/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 1672/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1673/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1674/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1675/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1676/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6927\n",
      "Epoch 1677/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 1678/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1679/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1680/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1681/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1682/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 1683/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6927\n",
      "Epoch 1684/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1685/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1686/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1687/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1688/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1689/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1690/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1691/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1692/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1693/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1694/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1695/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1696/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 1697/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1698/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1699/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1700/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 1701/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1702/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1703/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1704/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1705/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1706/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1707/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 1708/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1709/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1710/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1711/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1712/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1713/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1714/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1715/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1716/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 1717/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1718/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1719/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1720/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1721/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1722/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6931\n",
      "Epoch 1723/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1724/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1725/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1726/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1727/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1728/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1729/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1730/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1731/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1732/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1733/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1734/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1735/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1736/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1737/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1738/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1739/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1740/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 1741/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 1742/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1743/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1744/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1745/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1746/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1747/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1748/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1749/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1750/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1751/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1752/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1753/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1754/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1755/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1756/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1757/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 1758/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1759/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1760/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1761/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1762/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1763/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1764/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1765/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1766/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1767/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1768/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1769/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1770/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1771/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1772/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1773/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6931\n",
      "Epoch 1774/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1775/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1776/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1777/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1778/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1779/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1780/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1781/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1782/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1783/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1784/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1785/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1786/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1787/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1788/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1789/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1790/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1791/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1792/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1793/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 1794/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1795/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1796/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1797/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1798/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1799/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1800/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1801/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1802/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1803/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1804/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1805/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1806/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1807/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1808/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1809/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1810/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 1811/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1812/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1813/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1814/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1815/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1816/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6931\n",
      "Epoch 1817/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1818/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6929\n",
      "Epoch 1819/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1820/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1821/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1822/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1823/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1824/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1825/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1826/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1827/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6932\n",
      "Epoch 1828/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1829/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 1830/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1831/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1832/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1833/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1834/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1835/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1836/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1837/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1838/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1839/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1840/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1841/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1842/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1843/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1844/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1845/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1846/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1847/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1848/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1849/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1850/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1851/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1852/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1853/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1854/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1855/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1856/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1857/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1858/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1859/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1860/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1861/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1862/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1863/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1864/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1865/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1866/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1867/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1868/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1869/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1870/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1871/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1872/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1873/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 1874/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1875/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1876/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1877/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1878/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1879/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1880/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 1881/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1882/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1883/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1884/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1885/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1886/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1887/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1888/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1889/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1890/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1891/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1892/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1893/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1894/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1895/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1896/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1897/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1898/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1899/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1900/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1901/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1902/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 1903/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1904/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1905/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 1906/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1907/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1908/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1909/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1910/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1911/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1912/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1913/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1914/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929: 0s - g_loss: 0.6933 - d_loss: \n",
      "Epoch 1915/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1916/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1917/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1918/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 1919/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 1920/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1921/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1922/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 1923/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1924/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1925/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 1926/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1927/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1928/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 1929/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1930/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1931/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1932/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1933/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1934/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1935/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1936/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1937/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1938/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1939/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 1940/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 1941/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 1942/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1943/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1944/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1945/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1946/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1947/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1948/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1949/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 1950/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 1951/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 1952/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 1953/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1954/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1955/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 1956/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1957/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1958/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6932\n",
      "Epoch 1959/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1960/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1961/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1962/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1963/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 1964/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1965/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6931\n",
      "Epoch 1966/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 1967/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1968/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1969/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1970/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1971/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1972/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1973/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1974/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1975/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 1976/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 1977/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 1978/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 1979/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 1980/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 1981/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1982/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1983/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 1984/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1985/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1986/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 1987/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1988/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1989/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1990/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 1991/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 1992/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 1993/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 1994/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 1995/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 1996/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 1997/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 1998/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 1999/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2000/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 2001/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2002/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2003/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2004/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2005/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2006/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2007/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2008/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2009/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2010/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2011/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2012/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2013/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2014/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2015/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 2016/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2017/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2018/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2019/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 2020/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2021/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2022/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2023/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2024/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2025/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 2026/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2027/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2028/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2029/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2030/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2031/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2032/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2033/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2034/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2035/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 2036/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6932\n",
      "Epoch 2037/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2038/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2039/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2040/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2041/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2042/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2043/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2044/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2045/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2046/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2047/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2048/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2049/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2050/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2051/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 2052/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2053/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2054/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 2055/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2056/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2057/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2058/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2059/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2060/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 2061/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2062/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2063/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2064/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2065/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2066/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2067/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 2068/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2069/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 2070/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 2071/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 2072/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2073/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2074/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2075/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2076/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2077/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 2078/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2079/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2080/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2081/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2082/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2083/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2084/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6931\n",
      "Epoch 2085/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2086/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2087/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2088/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2089/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2090/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2091/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2092/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2093/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2094/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2095/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2096/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2097/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 2098/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2099/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2100/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 2101/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2102/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2103/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2104/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2105/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2106/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2107/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2108/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2109/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2110/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2111/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2112/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2113/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2114/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2115/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2116/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2117/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2118/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2119/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2120/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2121/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2122/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2123/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2124/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6932\n",
      "Epoch 2125/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2126/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2127/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2128/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2129/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2130/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2131/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2132/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2133/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2134/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 2135/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2136/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2137/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2138/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2139/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2140/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2141/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 2142/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6928\n",
      "Epoch 2143/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2144/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2145/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2146/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2147/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2148/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2149/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2150/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6929\n",
      "Epoch 2151/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2152/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6932\n",
      "Epoch 2153/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2154/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2155/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2156/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2157/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6929\n",
      "Epoch 2158/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2159/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 2160/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6928\n",
      "Epoch 2161/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2162/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2163/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2164/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2165/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2166/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 2167/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 2168/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 2169/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2170/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2171/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2172/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2173/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2174/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2175/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2176/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2177/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2178/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 2179/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2180/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2181/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2182/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2183/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2184/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2185/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2186/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2187/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2188/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2189/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 2190/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 2191/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2192/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2193/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2194/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2195/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2196/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2197/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2198/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2199/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2200/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2201/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2202/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2203/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2204/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2205/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2206/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2207/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 2208/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2209/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 2210/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2211/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2212/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 2213/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2214/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2215/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2216/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2217/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2218/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2219/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2220/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2221/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2222/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2223/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2224/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2225/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2226/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2227/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2228/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2229/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2230/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2231/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2232/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2233/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2234/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2235/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6932\n",
      "Epoch 2236/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 2237/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2238/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2239/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2240/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 2241/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2242/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2243/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2244/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2245/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2246/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2247/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2248/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 2249/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2250/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2251/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2252/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2253/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2254/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2255/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2256/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2257/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2258/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2259/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2260/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2261/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2262/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2263/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2264/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2265/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2266/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2267/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2268/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2269/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2270/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 2271/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2272/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2273/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2274/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2275/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2276/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2277/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2278/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2279/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2280/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2281/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2282/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2283/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2284/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2285/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2286/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2287/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2288/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2289/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 2290/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2291/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2292/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2293/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2294/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2295/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2296/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2297/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2298/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2299/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2300/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2301/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2302/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2303/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2304/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2305/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2306/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2307/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2308/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2309/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2310/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2311/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2312/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2313/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2314/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2315/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2316/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2317/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2318/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 2319/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2320/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2321/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2322/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2323/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2324/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 2325/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2326/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2327/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 2328/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2329/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2330/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 2331/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2332/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 2333/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6931\n",
      "Epoch 2334/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2335/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2336/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2337/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2338/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 2339/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2340/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2341/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2342/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2343/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2344/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2345/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2346/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2347/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2348/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2349/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2350/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2351/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 2352/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2353/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2354/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2355/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2356/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2357/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2358/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2359/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2360/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2361/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2362/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2363/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2364/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2365/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2366/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2367/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2368/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2369/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2370/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2371/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2372/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2373/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2374/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2375/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2376/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2377/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2378/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2379/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2380/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2381/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2382/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2383/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2384/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 2385/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2386/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2387/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2388/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6931\n",
      "Epoch 2389/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2390/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2391/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6932\n",
      "Epoch 2392/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2393/3000\n",
      "36/36 [==============================] - 0s 2ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2394/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2395/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2396/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 2397/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 2398/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2399/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 2400/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2401/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2402/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2403/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2404/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2405/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2406/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 2407/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2408/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2409/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2410/3000\n",
      "36/36 [==============================] - 0s 2ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2411/3000\n",
      "36/36 [==============================] - 0s 2ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2412/3000\n",
      "36/36 [==============================] - 0s 2ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2413/3000\n",
      "36/36 [==============================] - 0s 2ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2414/3000\n",
      "36/36 [==============================] - 0s 2ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2415/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2416/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2417/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2418/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2419/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2420/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2421/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2422/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2423/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2424/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 2425/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2426/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2427/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2428/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2429/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2430/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2431/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6930\n",
      "Epoch 2432/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2433/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2434/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2435/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2436/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2437/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2438/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 2439/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2440/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2441/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 2442/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2443/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2444/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2445/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2446/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2447/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2448/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2449/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 2450/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2451/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2452/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2453/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2454/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2455/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2456/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2457/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2458/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2459/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6932\n",
      "Epoch 2460/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2461/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2462/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2463/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2464/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2465/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2466/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2467/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2468/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2469/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2470/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2471/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2472/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2473/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2474/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2475/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2476/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2477/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2478/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2479/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2480/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6931\n",
      "Epoch 2481/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2482/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2483/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2484/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2485/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2486/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2487/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2488/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 2489/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2490/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 2491/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2492/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2493/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2494/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2495/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2496/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2497/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2498/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 2499/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2500/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2501/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2502/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2503/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2504/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6931\n",
      "Epoch 2505/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2506/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 2507/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2508/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2509/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2510/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2511/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2512/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6928\n",
      "Epoch 2513/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2514/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2515/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2516/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2517/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2518/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2519/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2520/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2521/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2522/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2523/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2524/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2525/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2526/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2527/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2528/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2529/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2530/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2531/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2532/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2533/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2534/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2535/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2536/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 2537/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6928 - d_loss: 0.6929\n",
      "Epoch 2538/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2539/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2540/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 2541/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2542/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2543/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2544/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2545/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2546/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 2547/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2548/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 2549/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 2550/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 2551/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6929\n",
      "Epoch 2552/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2553/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2554/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2555/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2556/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2557/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 2558/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2559/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2560/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2561/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2562/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2563/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 2564/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2565/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2566/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2567/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2568/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2569/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2570/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2571/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2572/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2573/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2574/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 2575/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2576/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2577/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2578/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2579/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2580/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2581/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2582/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2583/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 2584/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2585/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 2586/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 2587/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2588/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2589/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2590/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2591/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2592/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2593/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2594/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2595/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2596/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2597/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2598/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2599/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2600/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2601/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2602/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6928 - d_loss: 0.6931\n",
      "Epoch 2603/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2604/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 2605/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2606/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2607/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2608/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2609/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2610/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2611/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2612/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2613/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2614/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2615/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2616/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2617/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2618/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2619/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2620/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2621/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2622/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2623/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2624/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2625/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2626/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2627/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6932\n",
      "Epoch 2628/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2629/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 2630/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2631/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2632/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2633/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2634/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2635/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2636/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 2637/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2638/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 2639/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2640/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2641/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6930\n",
      "Epoch 2642/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2643/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2644/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2645/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2646/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2647/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2648/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2649/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2650/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2651/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2652/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6931\n",
      "Epoch 2653/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2654/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2655/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2656/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2657/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2658/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2659/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2660/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2661/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2662/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2663/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2664/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2665/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6931\n",
      "Epoch 2666/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2667/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2668/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2669/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2670/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2671/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2672/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2673/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2674/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931: 0s - g_loss: 0.6932 - d_loss: 0.\n",
      "Epoch 2675/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2676/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 2677/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2678/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2679/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2680/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2681/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2682/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 2683/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2684/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2685/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2686/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2687/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2688/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2689/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2690/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2691/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2692/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2693/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2694/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2695/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2696/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2697/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2698/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2699/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2700/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 2701/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2702/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2703/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2704/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2705/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2706/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2707/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2708/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2709/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929: 0s - g_loss: 0.6937 - d_loss: \n",
      "Epoch 2710/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2711/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2712/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2713/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2714/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2715/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2716/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2717/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6929\n",
      "Epoch 2718/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2719/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2720/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2721/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2722/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2723/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2724/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2725/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2726/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2727/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2728/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2729/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2730/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2731/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2732/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2733/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2734/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2735/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6928\n",
      "Epoch 2736/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2737/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2738/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2739/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2740/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2741/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2742/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2743/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2744/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2745/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2746/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2747/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2748/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2749/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2750/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2751/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2752/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2753/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2754/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2755/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2756/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2757/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2758/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2759/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2760/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2761/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2762/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2763/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2764/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2765/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2766/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2767/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2768/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 2769/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2770/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 2771/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2772/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2773/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2774/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2775/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2776/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2777/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2778/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2779/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2780/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2781/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2782/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2783/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2784/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2785/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2786/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2787/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2788/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2789/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2790/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2791/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6931\n",
      "Epoch 2792/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2793/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2794/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2795/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2796/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2797/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2798/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2799/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2800/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2801/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2802/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2803/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2804/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2805/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6928\n",
      "Epoch 2806/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2807/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2808/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6931\n",
      "Epoch 2809/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2810/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6929\n",
      "Epoch 2811/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2812/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2813/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6932\n",
      "Epoch 2814/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2815/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2816/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6932\n",
      "Epoch 2817/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2818/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2819/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2820/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2821/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2822/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2823/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 2824/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6928\n",
      "Epoch 2825/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2826/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2827/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6932\n",
      "Epoch 2828/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2829/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2830/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2831/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2832/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2833/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2834/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2835/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2836/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2837/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2838/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2839/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2840/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2841/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2842/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2843/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2844/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2845/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2846/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2847/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2848/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2849/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2850/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2851/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2852/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6928 - d_loss: 0.6931\n",
      "Epoch 2853/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6930\n",
      "Epoch 2854/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2855/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2856/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2857/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2858/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2859/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2860/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2861/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2862/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2863/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2864/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2865/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6929 - d_loss: 0.6929\n",
      "Epoch 2866/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2867/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2868/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2869/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2870/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2871/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2872/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2873/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2874/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2875/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2876/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2877/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2878/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2879/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2880/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2881/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2882/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2883/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2884/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2885/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2886/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2887/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2888/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2889/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2890/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2891/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2892/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2893/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2894/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2895/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2896/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2897/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2898/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2899/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2900/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6930\n",
      "Epoch 2901/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2902/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2903/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2904/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2905/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2906/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2907/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2908/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2909/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2910/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2911/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2912/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2913/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2914/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6928\n",
      "Epoch 2915/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2916/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2917/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6931\n",
      "Epoch 2918/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6931\n",
      "Epoch 2919/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2920/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2921/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2922/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2923/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2924/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2925/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2926/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2927/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2928/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2929/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2930/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2931/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2932/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2933/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2934/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2935/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2936/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2937/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2938/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2939/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2940/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2941/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2942/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2943/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2944/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6930\n",
      "Epoch 2945/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2946/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2947/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2948/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2949/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2950/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2951/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2952/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2953/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2954/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2955/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2956/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2957/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2958/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2959/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2960/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6930\n",
      "Epoch 2961/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2962/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2963/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6931\n",
      "Epoch 2964/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6932\n",
      "Epoch 2965/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2966/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2967/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2968/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6930\n",
      "Epoch 2969/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2970/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2971/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6931\n",
      "Epoch 2972/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2973/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2974/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6928 - d_loss: 0.6930\n",
      "Epoch 2975/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6929\n",
      "Epoch 2976/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6931\n",
      "Epoch 2977/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6930 - d_loss: 0.6929\n",
      "Epoch 2978/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6929\n",
      "Epoch 2979/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2980/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2981/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2982/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6929\n",
      "Epoch 2983/3000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6933 - d_loss: 0.6930\n",
      "Epoch 2984/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2985/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2986/3000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6931 - d_loss: 0.6932\n",
      "Epoch 2987/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2988/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2989/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6928\n",
      "Epoch 2990/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6931: 0s - g_loss: 0.6930 - d_loss: \n",
      "Epoch 2991/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6929\n",
      "Epoch 2992/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2993/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6929\n",
      "Epoch 2994/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6930 - d_loss: 0.6931\n",
      "Epoch 2995/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2996/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2997/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6931 - d_loss: 0.6930\n",
      "Epoch 2998/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 2999/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6930\n",
      "Epoch 3000/3000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8840157be0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_gan.fit(dataset, epochs=3000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "yIGzgl7VX_og"
   },
   "outputs": [],
   "source": [
    "dist_age_eth = pd.DataFrame(\n",
    "    columns = ['age', 'ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "65I9E3fmX_og"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [age, ethnicity]\n",
       "Index: []"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_age_eth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Uf8k3MAKX_og"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [age, ethnicity]\n",
       "Index: []"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_age_eth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "S441_DRCX_og"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating:  231  ages for unit type: [1., 0., 0.]\n",
      "Generated Ages:\n",
      "min:  17.409628711640835\n",
      "mean:  55.75918224118489\n",
      "max:  84.76877355575562\n",
      "stdv:  14.850852962529551\n",
      "True Ages:\n",
      "min:  19\n",
      "mean:  56.15151515151515\n",
      "max:  90\n",
      "stdv:  16.825009339158104\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Generate n ages for a class\"\"\"\n",
    "print(\"Generating: \", 231, \" ages for unit type: [1., 0., 0.]\")\n",
    "\n",
    "age_one_hot_labels = tf.repeat([[1., 0., 0.]],231, axis=0)\n",
    "\n",
    "input_noise = tf.random.normal((231, dist_gan.noise_dim), 0, 1)\n",
    "random_vector_labels = tf.concat([input_noise, age_one_hot_labels], axis=1)\n",
    "\n",
    "ages = dist_gan.generator(random_vector_labels)\n",
    "\n",
    "inv_gen_ages = [(val * (max_age_filtered-min_age_filtered)) + min_age_filtered for val in ages.numpy().flatten()]\n",
    "\n",
    "print(\"Generated Ages:\")\n",
    "print(\"min: \", np.min(inv_gen_ages))\n",
    "print(\"mean: \", np.mean(inv_gen_ages))\n",
    "print(\"max: \", np.max(inv_gen_ages))\n",
    "print(\"stdv: \", np.std(inv_gen_ages))\n",
    "\n",
    "df_ages_class = final_df.query(\"ethnicity == 'African American'\")\n",
    "\n",
    "print(\"True Ages:\")\n",
    "print(\"min: \", np.min(df_ages_class.age))\n",
    "print(\"mean: \", np.mean(df_ages_class.age))\n",
    "print(\"max: \", np.max(df_ages_class.age))\n",
    "print(\"stdv: \", np.std(df_ages_class.age))\n",
    "\n",
    "\n",
    "sns.histplot(inv_gen_ages, bins=70, label='GAN', kde=True,)\n",
    "sns.histplot(df_ages_class.age, bins=70, color='orange', label='Truth', alpha=0.3, kde=True,)\n",
    "plt.title('African American Ages')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "df_temp = pd.DataFrame(columns = ['age', 'ethnicity'])\n",
    "\n",
    "df_temp['age'] = inv_gen_ages\n",
    "df_temp['ethnicity'] = 'African American'\n",
    "\n",
    "dist_age_eth = dist_age_eth.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "wt0QFiQrX_og"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating:  2010  ages for unit type: [0., 1., 0.]\n",
      "Generated Ages:\n",
      "min:  15.619512248784304\n",
      "mean:  64.35516843697455\n",
      "max:  88.61874413490295\n",
      "stdv:  16.687965692425227\n",
      "True Ages:\n",
      "min:  15\n",
      "mean:  64.44378109452737\n",
      "max:  89\n",
      "stdv:  17.41515533822681\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Generate n ages for a class\"\"\"\n",
    "print(\"Generating: \", 2010, \" ages for unit type: [0., 1., 0.]\")\n",
    "\n",
    "age_one_hot_labels = tf.repeat([[0., 1., 0.]],2010, axis=0)\n",
    "\n",
    "input_noise = tf.random.normal((2010, dist_gan.noise_dim), 0, 1)\n",
    "random_vector_labels = tf.concat([input_noise, age_one_hot_labels], axis=1)\n",
    "\n",
    "ages = dist_gan.generator(random_vector_labels)\n",
    "\n",
    "inv_gen_ages = [(val * (max_age_filtered-min_age_filtered)) + min_age_filtered for val in ages.numpy().flatten()]\n",
    "\n",
    "print(\"Generated Ages:\")\n",
    "print(\"min: \", np.min(inv_gen_ages))\n",
    "print(\"mean: \", np.mean(inv_gen_ages))\n",
    "print(\"max: \", np.max(inv_gen_ages))\n",
    "print(\"stdv: \", np.std(inv_gen_ages))\n",
    "\n",
    "df_ages_class = final_df.query(\"ethnicity == 'Caucasian'\")\n",
    "\n",
    "print(\"True Ages:\")\n",
    "print(\"min: \", np.min(df_ages_class.age))\n",
    "print(\"mean: \", np.mean(df_ages_class.age))\n",
    "print(\"max: \", np.max(df_ages_class.age))\n",
    "print(\"stdv: \", np.std(df_ages_class.age))\n",
    "\n",
    "\n",
    "sns.histplot(inv_gen_ages, bins=70, label='GAN', kde=True,)\n",
    "sns.histplot(df_ages_class.age, bins=70, color='orange', label='Truth', alpha=0.3, kde=True,)\n",
    "plt.title('Caucasian Ages')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "df_temp = pd.DataFrame(columns = ['age', 'ethnicity'])\n",
    "\n",
    "df_temp['age'] = inv_gen_ages\n",
    "df_temp['ethnicity'] = 'Caucasian'\n",
    "\n",
    "dist_age_eth = dist_age_eth.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "iZTj1k1zX_og"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethnicity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>African American</th>\n",
       "      <td>231.0000</td>\n",
       "      <td>56.1515</td>\n",
       "      <td>16.8615</td>\n",
       "      <td>19.0000</td>\n",
       "      <td>46.5000</td>\n",
       "      <td>58.0000</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>90.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caucasian</th>\n",
       "      <td>2,010.0000</td>\n",
       "      <td>64.4438</td>\n",
       "      <td>17.4195</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>55.0000</td>\n",
       "      <td>67.0000</td>\n",
       "      <td>78.0000</td>\n",
       "      <td>89.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Native American</th>\n",
       "      <td>12.0000</td>\n",
       "      <td>50.5000</td>\n",
       "      <td>20.3313</td>\n",
       "      <td>19.0000</td>\n",
       "      <td>39.2500</td>\n",
       "      <td>48.0000</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>88.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        age                                                  \\\n",
       "                      count    mean     std     min     25%     50%     75%   \n",
       "ethnicity                                                                     \n",
       "African American   231.0000 56.1515 16.8615 19.0000 46.5000 58.0000 69.0000   \n",
       "Caucasian        2,010.0000 64.4438 17.4195 15.0000 55.0000 67.0000 78.0000   \n",
       "Native American     12.0000 50.5000 20.3313 19.0000 39.2500 48.0000 66.0000   \n",
       "\n",
       "                          \n",
       "                     max  \n",
       "ethnicity                 \n",
       "African American 90.0000  \n",
       "Caucasian        89.0000  \n",
       "Native American  88.0000  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ages.groupby('ethnicity').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "hmpSbhAeX_og"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating:  12  ages for unit type: [0., 0., 1]\n",
      "Generated Ages:\n",
      "min:  17.959401726722717\n",
      "mean:  45.437550408144794\n",
      "max:  66.76269745826721\n",
      "stdv:  16.083163984885793\n",
      "True Ages:\n",
      "min:  19\n",
      "mean:  50.5\n",
      "max:  88\n",
      "stdv:  19.465781943365815\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Generate n ages for a class\"\"\"\n",
    "print(\"Generating: \", 12, \" ages for unit type: [0., 0., 1]\")\n",
    "\n",
    "age_one_hot_labels = tf.repeat([[0., 0., 1.]],12, axis=0)\n",
    "\n",
    "input_noise = tf.random.normal((12, dist_gan.noise_dim), 0, 1)\n",
    "random_vector_labels = tf.concat([input_noise, age_one_hot_labels], axis=1)\n",
    "\n",
    "ages = dist_gan.generator(random_vector_labels)\n",
    "\n",
    "inv_gen_ages = [(val * (max_age_filtered-min_age_filtered)) + min_age_filtered for val in ages.numpy().flatten()]\n",
    "\n",
    "print(\"Generated Ages:\")\n",
    "print(\"min: \", np.min(inv_gen_ages))\n",
    "print(\"mean: \", np.mean(inv_gen_ages))\n",
    "print(\"max: \", np.max(inv_gen_ages))\n",
    "print(\"stdv: \", np.std(inv_gen_ages))\n",
    "\n",
    "df_ages_class = final_df.query(\"ethnicity == 'Native American'\")\n",
    "\n",
    "print(\"True Ages:\")\n",
    "print(\"min: \", np.min(df_ages_class.age))\n",
    "print(\"mean: \", np.mean(df_ages_class.age))\n",
    "print(\"max: \", np.max(df_ages_class.age))\n",
    "print(\"stdv: \", np.std(df_ages_class.age))\n",
    "\n",
    "\n",
    "sns.histplot(inv_gen_ages, bins=70, label='GAN', kde=True,)\n",
    "sns.histplot(df_ages_class.age, bins=70, color='orange', label='Truth', alpha=0.3, kde=True,)\n",
    "plt.title('Native American Ages')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "df_temp = pd.DataFrame(columns = ['age', 'ethnicity'])\n",
    "\n",
    "df_temp['age'] = inv_gen_ages\n",
    "df_temp['ethnicity'] = 'Native American'\n",
    "\n",
    "dist_age_eth = dist_age_eth.append(df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_my3XYUTX_og"
   },
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "fVARAPJWX_og"
   },
   "outputs": [],
   "source": [
    "dist_age_eth['data'] = 'GAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "jtjQKBkjX_og"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: table_evaluator in /home/dianam/.local/lib/python3.8/site-packages (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /home/dianam/.local/lib/python3.8/site-packages (from table_evaluator) (0.24.2)\n",
      "Requirement already satisfied: seaborn in /home/dianam/.local/lib/python3.8/site-packages (from table_evaluator) (0.11.1)\n",
      "Requirement already satisfied: numpy in /home/dianam/.local/lib/python3.8/site-packages (from table_evaluator) (1.22.3)\n",
      "Requirement already satisfied: dython==0.5.1 in /home/dianam/.local/lib/python3.8/site-packages (from table_evaluator) (0.5.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from table_evaluator) (5.8.0)\n",
      "Requirement already satisfied: matplotlib in /home/dianam/.local/lib/python3.8/site-packages (from table_evaluator) (3.4.2)\n",
      "Requirement already satisfied: pandas in /home/dianam/.local/lib/python3.8/site-packages (from table_evaluator) (1.2.4)\n",
      "Requirement already satisfied: scipy in /home/dianam/.local/lib/python3.8/site-packages (from table_evaluator) (1.6.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from table_evaluator) (4.56.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->table_evaluator) (8.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/dianam/.local/lib/python3.8/site-packages (from matplotlib->table_evaluator) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib->table_evaluator) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dianam/.local/lib/python3.8/site-packages (from matplotlib->table_evaluator) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->table_evaluator) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->table_evaluator) (2021.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/dianam/.local/lib/python3.8/site-packages (from scikit-learn->table_evaluator) (0.11)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/dianam/.local/lib/python3.8/site-packages (from scikit-learn->table_evaluator) (2.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->table_evaluator) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.2; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install table_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "CnBPATxiX_og"
   },
   "outputs": [],
   "source": [
    "#https://pypi.org/project/table-evaluator/\n",
    "from table_evaluator import load_data, TableEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "RrXR-R5qX_og"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>African American</th>\n",
       "      <th>Caucasian</th>\n",
       "      <th>Native American</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Truth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Truth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Truth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Truth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Truth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  ethnicity  African American  Caucasian  Native American   data\n",
       "0   87  Caucasian            0.0000     1.0000           0.0000  Truth\n",
       "1   87  Caucasian            0.0000     1.0000           0.0000  Truth\n",
       "2   76  Caucasian            0.0000     1.0000           0.0000  Truth\n",
       "3   34  Caucasian            0.0000     1.0000           0.0000  Truth\n",
       "4   61  Caucasian            0.0000     1.0000           0.0000  Truth"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "_Zo_XwhtX_oh"
   },
   "outputs": [],
   "source": [
    "final_df['data'] = 'Truth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "v1CiCtFnX_oh"
   },
   "outputs": [],
   "source": [
    "df_true = final_df[['age', 'ethnicity']].reset_index().drop('index', axis = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_age_eth.reset_index(level = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.5996</td>\n",
       "      <td>African American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.2825</td>\n",
       "      <td>African American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.3859</td>\n",
       "      <td>African American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.9641</td>\n",
       "      <td>African American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72.0217</td>\n",
       "      <td>African American</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age         ethnicity\n",
       "0 75.5996  African American\n",
       "1 46.2825  African American\n",
       "2 77.3859  African American\n",
       "3 24.9641  African American\n",
       "4 72.0217  African American"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_age_eth[['age', 'ethnicity']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>Caucasian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>Caucasian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76</td>\n",
       "      <td>Caucasian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>Caucasian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>Caucasian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  ethnicity\n",
       "0   87  Caucasian\n",
       "1   87  Caucasian\n",
       "2   76  Caucasian\n",
       "3   34  Caucasian\n",
       "4   61  Caucasian"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "PslivqmPX_oh"
   },
   "outputs": [],
   "source": [
    "table_evaluator = TableEvaluator(df_true, dist_age_eth[['age', 'ethnicity']], cat_cols=['ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "e_rMRfKMX_oh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dianam/.local/lib/python3.8/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier F1-scores and their Jaccard similarities::\n",
      "                             f1_real  f1_fake  jaccard_similarity\n",
      "index                                                            \n",
      "DecisionTreeClassifier_fake   0.8625   0.8049              0.8185\n",
      "DecisionTreeClassifier_real   0.9002   0.7960              0.7652\n",
      "LogisticRegression_fake       0.8714   0.8714              1.0000\n",
      "LogisticRegression_real       0.9047   0.9047              1.0000\n",
      "MLPClassifier_fake            0.8714   0.8714              1.0000\n",
      "MLPClassifier_real            0.9047   0.9047              1.0000\n",
      "RandomForestClassifier_fake   0.8692   0.8004              0.8259\n",
      "RandomForestClassifier_real   0.9002   0.8160              0.8040\n",
      "\n",
      "Privacy results:\n",
      "                                            result\n",
      "Duplicate rows between sets (real/fake)  (2232, 0)\n",
      "nearest neighbor mean                       0.0031\n",
      "nearest neighbor std                        0.0293\n",
      "\n",
      "Miscellaneous results:\n",
      "                                  Result\n",
      "Column Correlation Distance RMSE  0.0149\n",
      "Column Correlation distance MAE   0.0106\n",
      "\n",
      "Results:\n",
      "                                                result\n",
      "Basic statistics                                1.0000\n",
      "Correlation column correlations                    NaN\n",
      "Mean Correlation between fake and real columns  0.9992\n",
      "1 - MAPE Estimator results                      0.9556\n",
      "Similarity Score                                   NaN\n"
     ]
    }
   ],
   "source": [
    "table_evaluator.evaluate(target_col='ethnicity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "3OG8rqpLX_oh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_age_eth[['age', 'ethnicity']].reset_index().drop('index', axis = True).index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "IqflnNEIX_oh"
   },
   "outputs": [],
   "source": [
    "df_true.append(dist_age_eth[['age', 'ethnicity']]).to_csv(path_prefix + 'dist_age_eth_output_DM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "rGytAGNOX_oh"
   },
   "outputs": [],
   "source": [
    "#dist_age_eth[['age', 'ethnicity']].reset_index(level = 0, inplace = True)#.drop('index', axis = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [age, ethnicity]\n",
       "Index: []"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_age_eth[['age', 'ethnicity']][dist_age_eth[['age', 'ethnicity']].index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [age, ethnicity]\n",
       "Index: []"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true[df_true.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>75.5996</td>\n",
       "      <td>African American</td>\n",
       "      <td>GAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>46.2825</td>\n",
       "      <td>African American</td>\n",
       "      <td>GAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>77.3859</td>\n",
       "      <td>African American</td>\n",
       "      <td>GAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>24.9641</td>\n",
       "      <td>African American</td>\n",
       "      <td>GAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>72.0217</td>\n",
       "      <td>African American</td>\n",
       "      <td>GAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     age         ethnicity data\n",
       "0      0 75.5996  African American  GAN\n",
       "1      1 46.2825  African American  GAN\n",
       "2      2 77.3859  African American  GAN\n",
       "3      3 24.9641  African American  GAN\n",
       "4      4 72.0217  African American  GAN"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_age_eth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "03o0aJCWX_oh"
   },
   "outputs": [],
   "source": [
    "#table_evaluator.visual_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "age_ethnicity_gan_DF.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "34b722ad9db4b3bab2fc22ea7ad4e4dafa3a71a0a38143cc4f1aba484bf1fe4b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
