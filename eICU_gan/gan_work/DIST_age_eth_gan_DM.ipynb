{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "c_5e8_VcuC7r"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-td63pu6r because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, LeakyReLU, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc5PSNlfuJ3w",
    "outputId": "465f558f-ba2c-49df-d868-911343f86e5b"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0qzXBK53uC7t"
   },
   "outputs": [],
   "source": [
    "#https://keras.io/examples/generative/conditional_gan/#interpolating-between-classes-with-the-trained-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "K0Kj1O1juC7u"
   },
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, noise_dim=50, \n",
    "                 data_shape=1,\n",
    "                 num_classes=3, \n",
    "                 d_learning_rate=1e-5, \n",
    "                 g_learning_rate=1e-6, \n",
    "                 batch_size=64, \n",
    "                 start_epoch=0,\n",
    "                 verbose = False, \n",
    "                 distribute = False):\n",
    "\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.data_shape = data_shape # output shape of the generator and goes to discriminator\n",
    "        self.num_classes = num_classes\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(d_learning_rate)\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(g_learning_rate)\n",
    "        self.batch_size = batch_size\n",
    "        self.start_epoch = start_epoch\n",
    "        self.verbose = verbose\n",
    "        self.distribute = distribute\n",
    "\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "        # add number of class labels to the input channels for generator\n",
    "        self.g_dim = self.noise_dim + self.num_classes\n",
    "\n",
    "        # add the number of class labels to the input to the discriminator\n",
    "        self.d_dim = self.data_shape + self.num_classes\n",
    "\n",
    "        if (self.verbose):\n",
    "            print(\"Generator input dim: \", self.g_dim)\n",
    "            print(\"Dicrimination input dim: \", self.d_dim)\n",
    "\n",
    "\n",
    "        # build generator and discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "    \n",
    "    def build_generator(self):\n",
    "        \"Build the generator model\"\n",
    "        inputs = Input(shape=(self.g_dim,))\n",
    "        hidden = Dense(64)(inputs) # 128\n",
    "        hidden = LeakyReLU(alpha=0.2)(hidden)\n",
    "        hidden = BatchNormalization()(hidden)\n",
    "        hidden = Dense(64)(hidden)\n",
    "        hidden = LeakyReLU(alpha=0.2)(hidden)\n",
    "\n",
    "        output = Dense(self.data_shape, activation=\"sigmoid\")(hidden)\n",
    "        #output = Dense(self.data_shape, activation=\"linear\")(hidden)\n",
    "\n",
    "        if self.distribute:\n",
    "          mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() #tf.distribute.MirroredStrategy()\n",
    "          with mirrored_strategy.scope():\n",
    "\n",
    "            generator = Model(inputs=inputs, outputs=output, name=\"generator\")\n",
    "            print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))\n",
    "        \n",
    "        else: \n",
    "          generator = Model(inputs=inputs, outputs=output, name=\"generator\")\n",
    "\n",
    "        generator.summary()\n",
    "        return generator\n",
    "\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        \"build the discriminator model\"\n",
    "        d_inputs = Input(shape=(self.d_dim,))\n",
    "        h = Dense(64, input_shape=(self.g_dim,))(d_inputs) \n",
    "        h = LeakyReLU(alpha=0.2)(h)\n",
    "        h = Dropout(0.1)(h)\n",
    "        h = Dense(64)(h) #32\n",
    "        h = LeakyReLU(alpha=0.2)(h)\n",
    "        h = Dropout(0.1)(h)\n",
    "        h = Dense(1, activation=\"sigmoid\")(h)\n",
    "\n",
    "        if self.distribute:\n",
    "          mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() #tf.distribute.MirroredStrategy()\n",
    "          with mirrored_strategy.scope():\n",
    "            discriminator = Model(d_inputs, h, name=\"discriminator\")\n",
    "            print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))\n",
    "        else:\n",
    "          discriminator = Model(d_inputs, h, name=\"discriminator\")\n",
    "\n",
    "        discriminator.summary()\n",
    "        return discriminator\n",
    "\n",
    "    def compile(self, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_ages, one_hot_labels = data\n",
    "        real_ages = tf.cast(real_ages, tf.float32)\n",
    "        #print(\"real_ages:\", real_ages[0:2])\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        age_one_hot_labels = one_hot_labels[:, None]\n",
    "        print(\"age_one_hot_labels1:\", age_one_hot_labels[0:2])\n",
    "\n",
    "        age_one_hot_labels = tf.repeat(age_one_hot_labels, repeats=[1])\n",
    "        print(\"age_one_hot_labels2:\", age_one_hot_labels[0:2])\n",
    "\n",
    "        age_one_hot_labels = tf.reshape(age_one_hot_labels, (-1, self.num_classes))\n",
    "        age_one_hot_labels = tf.cast(age_one_hot_labels, tf.float32)\n",
    "        print(\"age_one_hot_labels3:\", age_one_hot_labels[0:2])\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_ages)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.noise_dim))\n",
    "        random_vector_labels = tf.concat([random_latent_vectors, one_hot_labels], axis=1)\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake ages.\n",
    "        generated_ages = self.generator(random_vector_labels)\n",
    "        generated_ages= tf.cast(generated_ages, tf.float32)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here. and tf.concat is on the last dimension (-1)\n",
    "        fake_ages_and_labels = tf.concat([generated_ages, age_one_hot_labels], -1)\n",
    "        real_ages_and_labels = tf.concat([real_ages, age_one_hot_labels], -1) \n",
    "        combined_ages = tf.concat([fake_ages_and_labels, real_ages_and_labels], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake ages. 1 == fake, 0 == real\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # TODO: concerned that observations are ordered fake then real - do we want to concat, then shuffle, then separate?\n",
    "        \n",
    "        #labels = tf.random.shuffle(labels, seed = 24)\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_ages)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.t\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.noise_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_ages = self.generator(random_vector_labels)\n",
    "            fake_ages_and_labels = tf.concat([fake_ages, age_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_ages_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def generate(self, n=1000, one_hot_label=[1., 0., 0.]):\n",
    "        \"\"\"Generate n ages for a class\"\"\"\n",
    "        print(\"Generating: \", n, \" ages for unit type: \", one_hot_label)\n",
    "        input_noise = tf.random.normal((n, self.noise_dim), 0, 1)\n",
    "        random_vector_labels = tf.concat([input_noise, one_hot_label], axis=1)\n",
    "\n",
    "        ages = self.generator(random_vector_labels)\n",
    "\n",
    "        return ages.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dianam/Documents/data_science/PlayGround/DS6050/eICU_gan/gan_work\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "path_prefix = ''\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    #path_prefix = '/content/drive/My Drive/eICU_gan/eICU_gan_data.db'\n",
    "    path_prefix = '/sfs/qumulo/qhome/dmf4ns/DS6050/'\n",
    "\n",
    "else:\n",
    "    path_prefix = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4wxEV4vouC7x"
   },
   "outputs": [],
   "source": [
    "ages_ethnicity_np = np.load(path_prefix + \"eICU_age_ethnicity.npy\", allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZmKz22HuC7y",
    "outputId": "5ac478bd-32e3-4a7f-f00e-a83cf9e7ef57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(87,), ('Caucasian',)],\n",
       "       [(87,), ('Caucasian',)],\n",
       "       [(76,), ('Caucasian',)],\n",
       "       ...,\n",
       "       [(41,), ('African American',)],\n",
       "       [(72,), ('Caucasian',)],\n",
       "       [(50,), ('Caucasian',)]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages_ethnicity_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIWt5ZbvuC7y",
    "outputId": "5eac12d1-75ee-4e31-b3c6-5d0d45998da2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethnicity length:  2253\n"
     ]
    }
   ],
   "source": [
    "ethnicity_np = np.asarray(ages_ethnicity_np[:,1].flatten().tolist()).flatten()\n",
    "print('ethnicity length: ', len(ethnicity_np))\n",
    "#print(ethnicity_np[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhTefOGQuC7y",
    "outputId": "5919582e-0545-4620-a778-e47ed83f2c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  2253\n",
      "[[(87,) ('Caucasian',)]\n",
      " [(87,) ('Caucasian',)]\n",
      " [(76,) ('Caucasian',)]\n",
      " [(34,) ('Caucasian',)]\n",
      " [(61,) ('Caucasian',)]]\n",
      "ages length:  2253\n",
      "ethnicity length:  2253\n",
      "(2253, 2)\n",
      "                   age\n",
      "ethnicity             \n",
      "African American   231\n",
      "Caucasian         2010\n",
      "Native American     12\n",
      "============================================================\n",
      "FILTERED:\n",
      "                   age  ethnicity_code\n",
      "ethnicity                             \n",
      "African American   230             230\n",
      "Caucasian         2010            2010\n",
      "Native American     12              12\n",
      "mean age:  63.507548845470694\n",
      "std age:  17.573\n",
      "min age:  15\n",
      "max age:  89\n",
      "[0.972972972972973, 0.972972972972973, 0.8243243243243243, 0.25675675675675674, 0.6216216216216216]\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Shape of ages: (2252, 1)\n",
      "Shape of labels: (2252, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-8a7ece02a874>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ages_filtered['ethnicity_code'] = df_ages_filtered['ethnicity'].astype('category').cat.codes\n"
     ]
    }
   ],
   "source": [
    "ages_ethnicity_np = np.load(path_prefix + \"eICU_age_ethnicity.npy\", allow_pickle=True)\n",
    "print('length: ', len(ages_ethnicity_np))\n",
    "print(ages_ethnicity_np[0:5])\n",
    "\n",
    "ages_np = np.asarray(ages_ethnicity_np[:,0].flatten().tolist()).flatten()\n",
    "print('ages length: ', len(ages_np))\n",
    "#print(ages_np[0:5])\n",
    "\n",
    "ethnicity_np = np.asarray(ages_ethnicity_np[:,1].flatten().tolist()).flatten()\n",
    "print('ethnicity length: ', len(ethnicity_np))\n",
    "#print(ethnicity_np[0:5])\n",
    "\n",
    "df_ages = pd.DataFrame(zip(ages_np, ethnicity_np), columns=['age','ethnicity'])\n",
    "print(df_ages.shape)\n",
    "print(df_ages.groupby('ethnicity').count())\n",
    "\n",
    "# create data set without 90 or greater since that was a category flattened\n",
    "print(\"==\" * 30)\n",
    "print(\"FILTERED:\")\n",
    "df_ages_filtered = df_ages.query(\"age < 90\")\n",
    "df_ages_filtered['ethnicity_code'] = df_ages_filtered['ethnicity'].astype('category').cat.codes\n",
    "print(df_ages_filtered.groupby('ethnicity').count())\n",
    "\n",
    "\n",
    "mean_age_filtered = df_ages_filtered.age.mean()\n",
    "std_age_filtered =  df_ages_filtered.age.std()\n",
    "min_age_filtered =  df_ages_filtered.age.min()\n",
    "max_age_filtered =  df_ages_filtered.age.max()\n",
    "\n",
    "print(\"mean age: \", mean_age_filtered)\n",
    "print(\"std age: \", np.round(std_age_filtered,3))\n",
    "print(\"min age: \", min_age_filtered)\n",
    "print(\"max age: \", max_age_filtered)\n",
    "\n",
    "scaled_ages_filtered = [(x - min_age_filtered)/(max_age_filtered - min_age_filtered) for x in df_ages_filtered['age']]\n",
    "all_ages = np.reshape(scaled_ages_filtered, (-1, 1))\n",
    "\n",
    "all_labels = keras.utils.to_categorical(df_ages_filtered['ethnicity_code'], 3)\n",
    "\n",
    "\n",
    "print(scaled_ages_filtered[0:5])\n",
    "print(np.array(all_labels[0:5]))\n",
    "print(f\"Shape of ages: {all_ages.shape}\")\n",
    "\n",
    "print(f\"Shape of labels: {all_labels.shape}\")\n",
    "\n",
    "# Create tf.data.Dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_ages, all_labels))\n",
    "dataset = dataset.shuffle(buffer_size=200).batch(64)\n",
    "#list(dataset.as_numpy_iterator())\n",
    "#for element in dataset:\n",
    "#    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "RJ80Cu_fuC7z",
    "outputId": "32a173a8-5f3e-4ed5-d2c9-b16dda72a4d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(data=df_ages, x='age', hue='ethnicity', bins=70, color='orange', kde=True, alpha=0.6)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GWKp3RHXuC7z",
    "outputId": "75bedaf3-e90c-4ff1-c463-15437044694a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>African American</th>\n",
       "      <th>Caucasian</th>\n",
       "      <th>Native American</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>62</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>41</td>\n",
       "      <td>African American</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>41</td>\n",
       "      <td>African American</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>72</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>50</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age         ethnicity  African American  Caucasian  Native American\n",
       "2248   62         Caucasian               0.0        1.0              0.0\n",
       "2249   41  African American               1.0        0.0              0.0\n",
       "2250   41  African American               1.0        0.0              0.0\n",
       "2251   72         Caucasian               0.0        1.0              0.0\n",
       "2252   50         Caucasian               0.0        1.0              0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode the ethnicity\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder_df = pd.DataFrame(encoder.fit_transform(df_ages[['ethnicity']]).toarray())\n",
    "\n",
    "#merge one-hot encoded columns back with original DataFrame\n",
    "final_df = df_ages.join(encoder_df)\n",
    "final_df.columns = ['age', 'ethnicity', 'African American', 'Caucasian', 'Native American']\n",
    "final_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "x4TWdUNGuC70",
    "outputId": "5253d321-2c7a-41ff-8de5-8aa7d50971f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethnicity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>African American</th>\n",
       "      <td>231.0</td>\n",
       "      <td>56.151515</td>\n",
       "      <td>16.861546</td>\n",
       "      <td>19.0</td>\n",
       "      <td>46.50</td>\n",
       "      <td>58.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caucasian</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>64.443781</td>\n",
       "      <td>17.419489</td>\n",
       "      <td>15.0</td>\n",
       "      <td>55.00</td>\n",
       "      <td>67.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Native American</th>\n",
       "      <td>12.0</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>20.331346</td>\n",
       "      <td>19.0</td>\n",
       "      <td>39.25</td>\n",
       "      <td>48.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     age                                                     \n",
       "                   count       mean        std   min    25%   50%   75%   max\n",
       "ethnicity                                                                    \n",
       "African American   231.0  56.151515  16.861546  19.0  46.50  58.0  69.0  90.0\n",
       "Caucasian         2010.0  64.443781  17.419489  15.0  55.00  67.0  78.0  89.0\n",
       "Native American     12.0  50.500000  20.331346  19.0  39.25  48.0  66.0  88.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ages.groupby('ethnicity').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7jflWgEuC70"
   },
   "source": [
    "## Create the ConditionalGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYoRh28vuC71",
    "outputId": "33d9638c-ec0a-49f9-986f-f362a53a03a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator input dim:  53\n",
      "Dicrimination input dim:  4\n",
      "WARNING:tensorflow:From <ipython-input-4-20adeaf39045>:83: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CommunicationImplementation.AUTO\n",
      "Number of devices: 2\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,545\n",
      "Trainable params: 4,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CommunicationImplementation.AUTO\n",
      "Number of devices: 2\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 53)]              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                3456      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 7,937\n",
      "Trainable params: 7,809\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cgan = ConditionalGAN(noise_dim=50,\n",
    "                 data_shape=1,\n",
    "                 num_classes=3, \n",
    "                 d_learning_rate=1e-6, \n",
    "                 g_learning_rate=1e-6, \n",
    "                 batch_size=32, \n",
    "                 start_epoch=0,\n",
    "                 verbose = True,\n",
    "                 distribute= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "g4SId-gauC72"
   },
   "outputs": [],
   "source": [
    "cgan.compile(loss_fn=keras.losses.BinaryCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lO40wQe_uC72",
    "outputId": "29bc98e9-a8e2-464c-a647-42bdee64f7ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "age_one_hot_labels1: Tensor(\"strided_slice_1:0\", shape=(None, 1, 3), dtype=float32)\n",
      "age_one_hot_labels2: Tensor(\"strided_slice_2:0\", shape=(None,), dtype=float32)\n",
      "age_one_hot_labels3: Tensor(\"strided_slice_3:0\", shape=(None, 3), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dianam/.local/lib/python3.8/site-packages/keras/backend.py:4993: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_one_hot_labels1: Tensor(\"strided_slice_1:0\", shape=(None, 1, 3), dtype=float32)\n",
      "age_one_hot_labels2: Tensor(\"strided_slice_2:0\", shape=(None,), dtype=float32)\n",
      "age_one_hot_labels3: Tensor(\"strided_slice_3:0\", shape=(None, 3), dtype=float32)\n",
      "36/36 [==============================] - 1s 4ms/step - g_loss: 0.6988 - d_loss: 0.6939\n",
      "Epoch 2/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6985 - d_loss: 0.6938\n",
      "Epoch 3/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6984 - d_loss: 0.6938\n",
      "Epoch 4/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6981 - d_loss: 0.6937\n",
      "Epoch 5/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6980 - d_loss: 0.6936\n",
      "Epoch 6/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6978 - d_loss: 0.6936\n",
      "Epoch 7/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6935\n",
      "Epoch 8/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6934\n",
      "Epoch 9/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6973 - d_loss: 0.6934\n",
      "Epoch 10/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6933\n",
      "Epoch 11/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6933\n",
      "Epoch 12/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6932\n",
      "Epoch 13/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6931\n",
      "Epoch 14/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6931\n",
      "Epoch 15/1000\n",
      "36/36 [==============================] - 0s 6ms/step - g_loss: 0.6965 - d_loss: 0.6930\n",
      "Epoch 16/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6929\n",
      "Epoch 17/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6929\n",
      "Epoch 18/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6961 - d_loss: 0.6928\n",
      "Epoch 19/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6927\n",
      "Epoch 20/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6927\n",
      "Epoch 21/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6926\n",
      "Epoch 22/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6925\n",
      "Epoch 23/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6925\n",
      "Epoch 24/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6924\n",
      "Epoch 25/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6924\n",
      "Epoch 26/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6923\n",
      "Epoch 27/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6922\n",
      "Epoch 28/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6922\n",
      "Epoch 29/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6922\n",
      "Epoch 30/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6949 - d_loss: 0.6921\n",
      "Epoch 31/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6948 - d_loss: 0.6920\n",
      "Epoch 32/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6947 - d_loss: 0.6920\n",
      "Epoch 33/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6947 - d_loss: 0.6919\n",
      "Epoch 34/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6919\n",
      "Epoch 35/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6945 - d_loss: 0.6918\n",
      "Epoch 36/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6945 - d_loss: 0.6918\n",
      "Epoch 37/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6918\n",
      "Epoch 38/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6917\n",
      "Epoch 39/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6916\n",
      "Epoch 40/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6916\n",
      "Epoch 41/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6917\n",
      "Epoch 42/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6916\n",
      "Epoch 43/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6915\n",
      "Epoch 44/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6915\n",
      "Epoch 45/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6915\n",
      "Epoch 46/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6915\n",
      "Epoch 47/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6915\n",
      "Epoch 48/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6914\n",
      "Epoch 49/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6914\n",
      "Epoch 50/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6914\n",
      "Epoch 51/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6914\n",
      "Epoch 52/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6914\n",
      "Epoch 53/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6913\n",
      "Epoch 54/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6913\n",
      "Epoch 55/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6913\n",
      "Epoch 56/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6912\n",
      "Epoch 57/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6911\n",
      "Epoch 58/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6913\n",
      "Epoch 59/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6912\n",
      "Epoch 60/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6936 - d_loss: 0.6912\n",
      "Epoch 61/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6912\n",
      "Epoch 62/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6912\n",
      "Epoch 63/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6913\n",
      "Epoch 64/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6912\n",
      "Epoch 65/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6912\n",
      "Epoch 66/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6911\n",
      "Epoch 67/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6912\n",
      "Epoch 68/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6912\n",
      "Epoch 69/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6932 - d_loss: 0.6912\n",
      "Epoch 70/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6911: 0s - g_loss: 0.6938 - d_loss: 0.\n",
      "Epoch 71/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6912\n",
      "Epoch 72/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6913\n",
      "Epoch 73/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6911\n",
      "Epoch 74/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6912\n",
      "Epoch 75/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6912\n",
      "Epoch 76/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6912\n",
      "Epoch 77/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6914\n",
      "Epoch 78/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6912\n",
      "Epoch 79/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6914\n",
      "Epoch 80/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6912\n",
      "Epoch 81/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6913\n",
      "Epoch 82/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6912\n",
      "Epoch 83/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6913\n",
      "Epoch 84/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6914\n",
      "Epoch 85/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6912\n",
      "Epoch 86/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6912\n",
      "Epoch 87/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6912\n",
      "Epoch 88/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6911\n",
      "Epoch 89/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6913\n",
      "Epoch 90/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6912\n",
      "Epoch 91/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6912\n",
      "Epoch 92/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6913\n",
      "Epoch 93/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6914\n",
      "Epoch 94/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6914\n",
      "Epoch 95/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6913\n",
      "Epoch 96/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6934 - d_loss: 0.6913\n",
      "Epoch 97/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6915\n",
      "Epoch 98/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6914\n",
      "Epoch 99/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6916\n",
      "Epoch 100/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6933 - d_loss: 0.6914\n",
      "Epoch 101/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6914\n",
      "Epoch 102/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6914\n",
      "Epoch 103/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6916\n",
      "Epoch 104/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6917\n",
      "Epoch 105/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6915\n",
      "Epoch 106/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6917\n",
      "Epoch 107/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6917\n",
      "Epoch 108/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6916\n",
      "Epoch 109/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6916\n",
      "Epoch 110/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6916\n",
      "Epoch 111/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6916\n",
      "Epoch 112/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6916\n",
      "Epoch 113/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6918\n",
      "Epoch 114/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6934 - d_loss: 0.6916\n",
      "Epoch 115/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6918\n",
      "Epoch 116/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6918\n",
      "Epoch 117/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6917\n",
      "Epoch 118/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6933 - d_loss: 0.6918\n",
      "Epoch 119/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6917\n",
      "Epoch 120/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6918\n",
      "Epoch 121/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6920\n",
      "Epoch 122/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6917\n",
      "Epoch 123/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6917\n",
      "Epoch 124/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6919\n",
      "Epoch 125/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6918\n",
      "Epoch 126/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6922\n",
      "Epoch 127/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6919\n",
      "Epoch 128/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6919\n",
      "Epoch 129/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6918\n",
      "Epoch 130/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6917\n",
      "Epoch 131/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6919\n",
      "Epoch 132/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6942 - d_loss: 0.6921\n",
      "Epoch 133/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6921\n",
      "Epoch 134/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6920\n",
      "Epoch 135/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6921\n",
      "Epoch 136/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6922\n",
      "Epoch 137/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6922\n",
      "Epoch 138/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6921\n",
      "Epoch 139/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6921\n",
      "Epoch 140/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6922\n",
      "Epoch 141/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6922\n",
      "Epoch 142/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6923\n",
      "Epoch 143/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6919\n",
      "Epoch 144/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6922\n",
      "Epoch 145/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6922\n",
      "Epoch 146/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6923\n",
      "Epoch 147/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6922\n",
      "Epoch 148/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6923\n",
      "Epoch 149/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6923\n",
      "Epoch 150/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6923\n",
      "Epoch 151/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6923\n",
      "Epoch 152/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6923\n",
      "Epoch 153/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6923\n",
      "Epoch 154/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6922\n",
      "Epoch 155/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6925\n",
      "Epoch 156/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6923\n",
      "Epoch 157/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6923\n",
      "Epoch 158/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6923\n",
      "Epoch 159/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6925\n",
      "Epoch 160/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6922\n",
      "Epoch 161/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6925\n",
      "Epoch 162/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6926\n",
      "Epoch 163/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6923\n",
      "Epoch 164/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6924\n",
      "Epoch 165/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6924\n",
      "Epoch 166/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6924\n",
      "Epoch 167/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6924\n",
      "Epoch 168/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6925\n",
      "Epoch 169/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6924\n",
      "Epoch 170/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6924\n",
      "Epoch 171/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6922\n",
      "Epoch 172/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6955 - d_loss: 0.6925\n",
      "Epoch 173/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6955 - d_loss: 0.6924\n",
      "Epoch 174/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6959 - d_loss: 0.6924\n",
      "Epoch 175/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6958 - d_loss: 0.6924\n",
      "Epoch 176/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6924\n",
      "Epoch 177/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6923\n",
      "Epoch 178/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6961 - d_loss: 0.6922\n",
      "Epoch 179/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6924\n",
      "Epoch 180/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6959 - d_loss: 0.6923\n",
      "Epoch 181/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6961 - d_loss: 0.6923\n",
      "Epoch 182/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6963 - d_loss: 0.6923\n",
      "Epoch 183/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6962 - d_loss: 0.6924\n",
      "Epoch 184/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6962 - d_loss: 0.6924\n",
      "Epoch 185/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6963 - d_loss: 0.6923\n",
      "Epoch 186/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6964 - d_loss: 0.6923\n",
      "Epoch 187/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6922\n",
      "Epoch 188/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6964 - d_loss: 0.6922\n",
      "Epoch 189/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6922\n",
      "Epoch 190/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6923\n",
      "Epoch 191/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6923\n",
      "Epoch 192/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6922\n",
      "Epoch 193/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6923\n",
      "Epoch 194/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6970 - d_loss: 0.6922\n",
      "Epoch 195/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6920\n",
      "Epoch 196/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6970 - d_loss: 0.6922\n",
      "Epoch 197/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6970 - d_loss: 0.6921\n",
      "Epoch 198/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6973 - d_loss: 0.6920\n",
      "Epoch 199/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6921\n",
      "Epoch 200/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6920\n",
      "Epoch 201/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6976 - d_loss: 0.6921\n",
      "Epoch 202/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6974 - d_loss: 0.6921\n",
      "Epoch 203/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6921\n",
      "Epoch 204/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6977 - d_loss: 0.6920\n",
      "Epoch 205/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6920\n",
      "Epoch 206/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6979 - d_loss: 0.6919\n",
      "Epoch 207/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6980 - d_loss: 0.6919\n",
      "Epoch 208/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6919\n",
      "Epoch 209/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6981 - d_loss: 0.6918\n",
      "Epoch 210/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6982 - d_loss: 0.6918\n",
      "Epoch 211/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6982 - d_loss: 0.6918\n",
      "Epoch 212/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6983 - d_loss: 0.6918\n",
      "Epoch 213/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6983 - d_loss: 0.6918\n",
      "Epoch 214/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6985 - d_loss: 0.6917\n",
      "Epoch 215/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6985 - d_loss: 0.6917\n",
      "Epoch 216/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6985 - d_loss: 0.6917\n",
      "Epoch 217/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6987 - d_loss: 0.6917\n",
      "Epoch 218/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6917\n",
      "Epoch 219/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6988 - d_loss: 0.6916\n",
      "Epoch 220/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6991 - d_loss: 0.6916\n",
      "Epoch 221/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6990 - d_loss: 0.6915\n",
      "Epoch 222/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6990 - d_loss: 0.6916\n",
      "Epoch 223/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6992 - d_loss: 0.6915\n",
      "Epoch 224/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6992 - d_loss: 0.6915\n",
      "Epoch 225/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6995 - d_loss: 0.6914\n",
      "Epoch 226/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6994 - d_loss: 0.6914\n",
      "Epoch 227/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6996 - d_loss: 0.6915\n",
      "Epoch 228/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6995 - d_loss: 0.6915\n",
      "Epoch 229/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6998 - d_loss: 0.6914\n",
      "Epoch 230/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6998 - d_loss: 0.6913\n",
      "Epoch 231/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6998 - d_loss: 0.6913\n",
      "Epoch 232/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7000 - d_loss: 0.6913\n",
      "Epoch 233/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6999 - d_loss: 0.6912\n",
      "Epoch 234/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7000 - d_loss: 0.6913\n",
      "Epoch 235/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7001 - d_loss: 0.6912\n",
      "Epoch 236/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7000 - d_loss: 0.6911\n",
      "Epoch 237/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7003 - d_loss: 0.6911\n",
      "Epoch 238/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7004 - d_loss: 0.6912\n",
      "Epoch 239/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7005 - d_loss: 0.6911\n",
      "Epoch 240/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7005 - d_loss: 0.6911\n",
      "Epoch 241/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7007 - d_loss: 0.6909\n",
      "Epoch 242/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7007 - d_loss: 0.6910\n",
      "Epoch 243/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7007 - d_loss: 0.6910\n",
      "Epoch 244/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7009 - d_loss: 0.6909\n",
      "Epoch 245/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7009 - d_loss: 0.6909\n",
      "Epoch 246/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7009 - d_loss: 0.6909\n",
      "Epoch 247/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7008 - d_loss: 0.6909\n",
      "Epoch 248/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7010 - d_loss: 0.6908\n",
      "Epoch 249/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7012 - d_loss: 0.6908\n",
      "Epoch 250/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7011 - d_loss: 0.6908\n",
      "Epoch 251/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7013 - d_loss: 0.6908\n",
      "Epoch 252/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7013 - d_loss: 0.6907\n",
      "Epoch 253/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7014 - d_loss: 0.6907\n",
      "Epoch 254/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7014 - d_loss: 0.6907\n",
      "Epoch 255/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7015 - d_loss: 0.6906\n",
      "Epoch 256/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7015 - d_loss: 0.6906\n",
      "Epoch 257/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6905\n",
      "Epoch 258/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6906\n",
      "Epoch 259/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6905\n",
      "Epoch 260/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7015 - d_loss: 0.6905\n",
      "Epoch 261/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6905\n",
      "Epoch 262/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7017 - d_loss: 0.6904\n",
      "Epoch 263/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7017 - d_loss: 0.6904\n",
      "Epoch 264/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7021 - d_loss: 0.6904\n",
      "Epoch 265/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7019 - d_loss: 0.6904\n",
      "Epoch 266/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7019 - d_loss: 0.6903\n",
      "Epoch 267/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6904\n",
      "Epoch 268/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7019 - d_loss: 0.6904\n",
      "Epoch 269/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7019 - d_loss: 0.6903\n",
      "Epoch 270/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7017 - d_loss: 0.6904\n",
      "Epoch 271/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7020 - d_loss: 0.6903\n",
      "Epoch 272/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7018 - d_loss: 0.6904\n",
      "Epoch 273/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7019 - d_loss: 0.6903\n",
      "Epoch 274/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7017 - d_loss: 0.6903\n",
      "Epoch 275/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7020 - d_loss: 0.6902\n",
      "Epoch 276/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7019 - d_loss: 0.6904\n",
      "Epoch 277/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7015 - d_loss: 0.6903\n",
      "Epoch 278/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7017 - d_loss: 0.6902\n",
      "Epoch 279/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7017 - d_loss: 0.6903\n",
      "Epoch 280/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7015 - d_loss: 0.6902\n",
      "Epoch 281/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7015 - d_loss: 0.6903\n",
      "Epoch 282/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7015 - d_loss: 0.6902\n",
      "Epoch 283/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7016 - d_loss: 0.6903\n",
      "Epoch 284/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7015 - d_loss: 0.6903\n",
      "Epoch 285/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.7018 - d_loss: 0.6903\n",
      "Epoch 286/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6900\n",
      "Epoch 287/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7015 - d_loss: 0.6903\n",
      "Epoch 288/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7016 - d_loss: 0.6903\n",
      "Epoch 289/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7011 - d_loss: 0.6902\n",
      "Epoch 290/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7014 - d_loss: 0.6903\n",
      "Epoch 291/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7013 - d_loss: 0.6903\n",
      "Epoch 292/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7015 - d_loss: 0.6901\n",
      "Epoch 293/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7009 - d_loss: 0.6901\n",
      "Epoch 294/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7011 - d_loss: 0.6904\n",
      "Epoch 295/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7010 - d_loss: 0.6903\n",
      "Epoch 296/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7007 - d_loss: 0.6902\n",
      "Epoch 297/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7009 - d_loss: 0.6903\n",
      "Epoch 298/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7009 - d_loss: 0.6903\n",
      "Epoch 299/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7008 - d_loss: 0.6903\n",
      "Epoch 300/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7004 - d_loss: 0.6903\n",
      "Epoch 301/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7003 - d_loss: 0.6903\n",
      "Epoch 302/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7003 - d_loss: 0.6905\n",
      "Epoch 303/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7002 - d_loss: 0.6902\n",
      "Epoch 304/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7000 - d_loss: 0.6903\n",
      "Epoch 305/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7001 - d_loss: 0.6905\n",
      "Epoch 306/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6998 - d_loss: 0.6903\n",
      "Epoch 307/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.7001 - d_loss: 0.6903\n",
      "Epoch 308/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6997 - d_loss: 0.6902\n",
      "Epoch 309/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6999 - d_loss: 0.6903\n",
      "Epoch 310/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6996 - d_loss: 0.6903\n",
      "Epoch 311/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6998 - d_loss: 0.6905\n",
      "Epoch 312/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6999 - d_loss: 0.6904\n",
      "Epoch 313/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6997 - d_loss: 0.6904\n",
      "Epoch 314/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6995 - d_loss: 0.6904\n",
      "Epoch 315/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6993 - d_loss: 0.6905\n",
      "Epoch 316/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6991 - d_loss: 0.6904\n",
      "Epoch 317/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6904\n",
      "Epoch 318/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6906\n",
      "Epoch 319/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6904\n",
      "Epoch 320/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6987 - d_loss: 0.6905\n",
      "Epoch 321/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6906\n",
      "Epoch 322/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6905\n",
      "Epoch 323/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6906\n",
      "Epoch 324/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6982 - d_loss: 0.6906\n",
      "Epoch 325/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6987 - d_loss: 0.6906\n",
      "Epoch 326/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6981 - d_loss: 0.6905\n",
      "Epoch 327/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6981 - d_loss: 0.6906\n",
      "Epoch 328/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6907\n",
      "Epoch 329/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6906\n",
      "Epoch 330/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6906\n",
      "Epoch 331/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6976 - d_loss: 0.6905\n",
      "Epoch 332/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6973 - d_loss: 0.6905\n",
      "Epoch 333/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6973 - d_loss: 0.6907\n",
      "Epoch 334/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6906\n",
      "Epoch 335/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6908\n",
      "Epoch 336/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6906\n",
      "Epoch 337/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6970 - d_loss: 0.6908\n",
      "Epoch 338/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6907\n",
      "Epoch 339/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6907\n",
      "Epoch 340/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6908\n",
      "Epoch 341/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6909\n",
      "Epoch 342/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6909\n",
      "Epoch 343/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6910\n",
      "Epoch 344/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6907\n",
      "Epoch 345/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6909\n",
      "Epoch 346/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6907\n",
      "Epoch 347/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6908\n",
      "Epoch 348/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6961 - d_loss: 0.6907\n",
      "Epoch 349/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6961 - d_loss: 0.6907\n",
      "Epoch 350/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6962 - d_loss: 0.6908\n",
      "Epoch 351/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6960 - d_loss: 0.6907\n",
      "Epoch 352/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6907\n",
      "Epoch 353/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6909\n",
      "Epoch 354/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6907\n",
      "Epoch 355/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6954 - d_loss: 0.6908\n",
      "Epoch 356/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6957 - d_loss: 0.6907\n",
      "Epoch 357/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6956 - d_loss: 0.6908\n",
      "Epoch 358/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6954 - d_loss: 0.6909\n",
      "Epoch 359/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6953 - d_loss: 0.6908\n",
      "Epoch 360/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6908\n",
      "Epoch 361/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6908\n",
      "Epoch 362/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6906\n",
      "Epoch 363/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6909\n",
      "Epoch 364/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6908\n",
      "Epoch 365/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6910\n",
      "Epoch 366/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6909\n",
      "Epoch 367/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6909\n",
      "Epoch 368/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6908\n",
      "Epoch 369/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6909\n",
      "Epoch 370/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6908\n",
      "Epoch 371/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6907\n",
      "Epoch 372/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6908\n",
      "Epoch 373/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6907\n",
      "Epoch 374/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6908\n",
      "Epoch 375/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6906\n",
      "Epoch 376/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6908\n",
      "Epoch 377/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6907\n",
      "Epoch 378/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6907\n",
      "Epoch 379/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6909\n",
      "Epoch 380/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6907\n",
      "Epoch 381/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6907\n",
      "Epoch 382/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6945 - d_loss: 0.6906\n",
      "Epoch 383/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6907\n",
      "Epoch 384/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6942 - d_loss: 0.6907\n",
      "Epoch 385/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6944 - d_loss: 0.6907\n",
      "Epoch 386/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6907\n",
      "Epoch 387/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6907\n",
      "Epoch 388/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6905\n",
      "Epoch 389/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6905\n",
      "Epoch 390/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6906\n",
      "Epoch 391/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6905\n",
      "Epoch 392/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6906\n",
      "Epoch 393/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6942 - d_loss: 0.6906\n",
      "Epoch 394/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6906\n",
      "Epoch 395/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6904\n",
      "Epoch 396/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6905\n",
      "Epoch 397/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6905\n",
      "Epoch 398/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6906\n",
      "Epoch 399/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6904\n",
      "Epoch 400/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6905\n",
      "Epoch 401/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6905\n",
      "Epoch 402/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6904\n",
      "Epoch 403/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6906\n",
      "Epoch 404/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6903\n",
      "Epoch 405/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6903\n",
      "Epoch 406/1000\n",
      "36/36 [==============================] - 0s 6ms/step - g_loss: 0.6937 - d_loss: 0.6905\n",
      "Epoch 407/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6903\n",
      "Epoch 408/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6904\n",
      "Epoch 409/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6903\n",
      "Epoch 410/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6903\n",
      "Epoch 411/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6903\n",
      "Epoch 412/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6904\n",
      "Epoch 413/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6902\n",
      "Epoch 414/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6904\n",
      "Epoch 415/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6904\n",
      "Epoch 416/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6903\n",
      "Epoch 417/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6903\n",
      "Epoch 418/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6939 - d_loss: 0.6903\n",
      "Epoch 419/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6903\n",
      "Epoch 420/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6937 - d_loss: 0.6903\n",
      "Epoch 421/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6939 - d_loss: 0.6903\n",
      "Epoch 422/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6902\n",
      "Epoch 423/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6902\n",
      "Epoch 424/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6904\n",
      "Epoch 425/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6902\n",
      "Epoch 426/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6901\n",
      "Epoch 427/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6900\n",
      "Epoch 428/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6903\n",
      "Epoch 429/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6942 - d_loss: 0.6902\n",
      "Epoch 430/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6902\n",
      "Epoch 431/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6902\n",
      "Epoch 432/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6939 - d_loss: 0.6903\n",
      "Epoch 433/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6901\n",
      "Epoch 434/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6902\n",
      "Epoch 435/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6902\n",
      "Epoch 436/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6901\n",
      "Epoch 437/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6902\n",
      "Epoch 438/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6901\n",
      "Epoch 439/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6902\n",
      "Epoch 440/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6901\n",
      "Epoch 441/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6901\n",
      "Epoch 442/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6900\n",
      "Epoch 443/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6900\n",
      "Epoch 444/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6901\n",
      "Epoch 445/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6902\n",
      "Epoch 446/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6902\n",
      "Epoch 447/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6904\n",
      "Epoch 448/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6901\n",
      "Epoch 449/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6903\n",
      "Epoch 450/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6904\n",
      "Epoch 451/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6903\n",
      "Epoch 452/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6942 - d_loss: 0.6904\n",
      "Epoch 453/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6903\n",
      "Epoch 454/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6902\n",
      "Epoch 455/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6902\n",
      "Epoch 456/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6903\n",
      "Epoch 457/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6905\n",
      "Epoch 458/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6901\n",
      "Epoch 459/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6904\n",
      "Epoch 460/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6942 - d_loss: 0.6901\n",
      "Epoch 461/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6906\n",
      "Epoch 462/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6904\n",
      "Epoch 463/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6905\n",
      "Epoch 464/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6904\n",
      "Epoch 465/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6904\n",
      "Epoch 466/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6950 - d_loss: 0.6904\n",
      "Epoch 467/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6951 - d_loss: 0.6906\n",
      "Epoch 468/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6950 - d_loss: 0.6905\n",
      "Epoch 469/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6904\n",
      "Epoch 470/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6952 - d_loss: 0.6905\n",
      "Epoch 471/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6951 - d_loss: 0.6904\n",
      "Epoch 472/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6905\n",
      "Epoch 473/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6908: 0s - g_loss: 0.6955 - d_loss: \n",
      "Epoch 474/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6901\n",
      "Epoch 475/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6906\n",
      "Epoch 476/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6905\n",
      "Epoch 477/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6905\n",
      "Epoch 478/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6906\n",
      "Epoch 479/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6904\n",
      "Epoch 480/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6959 - d_loss: 0.6906\n",
      "Epoch 481/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6908\n",
      "Epoch 482/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6907\n",
      "Epoch 483/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6903\n",
      "Epoch 484/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6907\n",
      "Epoch 485/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6905\n",
      "Epoch 486/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6906\n",
      "Epoch 487/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6907\n",
      "Epoch 488/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6966 - d_loss: 0.6907\n",
      "Epoch 489/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6966 - d_loss: 0.6905\n",
      "Epoch 490/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6906\n",
      "Epoch 491/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6906\n",
      "Epoch 492/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6907\n",
      "Epoch 493/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6907\n",
      "Epoch 494/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6906\n",
      "Epoch 495/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6908\n",
      "Epoch 496/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6906\n",
      "Epoch 497/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6906\n",
      "Epoch 498/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6908\n",
      "Epoch 499/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6965 - d_loss: 0.6907\n",
      "Epoch 500/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6960 - d_loss: 0.6907\n",
      "Epoch 501/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6909\n",
      "Epoch 502/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6909\n",
      "Epoch 503/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6906\n",
      "Epoch 504/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6909\n",
      "Epoch 505/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6908\n",
      "Epoch 506/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6907\n",
      "Epoch 507/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6907\n",
      "Epoch 508/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6909\n",
      "Epoch 509/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6907\n",
      "Epoch 510/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6906\n",
      "Epoch 511/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6909\n",
      "Epoch 512/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6907\n",
      "Epoch 513/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6908\n",
      "Epoch 514/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6907\n",
      "Epoch 515/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6973 - d_loss: 0.6910\n",
      "Epoch 516/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6908\n",
      "Epoch 517/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6908\n",
      "Epoch 518/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6970 - d_loss: 0.6909\n",
      "Epoch 519/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6909\n",
      "Epoch 520/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6907\n",
      "Epoch 521/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6909\n",
      "Epoch 522/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6909\n",
      "Epoch 523/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6907\n",
      "Epoch 524/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6907\n",
      "Epoch 525/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6908\n",
      "Epoch 526/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6906\n",
      "Epoch 527/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6907\n",
      "Epoch 528/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6979 - d_loss: 0.6907\n",
      "Epoch 529/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6979 - d_loss: 0.6908\n",
      "Epoch 530/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6904\n",
      "Epoch 531/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6905\n",
      "Epoch 532/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6907\n",
      "Epoch 533/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6905\n",
      "Epoch 534/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6908\n",
      "Epoch 535/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6977 - d_loss: 0.6907\n",
      "Epoch 536/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6980 - d_loss: 0.6907\n",
      "Epoch 537/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6981 - d_loss: 0.6907\n",
      "Epoch 538/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6977 - d_loss: 0.6904\n",
      "Epoch 539/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6981 - d_loss: 0.6906\n",
      "Epoch 540/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6982 - d_loss: 0.6904\n",
      "Epoch 541/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6977 - d_loss: 0.6907\n",
      "Epoch 542/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6978 - d_loss: 0.6905\n",
      "Epoch 543/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6979 - d_loss: 0.6902\n",
      "Epoch 544/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6982 - d_loss: 0.6906\n",
      "Epoch 545/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6980 - d_loss: 0.6906\n",
      "Epoch 546/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6983 - d_loss: 0.6906\n",
      "Epoch 547/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6979 - d_loss: 0.6906\n",
      "Epoch 548/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6983 - d_loss: 0.6904\n",
      "Epoch 549/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6987 - d_loss: 0.6905\n",
      "Epoch 550/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6981 - d_loss: 0.6907\n",
      "Epoch 551/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6984 - d_loss: 0.6903\n",
      "Epoch 552/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6985 - d_loss: 0.6904\n",
      "Epoch 553/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6984 - d_loss: 0.6902\n",
      "Epoch 554/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6985 - d_loss: 0.6902\n",
      "Epoch 555/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6989 - d_loss: 0.6904\n",
      "Epoch 556/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6903\n",
      "Epoch 557/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6903\n",
      "Epoch 558/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6903\n",
      "Epoch 559/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6984 - d_loss: 0.6904\n",
      "Epoch 560/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6983 - d_loss: 0.6901\n",
      "Epoch 561/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6987 - d_loss: 0.6903\n",
      "Epoch 562/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6985 - d_loss: 0.6900\n",
      "Epoch 563/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6985 - d_loss: 0.6903\n",
      "Epoch 564/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6987 - d_loss: 0.6901\n",
      "Epoch 565/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6903\n",
      "Epoch 566/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6902\n",
      "Epoch 567/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6904\n",
      "Epoch 568/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6989 - d_loss: 0.6902\n",
      "Epoch 569/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6990 - d_loss: 0.6903\n",
      "Epoch 570/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6986 - d_loss: 0.6900\n",
      "Epoch 571/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6988 - d_loss: 0.6901\n",
      "Epoch 572/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6990 - d_loss: 0.6901\n",
      "Epoch 573/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6990 - d_loss: 0.6900\n",
      "Epoch 574/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6900\n",
      "Epoch 575/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6901\n",
      "Epoch 576/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6901\n",
      "Epoch 577/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6902\n",
      "Epoch 578/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6987 - d_loss: 0.6903\n",
      "Epoch 579/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6900\n",
      "Epoch 580/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6897\n",
      "Epoch 581/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6900\n",
      "Epoch 582/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6900\n",
      "Epoch 583/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6901\n",
      "Epoch 584/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6899\n",
      "Epoch 585/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6901\n",
      "Epoch 586/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6898\n",
      "Epoch 587/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6992 - d_loss: 0.6899\n",
      "Epoch 588/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6898\n",
      "Epoch 589/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6992 - d_loss: 0.6899\n",
      "Epoch 590/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6987 - d_loss: 0.6900\n",
      "Epoch 591/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6899\n",
      "Epoch 592/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6898\n",
      "Epoch 593/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6899\n",
      "Epoch 594/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6900\n",
      "Epoch 595/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6991 - d_loss: 0.6900\n",
      "Epoch 596/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6899\n",
      "Epoch 597/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6901\n",
      "Epoch 598/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6984 - d_loss: 0.6901\n",
      "Epoch 599/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6986 - d_loss: 0.6898\n",
      "Epoch 600/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6989 - d_loss: 0.6898\n",
      "Epoch 601/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6988 - d_loss: 0.6896\n",
      "Epoch 602/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6896\n",
      "Epoch 603/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6990 - d_loss: 0.6899\n",
      "Epoch 604/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6991 - d_loss: 0.6899\n",
      "Epoch 605/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6983 - d_loss: 0.6899\n",
      "Epoch 606/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6987 - d_loss: 0.6898\n",
      "Epoch 607/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6982 - d_loss: 0.6898\n",
      "Epoch 608/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6982 - d_loss: 0.6900\n",
      "Epoch 609/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6981 - d_loss: 0.6898\n",
      "Epoch 610/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6986 - d_loss: 0.6898\n",
      "Epoch 611/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6982 - d_loss: 0.6900\n",
      "Epoch 612/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6981 - d_loss: 0.6902\n",
      "Epoch 613/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6984 - d_loss: 0.6901\n",
      "Epoch 614/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6986 - d_loss: 0.6901\n",
      "Epoch 615/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6977 - d_loss: 0.6904\n",
      "Epoch 616/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6982 - d_loss: 0.6903\n",
      "Epoch 617/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6982 - d_loss: 0.6901: 0s - g_loss: 0.6979 - d_loss: 0.\n",
      "Epoch 618/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6978 - d_loss: 0.6901\n",
      "Epoch 619/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6979 - d_loss: 0.6905\n",
      "Epoch 620/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6980 - d_loss: 0.6900\n",
      "Epoch 621/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6903\n",
      "Epoch 622/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6986 - d_loss: 0.6901\n",
      "Epoch 623/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6973 - d_loss: 0.6904\n",
      "Epoch 624/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6903\n",
      "Epoch 625/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6981 - d_loss: 0.6900: 0s - g_loss: 0.6981 - d_loss: \n",
      "Epoch 626/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6977 - d_loss: 0.6903\n",
      "Epoch 627/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6981 - d_loss: 0.6901\n",
      "Epoch 628/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6904\n",
      "Epoch 629/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6902\n",
      "Epoch 630/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6966 - d_loss: 0.6903\n",
      "Epoch 631/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6970 - d_loss: 0.6900\n",
      "Epoch 632/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6905\n",
      "Epoch 633/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6974 - d_loss: 0.6901\n",
      "Epoch 634/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6971 - d_loss: 0.6904\n",
      "Epoch 635/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6904\n",
      "Epoch 636/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6902\n",
      "Epoch 637/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6966 - d_loss: 0.6902\n",
      "Epoch 638/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6965 - d_loss: 0.6905\n",
      "Epoch 639/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6902\n",
      "Epoch 640/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6977 - d_loss: 0.6904\n",
      "Epoch 641/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6903\n",
      "Epoch 642/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6903\n",
      "Epoch 643/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6904\n",
      "Epoch 644/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6906\n",
      "Epoch 645/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6970 - d_loss: 0.6904\n",
      "Epoch 646/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6905\n",
      "Epoch 647/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6964 - d_loss: 0.6905\n",
      "Epoch 648/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6962 - d_loss: 0.6903\n",
      "Epoch 649/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6903\n",
      "Epoch 650/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6906\n",
      "Epoch 651/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6956 - d_loss: 0.6907\n",
      "Epoch 652/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6960 - d_loss: 0.6906\n",
      "Epoch 653/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6965 - d_loss: 0.6905\n",
      "Epoch 654/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6904\n",
      "Epoch 655/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6906\n",
      "Epoch 656/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6906\n",
      "Epoch 657/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6905\n",
      "Epoch 658/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6955 - d_loss: 0.6907\n",
      "Epoch 659/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6952 - d_loss: 0.6906\n",
      "Epoch 660/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6950 - d_loss: 0.6909\n",
      "Epoch 661/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6959 - d_loss: 0.6911\n",
      "Epoch 662/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6956 - d_loss: 0.6907\n",
      "Epoch 663/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6954 - d_loss: 0.6906\n",
      "Epoch 664/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6951 - d_loss: 0.6905\n",
      "Epoch 665/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6953 - d_loss: 0.6909\n",
      "Epoch 666/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6958 - d_loss: 0.6909\n",
      "Epoch 667/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6909\n",
      "Epoch 668/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6952 - d_loss: 0.6908\n",
      "Epoch 669/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6948 - d_loss: 0.6910\n",
      "Epoch 670/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6947 - d_loss: 0.6908\n",
      "Epoch 671/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6908\n",
      "Epoch 672/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6906\n",
      "Epoch 673/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6905\n",
      "Epoch 674/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6910\n",
      "Epoch 675/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6906\n",
      "Epoch 676/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6908\n",
      "Epoch 677/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6912\n",
      "Epoch 678/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6909\n",
      "Epoch 679/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6942 - d_loss: 0.6906\n",
      "Epoch 680/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6908\n",
      "Epoch 681/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6910\n",
      "Epoch 682/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6907\n",
      "Epoch 683/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6911\n",
      "Epoch 684/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6950 - d_loss: 0.6910\n",
      "Epoch 685/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6910\n",
      "Epoch 686/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6907\n",
      "Epoch 687/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6911\n",
      "Epoch 688/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6949 - d_loss: 0.6906\n",
      "Epoch 689/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6909\n",
      "Epoch 690/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6942 - d_loss: 0.6909\n",
      "Epoch 691/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6909\n",
      "Epoch 692/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6909\n",
      "Epoch 693/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6908\n",
      "Epoch 694/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6908\n",
      "Epoch 695/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6905\n",
      "Epoch 696/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6909\n",
      "Epoch 697/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6909\n",
      "Epoch 698/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6909\n",
      "Epoch 699/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6913\n",
      "Epoch 700/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6908\n",
      "Epoch 701/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6910\n",
      "Epoch 702/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6910\n",
      "Epoch 703/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6908\n",
      "Epoch 704/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6910\n",
      "Epoch 705/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6905\n",
      "Epoch 706/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6907\n",
      "Epoch 707/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6907\n",
      "Epoch 708/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6908\n",
      "Epoch 709/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6912\n",
      "Epoch 710/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6909\n",
      "Epoch 711/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6911\n",
      "Epoch 712/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6909\n",
      "Epoch 713/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6906\n",
      "Epoch 714/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6909\n",
      "Epoch 715/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6911\n",
      "Epoch 716/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6911\n",
      "Epoch 717/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6909\n",
      "Epoch 718/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6909\n",
      "Epoch 719/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6908\n",
      "Epoch 720/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6908\n",
      "Epoch 721/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6932 - d_loss: 0.6908\n",
      "Epoch 722/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6909\n",
      "Epoch 723/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6910\n",
      "Epoch 724/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6936 - d_loss: 0.6909\n",
      "Epoch 725/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6942 - d_loss: 0.6911\n",
      "Epoch 726/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6909\n",
      "Epoch 727/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6906\n",
      "Epoch 728/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6907\n",
      "Epoch 729/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6938 - d_loss: 0.6909\n",
      "Epoch 730/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6912\n",
      "Epoch 731/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6904\n",
      "Epoch 732/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6908\n",
      "Epoch 733/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6937 - d_loss: 0.6907\n",
      "Epoch 734/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6908\n",
      "Epoch 735/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6940 - d_loss: 0.6910\n",
      "Epoch 736/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6906\n",
      "Epoch 737/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6908\n",
      "Epoch 738/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6909\n",
      "Epoch 739/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6910\n",
      "Epoch 740/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6935 - d_loss: 0.6906\n",
      "Epoch 741/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6906\n",
      "Epoch 742/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6905\n",
      "Epoch 743/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6905\n",
      "Epoch 744/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6906\n",
      "Epoch 745/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6905\n",
      "Epoch 746/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6907\n",
      "Epoch 747/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6909\n",
      "Epoch 748/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6935 - d_loss: 0.6911\n",
      "Epoch 749/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6941 - d_loss: 0.6908\n",
      "Epoch 750/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6943 - d_loss: 0.6908\n",
      "Epoch 751/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6907\n",
      "Epoch 752/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6940 - d_loss: 0.6910\n",
      "Epoch 753/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6939 - d_loss: 0.6908\n",
      "Epoch 754/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6911\n",
      "Epoch 755/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6907\n",
      "Epoch 756/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6906\n",
      "Epoch 757/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6910\n",
      "Epoch 758/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6908\n",
      "Epoch 759/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6907\n",
      "Epoch 760/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6910\n",
      "Epoch 761/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6907\n",
      "Epoch 762/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6939 - d_loss: 0.6906\n",
      "Epoch 763/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6936 - d_loss: 0.6909\n",
      "Epoch 764/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6948 - d_loss: 0.6907\n",
      "Epoch 765/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6908\n",
      "Epoch 766/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6941 - d_loss: 0.6909\n",
      "Epoch 767/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6910\n",
      "Epoch 768/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6948 - d_loss: 0.6908\n",
      "Epoch 769/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6938 - d_loss: 0.6912\n",
      "Epoch 770/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6946 - d_loss: 0.6907\n",
      "Epoch 771/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6951 - d_loss: 0.6905\n",
      "Epoch 772/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6906\n",
      "Epoch 773/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6908\n",
      "Epoch 774/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6948 - d_loss: 0.6908\n",
      "Epoch 775/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6947 - d_loss: 0.6912\n",
      "Epoch 776/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6937 - d_loss: 0.6909\n",
      "Epoch 777/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6911\n",
      "Epoch 778/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6945 - d_loss: 0.6910\n",
      "Epoch 779/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6934 - d_loss: 0.6911\n",
      "Epoch 780/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6945 - d_loss: 0.6911\n",
      "Epoch 781/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6943 - d_loss: 0.6911\n",
      "Epoch 782/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6942 - d_loss: 0.6912\n",
      "Epoch 783/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6909\n",
      "Epoch 784/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6944 - d_loss: 0.6907\n",
      "Epoch 785/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6946 - d_loss: 0.6909\n",
      "Epoch 786/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6940 - d_loss: 0.6913\n",
      "Epoch 787/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6948 - d_loss: 0.6909\n",
      "Epoch 788/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6942 - d_loss: 0.6911\n",
      "Epoch 789/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6913\n",
      "Epoch 790/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6938 - d_loss: 0.6913\n",
      "Epoch 791/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6943 - d_loss: 0.6910\n",
      "Epoch 792/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6942 - d_loss: 0.6911\n",
      "Epoch 793/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6954 - d_loss: 0.6906\n",
      "Epoch 794/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6944 - d_loss: 0.6914\n",
      "Epoch 795/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6945 - d_loss: 0.6911\n",
      "Epoch 796/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6913\n",
      "Epoch 797/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6913\n",
      "Epoch 798/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6947 - d_loss: 0.6916\n",
      "Epoch 799/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6945 - d_loss: 0.6909\n",
      "Epoch 800/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6950 - d_loss: 0.6910\n",
      "Epoch 801/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6945 - d_loss: 0.6913\n",
      "Epoch 802/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6946 - d_loss: 0.6917\n",
      "Epoch 803/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6915\n",
      "Epoch 804/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6951 - d_loss: 0.6913\n",
      "Epoch 805/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6914\n",
      "Epoch 806/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6950 - d_loss: 0.6912\n",
      "Epoch 807/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6911\n",
      "Epoch 808/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6944 - d_loss: 0.6914\n",
      "Epoch 809/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6914\n",
      "Epoch 810/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6915\n",
      "Epoch 811/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6914\n",
      "Epoch 812/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6917\n",
      "Epoch 813/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6917\n",
      "Epoch 814/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6920\n",
      "Epoch 815/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6917\n",
      "Epoch 816/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6917\n",
      "Epoch 817/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6949 - d_loss: 0.6920\n",
      "Epoch 818/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6918\n",
      "Epoch 819/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6946 - d_loss: 0.6918\n",
      "Epoch 820/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6918\n",
      "Epoch 821/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6912\n",
      "Epoch 822/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6913\n",
      "Epoch 823/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6916\n",
      "Epoch 824/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6917\n",
      "Epoch 825/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6947 - d_loss: 0.6917\n",
      "Epoch 826/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6918\n",
      "Epoch 827/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6959 - d_loss: 0.6920\n",
      "Epoch 828/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6956 - d_loss: 0.6918\n",
      "Epoch 829/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6958 - d_loss: 0.6916\n",
      "Epoch 830/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6951 - d_loss: 0.6917\n",
      "Epoch 831/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6920\n",
      "Epoch 832/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6916\n",
      "Epoch 833/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6959 - d_loss: 0.6924\n",
      "Epoch 834/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6921\n",
      "Epoch 835/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6919\n",
      "Epoch 836/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6950 - d_loss: 0.6916\n",
      "Epoch 837/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6919\n",
      "Epoch 838/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6923\n",
      "Epoch 839/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6952 - d_loss: 0.6916\n",
      "Epoch 840/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6917\n",
      "Epoch 841/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6923\n",
      "Epoch 842/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6921\n",
      "Epoch 843/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6916\n",
      "Epoch 844/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6922\n",
      "Epoch 845/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6916\n",
      "Epoch 846/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6922\n",
      "Epoch 847/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6960 - d_loss: 0.6922\n",
      "Epoch 848/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6920\n",
      "Epoch 849/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6921\n",
      "Epoch 850/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6923\n",
      "Epoch 851/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6918\n",
      "Epoch 852/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6920\n",
      "Epoch 853/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6961 - d_loss: 0.6921\n",
      "Epoch 854/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6919\n",
      "Epoch 855/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6961 - d_loss: 0.6920\n",
      "Epoch 856/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6915\n",
      "Epoch 857/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6920\n",
      "Epoch 858/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6959 - d_loss: 0.6920\n",
      "Epoch 859/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6921\n",
      "Epoch 860/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6918\n",
      "Epoch 861/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6918\n",
      "Epoch 862/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6921\n",
      "Epoch 863/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6922\n",
      "Epoch 864/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6918\n",
      "Epoch 865/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6922\n",
      "Epoch 866/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6919\n",
      "Epoch 867/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6916\n",
      "Epoch 868/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6919\n",
      "Epoch 869/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6921\n",
      "Epoch 870/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6918\n",
      "Epoch 871/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6923\n",
      "Epoch 872/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6972 - d_loss: 0.6920\n",
      "Epoch 873/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6964 - d_loss: 0.6922\n",
      "Epoch 874/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6919\n",
      "Epoch 875/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6921\n",
      "Epoch 876/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6972 - d_loss: 0.6919\n",
      "Epoch 877/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6920\n",
      "Epoch 878/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6920\n",
      "Epoch 879/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6971 - d_loss: 0.6921\n",
      "Epoch 880/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6964 - d_loss: 0.6921\n",
      "Epoch 881/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6971 - d_loss: 0.6916\n",
      "Epoch 882/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6916\n",
      "Epoch 883/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6919\n",
      "Epoch 884/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6971 - d_loss: 0.6921\n",
      "Epoch 885/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6973 - d_loss: 0.6920\n",
      "Epoch 886/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6919\n",
      "Epoch 887/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6919\n",
      "Epoch 888/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6965 - d_loss: 0.6920\n",
      "Epoch 889/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6969 - d_loss: 0.6916\n",
      "Epoch 890/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6974 - d_loss: 0.6917\n",
      "Epoch 891/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6966 - d_loss: 0.6919\n",
      "Epoch 892/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6974 - d_loss: 0.6921\n",
      "Epoch 893/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6921\n",
      "Epoch 894/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6977 - d_loss: 0.6915\n",
      "Epoch 895/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6917\n",
      "Epoch 896/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6970 - d_loss: 0.6918\n",
      "Epoch 897/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6973 - d_loss: 0.6920\n",
      "Epoch 898/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6972 - d_loss: 0.6917\n",
      "Epoch 899/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6975 - d_loss: 0.6917\n",
      "Epoch 900/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6971 - d_loss: 0.6917\n",
      "Epoch 901/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6915\n",
      "Epoch 902/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6967 - d_loss: 0.6917\n",
      "Epoch 903/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6977 - d_loss: 0.6920\n",
      "Epoch 904/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6916\n",
      "Epoch 905/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6983 - d_loss: 0.6917\n",
      "Epoch 906/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6973 - d_loss: 0.6916\n",
      "Epoch 907/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6916\n",
      "Epoch 908/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6919\n",
      "Epoch 909/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6969 - d_loss: 0.6916\n",
      "Epoch 910/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6920\n",
      "Epoch 911/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6921\n",
      "Epoch 912/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6919\n",
      "Epoch 913/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6913\n",
      "Epoch 914/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6916\n",
      "Epoch 915/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6914\n",
      "Epoch 916/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6918\n",
      "Epoch 917/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6915\n",
      "Epoch 918/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6979 - d_loss: 0.6919\n",
      "Epoch 919/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6970 - d_loss: 0.6919\n",
      "Epoch 920/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6913\n",
      "Epoch 921/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6914\n",
      "Epoch 922/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6971 - d_loss: 0.6915\n",
      "Epoch 923/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6919\n",
      "Epoch 924/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6979 - d_loss: 0.6916\n",
      "Epoch 925/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6973 - d_loss: 0.6917\n",
      "Epoch 926/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6977 - d_loss: 0.6914\n",
      "Epoch 927/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6914\n",
      "Epoch 928/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6979 - d_loss: 0.6918\n",
      "Epoch 929/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6976 - d_loss: 0.6919\n",
      "Epoch 930/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6978 - d_loss: 0.6917\n",
      "Epoch 931/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6917\n",
      "Epoch 932/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6911\n",
      "Epoch 933/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6981 - d_loss: 0.6915\n",
      "Epoch 934/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6913\n",
      "Epoch 935/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6916\n",
      "Epoch 936/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6916\n",
      "Epoch 937/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6980 - d_loss: 0.6912\n",
      "Epoch 938/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6973 - d_loss: 0.6915\n",
      "Epoch 939/1000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6977 - d_loss: 0.6915\n",
      "Epoch 940/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6983 - d_loss: 0.6915\n",
      "Epoch 941/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6916\n",
      "Epoch 942/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6984 - d_loss: 0.6914\n",
      "Epoch 943/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6914\n",
      "Epoch 944/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6978 - d_loss: 0.6916\n",
      "Epoch 945/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6983 - d_loss: 0.6912\n",
      "Epoch 946/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6915\n",
      "Epoch 947/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6980 - d_loss: 0.6917\n",
      "Epoch 948/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6976 - d_loss: 0.6916\n",
      "Epoch 949/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6978 - d_loss: 0.6914\n",
      "Epoch 950/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6917\n",
      "Epoch 951/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6968 - d_loss: 0.6917\n",
      "Epoch 952/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6969 - d_loss: 0.6914\n",
      "Epoch 953/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6972 - d_loss: 0.6914\n",
      "Epoch 954/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6983 - d_loss: 0.6918\n",
      "Epoch 955/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6975 - d_loss: 0.6917\n",
      "Epoch 956/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6973 - d_loss: 0.6915\n",
      "Epoch 957/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6970 - d_loss: 0.6918\n",
      "Epoch 958/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6972 - d_loss: 0.6915\n",
      "Epoch 959/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6972 - d_loss: 0.6916\n",
      "Epoch 960/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6971 - d_loss: 0.6915\n",
      "Epoch 961/1000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6966 - d_loss: 0.6912\n",
      "Epoch 962/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6917\n",
      "Epoch 963/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6917\n",
      "Epoch 964/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6977 - d_loss: 0.6913\n",
      "Epoch 965/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6972 - d_loss: 0.6915\n",
      "Epoch 966/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6913\n",
      "Epoch 967/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6970 - d_loss: 0.6915\n",
      "Epoch 968/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6915\n",
      "Epoch 969/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6968 - d_loss: 0.6914\n",
      "Epoch 970/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6974 - d_loss: 0.6908\n",
      "Epoch 971/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6975 - d_loss: 0.6914\n",
      "Epoch 972/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6967 - d_loss: 0.6915\n",
      "Epoch 973/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6966 - d_loss: 0.6915\n",
      "Epoch 974/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6913\n",
      "Epoch 975/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6971 - d_loss: 0.6913\n",
      "Epoch 976/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6970 - d_loss: 0.6916\n",
      "Epoch 977/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6915\n",
      "Epoch 978/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6917\n",
      "Epoch 979/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6962 - d_loss: 0.6917\n",
      "Epoch 980/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6916\n",
      "Epoch 981/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6965 - d_loss: 0.6913\n",
      "Epoch 982/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6914\n",
      "Epoch 983/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6964 - d_loss: 0.6916\n",
      "Epoch 984/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6961 - d_loss: 0.6917\n",
      "Epoch 985/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6914\n",
      "Epoch 986/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6963 - d_loss: 0.6918\n",
      "Epoch 987/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6953 - d_loss: 0.6914\n",
      "Epoch 988/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6920\n",
      "Epoch 989/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6919\n",
      "Epoch 990/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6951 - d_loss: 0.6915\n",
      "Epoch 991/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6958 - d_loss: 0.6919\n",
      "Epoch 992/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6915\n",
      "Epoch 993/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6952 - d_loss: 0.6916\n",
      "Epoch 994/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6920\n",
      "Epoch 995/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6959 - d_loss: 0.6919\n",
      "Epoch 996/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6957 - d_loss: 0.6921\n",
      "Epoch 997/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6955 - d_loss: 0.6915\n",
      "Epoch 998/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6945 - d_loss: 0.6921\n",
      "Epoch 999/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6954 - d_loss: 0.6921\n",
      "Epoch 1000/1000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6956 - d_loss: 0.6924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f88303880a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cgan.fit(dataset, epochs=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NnUkQDJ3uC72"
   },
   "outputs": [],
   "source": [
    "df_age_eth = pd.DataFrame(columns = ['age', 'ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "mZC-txU3NOGN",
    "outputId": "4cd8191e-329f-4040-8187-811b114a60fd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [age, ethnicity]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_age_eth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "QWYYAtVAuC72",
    "outputId": "5797fa5c-c1b5-49cc-c16d-49743bdee189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating:  231  ages for unit type: [1., 0., 0.]\n",
      "Generated Ages:\n",
      "min:  16.754235614091158\n",
      "mean:  65.89910831662031\n",
      "max:  88.32518267631531\n",
      "stdv:  17.3657610103784\n",
      "True Ages:\n",
      "min:  19\n",
      "mean:  56.15151515151515\n",
      "max:  90\n",
      "stdv:  16.825009339158104\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Generate n ages for a class\"\"\"\n",
    "print(\"Generating: \", 231, \" ages for unit type: [1., 0., 0.]\")\n",
    "\n",
    "age_one_hot_labels = tf.repeat([[1., 0., 0.]],231, axis=0)\n",
    "\n",
    "input_noise = tf.random.normal((231, cgan.noise_dim), 0, 1)\n",
    "random_vector_labels = tf.concat([input_noise, age_one_hot_labels], axis=1)\n",
    "\n",
    "ages = cgan.generator(random_vector_labels)\n",
    "\n",
    "inv_gen_ages = [(val * (max_age_filtered-min_age_filtered)) + min_age_filtered for val in ages.numpy().flatten()]\n",
    "\n",
    "print(\"Generated Ages:\")\n",
    "print(\"min: \", np.min(inv_gen_ages))\n",
    "print(\"mean: \", np.mean(inv_gen_ages))\n",
    "print(\"max: \", np.max(inv_gen_ages))\n",
    "print(\"stdv: \", np.std(inv_gen_ages))\n",
    "\n",
    "df_ages_class = final_df.query(\"ethnicity == 'African American'\")\n",
    "\n",
    "print(\"True Ages:\")\n",
    "print(\"min: \", np.min(df_ages_class.age))\n",
    "print(\"mean: \", np.mean(df_ages_class.age))\n",
    "print(\"max: \", np.max(df_ages_class.age))\n",
    "print(\"stdv: \", np.std(df_ages_class.age))\n",
    "\n",
    "\n",
    "sns.histplot(inv_gen_ages, bins=70, label='GAN', kde=True,)\n",
    "sns.histplot(df_ages_class.age, bins=70, color='orange', label='Truth', alpha=0.3, kde=True,)\n",
    "plt.title('African American Ages')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "df_temp = pd.DataFrame(columns = ['age', 'ethnicity'])\n",
    "\n",
    "df_temp['age'] = inv_gen_ages\n",
    "df_temp['ethnicity'] = 'African American'\n",
    "\n",
    "df_age_eth = df_age_eth.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "drwqfXpEuC73",
    "outputId": "8cb645a5-8072-42c3-d524-864c6ab6acb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating:  2010  ages for unit type: [0., 1., 0.]\n",
      "Generated Ages:\n",
      "min:  17.947638273239136\n",
      "mean:  64.27453704972692\n",
      "max:  88.85329866409302\n",
      "stdv:  17.289540586291043\n",
      "True Ages:\n",
      "min:  15\n",
      "mean:  64.44378109452737\n",
      "max:  89\n",
      "stdv:  17.41515533822681\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABHJ0lEQVR4nO3dd3icZ5Xw4d+ZGY16t7osS7Zlucg1ju3E6SEhjRQIkMCyCQSyLIFAqAnLLmEX2JCFD9gFAlkCSZb0RipxQrody1Uusq1mW733rtHMPN8fMxrLsmTLsqQZSee+Ll2et593JM+Z96lijEEppZQCsPg7AKWUUoFDk4JSSikfTQpKKaV8NCkopZTy0aSglFLKR5OCUkopH00KSk0gEekSkfn+jkOp8dKkoAKeiHxGRHZ6P3BrReRvInKev+MaiTEmwhhzZLLOLyL3iogRkfWTdQ01u2lSUAFNRL4J/Ar4KZAEZAC/A67zY1h+ISIC/CPQ4v1XqQmnSUEFLBGJBv4duMMY87wxptsYM2CMedkY8x3vPutEZKuItHmfIn4jInbvtkzvt2rbkHO+KyJfHLL8JRE5JCKdInJQRNZ4198tIoeHrL9hyDELReQ9EWkXkSYReWrINiMiC72vrxaRfBHpEJFKEbl3yH6Dsd0iIhXe8/zLKd6S84EU4E7gpsH79J7PKiK/8J7nqIh8dei9i0i0iDzkfY+qReTHImI91f2o2UeTggpk5wAhwAsn2ccF3AXM8e5/KfCVsZxcRD4J3IvnW3cUcC3Q7N18GM+HcDTwI+AvIpLi3fYfwBtALJAO/M8ol+j2njsGuBr4ZxG5ftg+5wE53rj/TUSWnCTkW4CXgae9yx8bsu1LwJXAKmANMPw6DwNOYCGwGrgcGEyOY70fNQtoUlCBLB5oMsY4R9vBGLPLGJNnjHEaY8qAPwAXjvH8XwTuN8bsMB6lxphy73mfMcbUGGPcxpingBJgnfe4AWAekGqM6TPGbB4ltneNMfu959gHPDFCbD8yxvQaY/YCe4GVI51LRMKATwKPG2MGgGc5vgjpU8CvjTFVxphW4L4hxyYBVwHf8D5tNQC/BG46nftRs4MmBRXImoE5Q4t/hhORRSLyiojUiUgHnrqHOWM8/1w8TwQjnfcfRWSPt1iqDcgdct7vAgJsF5EDIvKFUc6xXkTeEZFGEWkHvjxCbHVDXvcAEaPEegOeb/qveZcfA64UkQTvcipQOWT/oa/nAUFA7ZD7+QOQeDr3o2YHTQoqkG0F+jmxKGSoB4BCINsYEwV8H88HHHiKbwDChuyfPOR1JbBg+AlFZB7wv8BXgXhjTAxQMHheY0ydMeZLxphU4J+A3w3WIwzzOPASMNcYEw38fkhsp+sWPAmjQkTqgGfwfNB/xru9Fk/Rz6C5w+6zH5hjjInx/kQZY5ad5v2oWUCTggpYxph24N+A34rI9SISJiJBInKliNzv3S0S6AC6RGQx8M9Djm8EqoF/8FbEfoHjk8AfgW+LyFnisdCbEMIBAzQCiMjn8Twp4F3+pIgMfgC3evd1j3ALkUCLMaZPRNZx7AP8tIhIGp46h2vw1BmswlPM9DOOFSE9DXxdRNJEJAb43pD3oRZPncEvRCRKRCwiskBELjzN+1GzgCYFFdCMMb8Avgn8AM+HdCWeb/B/9e7ybTwftp14vt0PbznzJeA7eIqilgEfDjn3M8BP8Hyj7/SeM84YcxD4BZ4nlXpgObBlyDnPBraJSBeeJ4Gvj9I34SvAv4tIJ57k9vQI+4zF54A9xpg3vN/q64wxdcB/AytEJNd7728A+4B8PMVMTjwV8eBJHnbgIJ4P/mfxtGQ6nftRs4DoJDtKzTwiciXwe2PMPH/HoqYXfVJQagYQkVARuUpEbN7iph9y8qa8So1InxSUmgG8TVbfAxYDvcCreIqBOvwamJp2Ju1JQUT+JCINIlIwwrZveXtbzvEui4j8t4iUisi+wV6lSqmxMcb0GGPONsZEGmMSjTGf14SgxmMyi48eBq4YvlJE5uLpTVkxZPWVQLb353Y8zQyVUkpNsVE7BZ0pY8z7IpI5wqZf4uks8+KQddcBjxpPWVaeiMSISIq3Kd2o5syZYzIzR7qEUkqp0ezatavJGJMw0rZJSwojEZHrgGpjzF6R4/rwpHF8D8wq77oTkoKI3I7naYKMjAx27tw5eQErpdQMJCLlo22bstZH3oqw7+Nprz1uxpgHjTFrjTFrExJGTHRKKaXGaSqfFBYAWcDgU0I6sNvb07Oa47vlp3vXKaWUmkJT9qTgHS0y0RiTaYzJxFNEtMbbM/Ml4B+9rZA2AO2nqk9QSik18SbtSUFEngAuwjPKZRXwQ2PMQ6Ps/hqeoX1L8YwU+fnxXndgYICqqir6+vrGe4ppJSQkhPT0dIKCgvwdilJqBpjM1kc3n2J75pDXBrhjIq5bVVVFZGQkmZmZDKvMnnGMMTQ3N1NVVUVWVpa/w1FKzQAzbpiLvr4+4uPjZ3xCABAR4uPjZ81TkVJq8s24pADMioQwaDbdq1Jq8k1pP4Wp1t/fT35+/oSec/Xq1QQHB0/oOZVSKlDM6KSQn5/PL59+i+TM7Ak5X11ZCXcBGzZsOOW+9fX13HXXXeTl5REbG4vdbue73/0uN9xwAwDf+MY3eOaZZ6isrMRi8TywPfzww3zhC19gz549rFixAoDc3FxeeeUVtOe2UrPTaF9uJ+sL6oxOCgDJmdlkLlk1pdc0xnD99ddzyy238PjjjwNQXl7OSy+9BIDb7eaFF15g7ty5vPfee1x88cW+Y9PT0/nJT37CU08NnytGKTUb5efnU/DBY+TmZPjWFRR5ho4byxfU0zXjk4I/vP3229jtdr785S/71s2bN4+vfe1rALz77rssW7aMT3/60zzxxBPHJYVrrrmG999/n6KiInJycqY8dqVU4MnNyWDDmsVTcq0ZWdHsbwcOHGDNmtFH/37iiSe4+eabueGGG3j11VcZGBjwbbNYLHz3u9/lpz/96VSEqpRSx9GkMAXuuOMOVq5cydlnn43D4eC1117j+uuvJyoqivXr17Np06bj9v/MZz5DXl4eR48e9VPESqnZSouPJsGyZct47rnnfMu//e1vaWpqYu3atWzatIm2tjaWL18OQE9PD6GhoVxzzTW+/W02G9/61rf42c9+NuWxK6VmtxmfFOrKSib2XOsyTrnfJZdcwve//30eeOAB/vmf/xnwfPiDp+joj3/8Izff7Onw3d3dTVZWlm/7oFtvvZX777+fzs7OCYtfKaVOZUYnhdWrV3PXRJ5wXQarV68+5W4iwl//+lfuuusu7r//fhISEggPD+dHP/oRd911F7///e99+4aHh3Peeefx8ssvH3cOu93OnXfeyde//vWJvAOllDop8Qw7ND2tXbvWDJ9k59ChQyxZssRPEfnHbLxnpWaLvLw8aPjguNZHebsLIfH8cTdJFZFdxpi1I23TimallFI+mhSUUkr5aFJQSinlo0lBKaWUjyYFpZRSPjO6SaoOna2UUqdnRieFkUYXPBNjGZmwubmZSy+9FIC6ujqsVisJCQkAbN++HbvdPuqxbW1tPP7443zlK18BPAPn/fznP+eVV16ZkPiVUupUZnRSgKkdXRAgPj6ePXv2AHDvvfcSERHBt7/9bd92p9OJzTby297W1sbvfvc7X1JQSqmpNuOTQiC49dZbCQkJIT8/n40bNxIVFXVcshicSOfuu+/m8OHDrFq1issuu4yrr76arq4ubrzxRgoKCjjrrLP4y1/+olNwKqUmjSaFKVJVVcWHH36I1Wrl3nvvHXGf++67j4KCAt+Txrvvvkt+fj4HDhwgNTWVjRs3smXLFs4777ypC1wpNatMWusjEfmTiDSISMGQdf8lIoUisk9EXhCRmCHb7hGRUhEpEpGPTlZc/vLJT34Sq9V62setW7eO9PR0LBYLq1atoqysbOKDU0opr8lskvowcMWwdW8CucaYFUAxcA+AiCwFbgKWeY/5nYic/idoAAsPD/e9ttlsuN1u33JfX9+oxw1t6WS1WnE6nZMToFJKMYnFR8aY90Ukc9i6N4Ys5gE3el9fBzxpjOkHjopIKbAO2HqmcQy2GJoIBUUV5Cae+XkyMzN9LYp2797tm0wnMjJSh8pWSvmVP+sUvgAMzk6fhidJDKryrjuBiNwO3A6QkXHypqZjGeb6dOQmTsw5P/GJT/Doo4+ybNky1q9fz6JFiwBPy6WNGzeSm5vLlVdeydVXX33G11JKqdPhl6QgIv8COIHHTvdYY8yDwIPgGTr7ZPsGBwePe2jZiTBahXJoaChvvPHGiNsef/zx45Yvuugi3+vf/OY3ExWaUkqNaMqTgojcClwDXGqOTeZQDcwdslu6d51SSqkpNKVjH4nIFcB3gWuNMUPnn3wJuElEgkUkC8gGtk9lbEoppSbxSUFEngAuAuaISBXwQzytjYKBN70dsPKMMV82xhwQkaeBg3iKle4wxrjGe21jzKzp4DWdZ85TSgWeyWx9dPMIqx86yf4/AX5yptcNCQmhubmZ+Pj4GZ8YjDE0NzcTEhLi71CUUjPEjOvRnJ6eTlVVFY2Njf4OZUqEhISQnp7u7zCUUjPEjEsKQUFBZGVl+TsMpZSalnSSHaWUUj6aFJRSSvloUlBKKeWjSUEppZSPJgWllFI+mhSUUkr5aFJQSinlo0lBKaWUjyYFpZRSPpoUlFJK+WhSUEop5aNJQSmllI8mBaWUUj6aFJRSSvloUlBKKeWjSUEppZSPJgWllFI+mhSUUkr5aFJQSinlo0lBKaWUz6QlBRH5k4g0iEjBkHVxIvKmiJR4/431rhcR+W8RKRWRfSKyZrLiUkopNbrJfFJ4GLhi2Lq7gbeMMdnAW95lgCuBbO/P7cADkxiXUkqpUUxaUjDGvA+0DFt9HfCI9/UjwPVD1j9qPPKAGBFJmazYlFJKjWyq6xSSjDG13td1QJL3dRpQOWS/Ku+6E4jI7SKyU0R2NjY2Tl6kSik1C/mtotkYYwAzjuMeNMasNcasTUhImITIlFJq9prqpFA/WCzk/bfBu74amDtkv3TvOqWUUlNoqpPCS8At3te3AC8OWf+P3lZIG4D2IcVMSimlpohtsk4sIk8AFwFzRKQK+CFwH/C0iNwGlAOf8u7+GnAVUAr0AJ+frLiUUkqNbtKSgjHm5lE2XTrCvga4Y7JiUUopNTbao1kppZSPJgWllFI+mhSUUkr5aFJQSinlo0lBKaWUjyYFpZRSPpoUlFJK+WhSUEop5aNJQSmllI8mBaWUUj6aFJRSSvloUlBKKeWjSUEppZSPJgWllFI+mhSUUkr5aFJQSinlo0lBKaWUjyYFpZRSPpM2HadSSp1Kf38/+fn5x61bvXo1wcHBfopIaVJQSvlNfn4+BR88Rm5OBgAFRRUAbNiwwZ9hzWqaFJRSfpWbk8GGNYv9HYby0joFpZRSPn5JCiJyl4gcEJECEXlCREJEJEtEtolIqYg8JSJ2f8SmlFKz2ZQnBRFJA+4E1hpjcgErcBPwM+CXxpiFQCtw21THppRSs52/io9sQKiI2IAwoBa4BHjWu/0R4Hr/hKaUUrPXlCcFY0w18HOgAk8yaAd2AW3GGKd3tyogbaTjReR2EdkpIjsbGxunImSllJo1/FF8FAtcB2QBqUA4cMVYjzfGPGiMWWuMWZuQkDBJUSql1Ozkj+KjjwBHjTGNxpgB4HlgIxDjLU4CSAeq/RCbUkrNamNKCiKycSzrxqgC2CAiYSIiwKXAQeAd4EbvPrcAL47z/EoppcZprE8K/zPGdadkjNmGp0J5N7DfG8ODwPeAb4pIKRAPPDSe8yullBq/k/ZoFpFzgHOBBBH55pBNUXiako6LMeaHwA+HrT4CrBvvOZVSSp25Uw1zYQcivPtFDlnfwbGiHqWUUjPESZOCMeY94D0RedgYUz5FMSmllPKTsQ6IFywiDwKZQ48xxlwyGUEppZTyj7EmhWeA3wN/BFyTF45SSil/GmtScBpjHpjUSJRSSvndWJukviwiXxGRFBGJG/yZ1MiUUkpNubE+Kdzi/fc7Q9YZYP7EhqOUUsqfxpQUjDFZkx2IUkop/xtTUhCRfxxpvTHm0YkNRymllD+Ntfjo7CGvQ/CMV7Qb0KSglFIzyFiLj742dFlEYoAnJyMgpZRS/jPeobO78cyHoJRSagYZa53Cy3haG4FnILwlwNOTFZRSSin/GGudws+HvHYC5caYqkmIRymllB+NqfjIOzBeIZ6RUmMBx2QGpZRSyj/GWnz0KeC/gHcBAf5HRL5jjHl2EmNTSqlJ0d/fT35+/nHrVq9eTXBwsJ8iChxjLT76F+BsY0wDgIgkAH/HM4OaUkpNK/n5+RR88Bi5ORkAFBRVALBhwwZ/hhUQxpoULIMJwauZ8bdcUkopv8vNyWDDmsX+DiPgjDUpvC4im4AnvMufBl6bnJCUUkr5y6nmaF4IJBljviMiHwfO827aCjw22cEppZSaWqd6UvgVcA+AMeZ54HkAEVnu3faxSYxNKaXUFDtVUkgyxuwfvtIYs19EMicnJKXUTDAdW/icbszT8R5P5VRJIeYk20LHe1Hv2El/BHLx9JT+AlAEPIVnHugy4FPGmNbxXkMp5V/TsYVPfn4+v3z6LZIzswGoKyvhLkaP+XT3nw5OlRR2isiXjDH/O3SliHwR2HUG1/018Lox5kYRsQNhwPeBt4wx94nI3cDdwPfO4BpKKT+bji18kjOzyVyyatL2D3SnSgrfAF4Qkc9yLAmsBezADeO5oIhEAxcAtwIYYxyAQ0SuAy7y7vYIno5ymhSUUtPWSMVLENhFTCdNCsaYeuBcEbkYT1EPwKvGmLfP4JpZQCPwZxFZiSfZfB1P/UWtd586IGmkg0XkduB2gIyMjDMIQymlJtfw4iUI/CKmsc6n8A7wzgRecw3wNWPMNhH5NZ6ioqHXMyJiRjrYGPMg8CDA2rVrR9xHKaUCxXQrXhpr57WJVAVUGWO2eZefxZMU6kUkxRhTKyIpQMOoZ1BKBZSRikkKCgpYHOfyU0THGx5fIMUWaKY8KRhj6kSkUkRyjDFFeKb2POj9uQW4z/vvi1Mdm1JqfIa3NAI4vGc7ETlJwDL/BeY1PL5Aii3Q+ONJAeBrwGPelkdHgM/jGUvpaRG5DSgHPuWn2JQ6YzOx/fqpDG9pVFBU7sdoTjQ0vrHG5nQOUFBQcNy6mf579EtSMMbswdOKabhLpzgUpSbFdGyjr07UWHWUp1tb2d8TBZy6knh4EikoKMDtCp+KUCeMv54UlJrxpmMbfXWi+LSsMVcUD08iB7buJGFBLvMnMb6JpklBKTWjjFbpvTC6n8KiQgBqa+uIChMGBgYICgoa87nH8iQwNInUlhWP8y78R5OCUmpGGa3SuzsGtgWFEJuUSnFDFxZHC++8+w4ZGRmUlB7h4NYWAGp6jo3g43I6sQ4590x4EjgVTQpKqRlnpErv7vZ6YpNSSUrPIiq2mp6GNjYfrCS9P4x386tocHXijkikPyKJ8gYXXQ7BYV+GzWmjcncVsWF2uixxxKXGT+sngVPRpKCUmpEGBgY4fOQw4CkucvQ0k54yz7fdYQnjSMQG8tqyKU3diFs8zwQR0ktqhBDd205ts4PgyFhcbkNRfScO2zyOugz1u6tYmR7DTOw9q0lBKTUjHT5ymBc37/MVFznbGoic10lTSwg7g8+nISMF02slWQbIHCgmw1bHgrAmQkKCWZS7mqLCI2wqbicxZjnr1q7GGMPbm16mLTiZ9t4gXt1fS5gth0x3u79vdUJpUlBKzViDxUVhsXUcDVnAY50X0H4wgmBLL+nt2zhvbh8bVmbzxqY9RNoEuxhg5D4IIkKY6SPS0sYN555NcV0n7xyo4KA7Bcuhei5YlDC1NzdJNCkopWYG44b+FkIdh7G5jhLqgHjpoKgmgn1RF1AbnkSitHFLThMNu/9OX2sJcZkrx3UpiwiLU6Jo2nOImuBMCmqgtqOPNOxMr14JJ9KkoJSaXgY6oaMQ2g9CxyHPv+2HoLsMjBPfx3wvzIv0vPzuMnC4LHS4oukMX85bcYY9rQNnHIoVQ6a1lfW5S3n9QB0Hgxax2EzvYds0KSilAo/bBT0V0FkCHcXQWQwdRZ4k0FN5bD9LEEQugrjVkPFJ+oMSeG53G+9WCd2uIFLtHZyf3kvf0a1EOKvJSbcxb2AbX8zpx5UNR/paaHLaEdxwXOPT0zMvPpxPrZ3LUx8Wc9CVwuK2XtJixj05pV9pUlBKTRmLu9vzrb6vDnrrSG7LI6h/HxS9Do5WlvfUEVR+Hxxt8xQHDbJFQGQ2JF4A0Ushaonn34gFYLHR3e/k0a3lPPj+YVp7BlgV08mtS5toaOjARC3lvboWehqsLA1eSc6yFRR/+BJLQ/Zz4fw6snt+Q/bqcN5tWM6R/qxx31tsmJ0lA8UU2nN4cU81H1+dfuZvmB9oUlBKTayBDug6ytLwfSSEtMC+zeBoYW1PA7aufs8M7F6ZgBsrtMWCPRaHxNAVlk1SxnIIz/QkgqhFEJIMIidcqsfh5P+2HuYP7x+hpdvBRTkJXJrUT7bzIDHh0DBCSY4RK0e7EjhwJIPC4Ks4P0dI6HqBGzPyaOo/yOaO9WBWjevW7ThZZq2j1L6Al/bWkD0N6xg0KSilzoyjlfiuN4jqexV21vqKd86NgX53MLjmQdhcGgcycESuYt7icyE0BUKS2VFQiatpDxvOWgJA0e5CSDifpJWjDxzY399P3o7dvHm0j5dLe2nvN5y3MJ5vXp7DmoxY8vLyxjwbiwsrNUGreXh/DxuSq7gqbQ/XJ7xJfU81tdbzxvV22MXFdatSeXpHJcVBC1hh6sZ1Hn/RpKCUOi39/f3s2/kuc7reJK77XSL79pCNCychEJULiRdBZDaPvXEEqz2Mmy66AoDy3YUQcz7zMo994Lus7SDi62hWVl5OV0Ms4BmiGjhuHKM+p+GxvDI+bLbTbUJYHt3FmuACrsy9mjUZsWdwV8KB9gxqnWmsiS/norhdfCGhBFfWWraMY86F2DA7V69I4bldVRx2z2GDMcgITzqBSJOCUmpsjIG6N+na/jNWdb5HkMVFnTON7Y6P8dIu2Lg0lZsuuMK3e6+7nogxnnqwo1lrNxxx1vHmwXru8m4r+OAx0rOyeL02jjfr4+h2RrEguIn7rrZzdmoYebujJu4WsZDflYs742oWND3EHblbuai7iQ9dq+iyJp/WudJjw0h31VIlqRRUd7A8PXrC4pxMmhSUUifn6ofyJ+DQL6C9gChLNFv6L6cq9Z9otHmKfeoPPg2cWTFJbFIq0mXotS/yravudPGhdT2bd8cw4IYrFkBS41YWR7axKuESCotGfro4U72WOJ5r+TjU7+DOlVuZ33odb0b+hK2neZ4Udz2dEsN7JY0kR4eQEBn4k/NoUlBqlhs+1LTD4QAg1NpPUsfzJHc8g93VDDHLYcPD7K7P4oUddWR6EwKAy+Wivr7eNzS1Z50bz4SKp8cYqOkx/Osb1ZT0hGAjiivSW/n4vGbOz83gkWfbgBOfLl7fV80V3mGtI3rLiYsQjOtMPoSFTVU51EZewN2Ln+fajjsIyTibx5pvAsDpcNBcW4klpJWyQ3sAaK6tJH5e+JAzQLalkUO2SP5WUMtn1mWMcJ3AoklBqVlu+FDTWz/YTGJoLZ9edJAwSzd72xdiz/k3llz8NRDBNOSdcI62hhr2NtUTmt4EQGt9DY72XqKzxl7k0um0s7Mnif39ofS6LFhd/aRSyzrJY6ktnh27a0gNO77D2dCni9qyYp7ecoiY2Fjm29qxORqJTp7H6RX6nKjOkciTMU+ysfuXXJ74J5ZEVfGOM4ddh7uJ6d5LSmwyaQ5PH4eq7r201h9faBYkbi5fmswL+dVsL2sh9QzjmWyaFJQKUFM5z3NuTgYbVmZCzausXrmJYEs/h+2XsDX8TnY09XNzWMaITUKHCouOIyn9WDv/+vbDvtcDAwPHFfMM3ovTbdjfGsEHJTFsbYzEICTYXaT1HSU+BJKTEkl1eM7rcjmpqKjwTZBT4X0ScDrd1NYUe7+1ez6QY+ITkb6JG8PULXY+iPger+918bX5z3Nz643Ygm/FmRjNvIwkFi32zKhQXriPDteJx2fEhbEkJZJd5a2ExLoCet5nTQpKBaipmudZ3H0kOd6H7T+BgXaaBuby421rsS+9CeintqyYgrCO4z64nM4BX5EJQHtTPSbaPfIF8BT1fLC3lMaoZPZ1R3G4opq5+7r5sKqfDkcG0UFO1kdVscBaQ1doLrVdbVgk5rhzdDTVs7mri6MNfUSFQGNXJ9HJ86iu7iak9SCLw0HsIdRV9tFmSyB2EjoI7OtYyN1Hvsc9y17mM5G/Z+HSFD4cSBvTsednJ1DW1MPuJit1hw6NOu/z8C8DBQUFLI4bIdNMEk0KSgWwSZ3n2dUHpX9kdeWPsLuaIGYlZH6WP/1xGw01VaxfuBOAUFsttYf2kJ+f6/vgaqw6SuJAIRnzPIUhkX0l9HTGn/x6cVm0Ry+jqC2c9qBwisv6OCvZznlRpVyY1sXWw020dRm6TnKKyPgkovq6iQwVIsOPPQmkpiYQG26wBIdjrN1n9LacSpszmuej/8yi8h9yRdIzLHQ+z15XKh3Wk/dgDg2ycuGiBF4/4GIgZeWo8z7n5+fzy6ffIjkzG/DM7vbFc20wjqax46FJQalZpr+ng5oP/p3UtkcJdjXS7FpEY+jHWbHiGu8e20hOjCHHWyRSHyFEDZz4QZsxL9W3z7b3I0e8VpfLziP74Im9mRRalkKnIT3WRlZwF3duTCYsyAINXQSdfn20Xxmxsqn3RoqLyvjiigNc0P0zCkI+icvlor2hZtSK50VJEWzd304lsfQ4nITZR/4ITs7MHja729R1gPNbUhARK7ATqDbGXCMiWcCTQDywC/icMcbhr/iUmnFc/XD4Icj/EVmuBjosmRwOuYH/e7uZlTlBrJigy/S7hANdCeyyZVFTl4a7zsL8CAur3PkkpuSwJHcRZYeaPAlhmitpi+ORuk9w47xdrOx7gs/lxNHSmEaaYy5wYsWziJDhrKbAvoS8Iy1csjgRp3PguDqGgoIC3C7/DY7hzyeFrwOHgMGeJz8DfmmMeVJEfg/cBjzgr+CUmjFcfXDkz3Dgp9BTRX/wCg4HXcfSs69hqQgLDm8640sYhJbgTP5SFMfe5lAcbgvh0sUFEUe552NZmKYjPLypiF7rolOfbJrpcYeSF3YH2Y432ZD+V3Ku76ZoziU02RaPWPEcSj9J0kFBtbAyPZrGqqM83drqq2M4sHUnCQtyme+HewE/JQURSQeuBn4CfFM8/b8vAT7j3eUR4F40KSg1fo42KHkAin4NffUw5xxY/ycOlkVA4+ZTtiYai7oeG1vrIti36Bs4bRGEtrg5K6GH+XKY4IrdLIpOJic+i8ImcDpdvonuByuvAW8l6viHrQ4IYqEk+KM8+nIJXz/3CBt7fs1h+8W8Y7HCCHXE6ZY2WiWWD0qaSALi07KGFRf5j7+eFH4FfBcYLIiMB9qMMU7vchUwYpW+iNwO3A6QkRH4HUGUmnI9VVD4Kyj9Azi7IOWjsOQ7kHSJJxGUn9jP4HT0OQ2Nljieb5tPXVM0FjFE9RSR7jjErdeuJcgC9VXt1A87rr6hhRA2k5baQaitlojeOvYW1RORk8Sc6HlnFFOgKGmJ4N4PVvO96+wscLzDPWeH8eOCE6fpDBI367Li+KCkiWCJ5BRV9FNqypOCiFwDNBhjdonIRad7vDHmQeBBgLVr105cQ2Slprv2g3Dov6DsMc9cBBmfhqXfoT9siaeJY9k2YPxNHMvbnbzy1/08t7OVXts8YkwP12a2sS6xmxcefoaoSDtBlrUnPUdqagI5i+dTE+oiLsxBmN1QX1/v63Mwnh7QgcbhtrI/5NPU2laypO8P/Grdk+zsjiYv7A7cYvfttzI9hn1V7VS604gLoJFU/fGksBG4VkSuAkLw1Cn8GogREZv3aSEdqPZDbEpNPw2b4dD9UP0yWENh4Zdh8TchIhOA/Ly8cTdxdBso6o5nV9syyqrbCbK0kxPeQ0vJbj622MHS9OxxhTxanwMn4UM6orXi6mkjJWnq2uhPpCbbYn6y7VwuX1jPR9N+T5bjPV6P/Jlvu9UinDM/ntcPDNBsjlUsu5xOWquPHteCyZUxdclyypOCMeYe4B4A75PCt40xnxWRZ4Ab8bRAugV4capjU2raMG5PEjh4PzR9CMFzYPm9kH0HhMw5YffTbeI4IHberorkg9oIWvrnEmE6Wefewe3rImmqPsojdTtpT8wGxpcUYOQ+B9XVDSN2RDuT6/hTnyuInx/8KH1LvsJHOv+Nz7Z+AmvyRbza4Wn+uygpgvf291JJDG63wWIR2hpriXUU+obOKG7dS1PT3CmLOZD6KXwPeFJEfgzkAw/5OR6lJs1IQ1jAqYc7EOOAw3/yFBN1FHpmJ1v7G5j/ebCFec6bd3ydweAAd2NxpBU+tG6gOG4BrjI7C6L6uCiqiOjqHeRkJfGRdWdRWOTm1W2lJz2Py+WkpbWF+jChsKiQiooKcI/e43moqeyINlWOBH+ER4PWcGnnvXw2fRNn95SwxZlDmy2LdFctJZb5FNZ3sjTF0wIpOSnW1wdk/559UxqrX5OCMeZd4F3v6yPAOn/Go9RUGT6EBZxiGAv3AEkDW0mt+C842gixq+HcJyDjRrAc+2884tAYsSs51vL7RMZAcd8cbn1ZeLdcsFgWkdpfzM0bYkmPGKC+qpl6Tq/6rqOpnpKaFhxWO6ElTVQUltDb13da55hp+ixxvBr1a/62/V+5bd6r/EPr9WyO+DZbTRDh9LPtSDM5SSN3ApxKgfSkoNSsMqYhLIwL6t6E8qfI6m+gI2QlwRc8BskfGbVJ6fDzFowyNWW3A54vgj+0n0+TK4KEXsNd69zUb36a9q4m0iOuH+edeYSERxEZl0BSehat9dVQUXFG55sRRNjSsoIi50q+k/sGF3f9mLjsBfy05qts68vmQE27vyPUpKBUQDKGuK63mdvzSyhugoiFHJKP0Z5yGxtSzhn1MIfDQU15OTHeesuy8nIO17TgTjzWX7kPO6+0LeYnDwudDiHF6uRTsXv58U3LsVvhPzb34f+Pppmt1RnNX6MeZHnf01zg/nd+t/BH3NP8I7aW20jAv9N2alJQKtC0HYBdd7Ko4W26JYmq6FvoCl7K3kMVdLUcYPWaNaPWOxQXF5O/t5SOIE9WOFLZzlv51SxYl4a1uZu9Ve0cDVqKpdtwTTbcutLNrre3EhkKduvyEc95JvUDs53L5T5uLCTwtC6yAoiwP/TTbNpWx7ezn+aB5G/xI9dt/C08nakc62g4TQpKBQir6SW96Zfwt+cgKIqj8d9me3E3ZcWdxCY1c6SynZ2VO8nNzT3p8NmDRTYAdR1W3FlR7HWlkbenhtAgK6nuOm5NO8TtH70IgN2n+GKq9QPj19jcQaKzkTSHZ8TZivIa2prCSYw61kKsvDeF7x/5Dt9f8Tr/nvYHMm1XsbskyF8ha1JQKiA0fsjK7v8hyPRQH3UdlXH/xJ5DVYS6dhGblEpSehZtXYY456knqTdAUVsweXXh7GlJwx1nIZx+Ll+aRHZiBDve2EmUtf+0wtP6gfFLmBPpa0kEcKjpxMK5HncYL0f9lpWN/84Xkp5kkW0RreZS3DL1yUGTglL+NNABpQ9C4/t0OBJ4oXQ9y9edC837ObxnO8kxEJruSQROp5Pm2srjRtQcbG5qt9tp6HbxUmk/h+VqugsiCbW6WRZSQ+uRUuZmzGNJSq4/7nDacjqdtDfVYwmJ8BX/+Ip+JoERK/lzfkjeHiv3zH2Mxp7fsC3sjkm62ug0KSjlJ7HOA7Dzp57xieZ9lhc/jCQyyeprOVRQVE53+7ERhKqrG4jpLiWiNwgaWgF4/s391IZk0hK9nJKuMCCT+IFKPp7rYEV8L0dLDrNpoMMftzftVVc3kGRKyQqFNId1xKKfiSYWCy8UL6LWfJtfzf0F63oe4HnLySfvmWiaFNSMNJXzG4+Vw+EgLy8Pi7uP2NIfkGPbQq81heqoz+Hsm0tN7TbCg90UHCjAZrNRW1uHo6eZ9JRjg8UlJ8Ywf+F8DluyealY2Bq+BBCWhhi+t9zN0feeo7ujjrWJ1/vtPmeSxPiI4+ZgHqnoZ6JF9NTw9+YL+GnIV/mXxN/wtTXNbG46f9KvO0iTgpqRhk9pOHweXH8oLi6m4HAB313yPMnWKt6uy+W9yhSMrYL0BaEUN3ThbKukzekmfUGOd7mBqMxOYpxClTWT2qgNvP5+Ji4jzI8xnB9aytmRtdz5qfMA+I/3uhjaB3ikIpDGqjJqglsoLCoEoLq6hshQfMstrS0Yi/8qOgPV8PeytqyYePfEj8skGBY69vLHuo9y1pwOrkx8hIiQbWA+BzL5Q4xrUlAz1tDxfgLBfOsO/mHZo7itofy+7R663AOExdRgCQ4nKT2LqNhqegbaiIxPIik9i/DYWqqCknm9ey0V25MZCEknxNXJDRnN3LohjmVz4KEnS4k4yef38CIQgH21W9nRH0TEPE/C3F5QRFT4seXiynoSkwJpMOfAMPy9rK7cR5+lc1KulT5QQkXMudxfcS3NPXn8w9IiOPp/MP/WSbneUJoUlJps7gHm9jxPWnAehV0ZPO64m8IjzaxJ6juhm5IbC+UDCWwvjiM/7OM4w4MIdfazIbkb55E8YrsL+eKiS1mcEDfmyw8vAtn2fiQhEXZfs9WQsBOX1ciGvpfl5TX0NLRNynUsuLk0rZPnjsTyVMO5rEgeYAXPQ9hcSL50Uq45SJOCUpOprx4O3U+au4R3yubyZtt5xCQexV25jzZbArHhnrGHStuD2W9fS23GtQx0hxPa5ybZeZT4pq2cuziaxQtW8UZpk7/vRk2hDUndvFEZRWXUWt6ra2fF/Ego+S2EpjCZ805oUlBqksQ4D8LuZ8EY3my+koM94WTOTycpPYuy8hqaieOgI4PDffPo3h+K1RZHfHcha5O6uXRlKm+/uZ2evqNYZKW/b0X5gd1quDitk5fK5lFjEmDJjbDn23DwP7EH3cHYx749PZoU1KjGO7zzTDHuFkxuJxnNvyW17y8QMR+WfI/yv+4D6mlwhLGzPIrNodfQY4nE4nIxz9bABQu6qdv9No7GIrLSV2KzpE7ejalpY2NyF68dCWGHrICgCFj2A9j9TRa6HueQuXJSrqlJQY3qtId3nmHG1YKppwa23ERq+wfU29aTtOo7lHfa2dzTzX7nBpprYhEM8aaOXPdBcsMqCQ2xsyhhNW/gmrRvf2p6CrEZUrv2URq1juJmN4vi02HRHUQV/oL01geB8yb8mpoU1EmNaXjnGWCkp4KCggIS584fewumurfhw5thoIudUf/K85WZHHg2mL0NAiwilXoujyvh/IVh5L3zPpGhQrAYwH6qM6tZLLVrL7VRq/ndLhu/utxA4oVUHCmgNeJy0ibhepoUlOLEpwLwzGWcsCCX+Sc5DgC3Cw7+J2bfD2m1ZnFP48/ZlO9pHZQd1ctt2R10HXgPe08F6XPPIsq+cMLidrnc1Nc3ah+DGSzI3ccKKeKlkmV8cz1kREON/VKYwL+joTQpKOU1vF+DZy7j0bndhkNHi4nY/XnmDWzlxdYL+X71V1k6N5XP5faT2vgqHR31xA6kUtxQ7+uIljyBMTc0NHCkuorQdE/q0j4GM9MaywH2m2U8sFv4z4tPbxa806VJQanT0O1w88q+Gt4pbKSv8m/cO+dnRFh7+YPjHkJXf4l3PpdCUlQIeXl5lLX2YfWOcDrYMe1MuY3bM5+BV0trC6ER0drHYJoYqYd5c20l8fPCRz3GuF30NVdyWW4rzx6M4eq4UnC6Ju3DW5OCmtUG6xIKCgqoLfNMbJK+YCk2u6ec3xiob+1iX2k5lW1uXqppwUITd6c/wW0pT9Flz8ax8Sn+KWWV71xH8dRHdNfXEzJk3KKJ0NfVweaDlaT3hwH6ZDDdjNTDvKp7L631EaMe09vVSWl3H6uXleI0Z3P/h24+k1xF5iQ1UNOkoGa1wRZWESGG+bZ2ukoKKXODJWkhZdZ02l0x5O2uBezESDc3xX7IHYmPkB5cAwtuI+qs/wZbmO9cg/UStWV1hFdXsjQibkKLiwDfMBigTwbT0fAe5uWF++g4xRBKIRFR5GSlsNbRw56mbLpdVZMWnyYF5Xf+6A8x9AkhPNgwYA+nKW4hRd3xVNeF4a6rxWKJI1r6WBzlZqVlF1clbGFdyBZc2ClJ/A+y1//ghPMOrZdwdsRMSuxqZhk+ZWd7Uz0meuTpTj8yt4MdjWFs7s5k2STFM+VJQUTmAo8CSXgmiXrQGPNrEYkDngIygTLgU8aY1qmOT009f/SH2LZzNz95fgetlih6LOfRJZ5v3DHWbrLCnaxYlEnltlcJCosmN8rJreEPk2avod41n63tF9DUk0xzXt6s6cinJs/wKTsj+0ro6Ry5SDA5zElOWBNbu+Zxi3NyKpz98aTgBL5ljNktIpHALhF5E7gVeMsYc5+I3A3cDXzPD/EpP5iK/hD13S4e+bCMd4oa2FLSwkDQAqxiSLe1sJodZIa0ExkcRLV9LRlxYTSIg+vn/I3rYt7EZay8VHcuO2vn4BAnR5x1vHmw3u/DcauZYeiUndveP3mR4IUxZdjCBwixfWxSYpnypGCMqQVqva87ReQQkAZcB1zk3e0R4F00KYzZ8CKYodM0Dppt32pdBipaeihr6qakIZQXa9qANrLmhHNpZgjtbW3EB7vIGCjAdNdgsYZjiAYMWf3vcPOy/yE5pJXd/eeQ17qKzs4OIuPDMbZoeu2L/Hx3araaY+8lSrpPveM4+bVOQUQygdXANiDJmzAA6vAUL410zO3A7QAZGRkj7TIrDS+C+dsb2wkLNlx24Xpg9OKYQJyhbCxGi7u51827RY08n9dBfn0YrtpqrBYhPsjNJ5ZEcOtH15E1J5y8vDye2N58wnlTgmq4IepVsjsOUmUS+HH5V2kMP580905g5GktR2rBNFkTsCg12fyWFEQkAngO+IYxpkPk2MjyxhgjIiMWmBljHgQeBFi7du3k9uKYZoYWwRQUlRMRwimLZAJxhrKxGIx7TkY2LQ4LZY0dBL3dRoW3GcecUAtzQ50sz85gbmwY1cX7uHJBKFlzRm4PnhzczLlzNpMdVk6nO5p3In7Ag7usEDqHxNGbkPtiGdqCKc7RMakTsCg1mfySFEQkCE9CeMwY87x3db2IpBhjakUkBWjwR2yzUaDNUHYy/U4XeyraeOZQD6URy9lRb8VtQKwhLLUL379qMRfnJNJ0pIAnd1SSOWf09t+Cm8VBe7ky8inmBVfQ57bzQcd5vOT4EqlJG3CZpxnr5Ie5ORnEhENHUBNJ6VmTOgGLUpPJH62PBHgIOGSM+X9DNr0E3ALc5/33xamOTQUWYwzlzd3sqWxjT2UbeyvbKKjpwOF0I0B0EKzOiGVubCgDdSV8bkMGGzYsAKD56PA5zYboriSt9U/8IPZF5lgb6HBF8nbjGvb3raDfmoDDHjI1N6hUAPLHk8JG4HPAfhHZ4133fTzJ4GkRuQ0oBz7lh9hmPadzgIKCAsDTKzeit5yzli8gKCjIu91FoXf7oNOtgxheH1BQUMDcKMPOWihpgZIWYUdZBmW9rXS++C4AIUEWlqdF848b5rEuKw5L8xFe3lNF5sI5AJSd4rkyztJActsWePO70LiZuRiKXLm81vNpGjqDcHXXYwk+vdFKh75XEb3lxEUIxhXYdTFKnYo/Wh9thhOmph00uZOPzjIul8s3emZZeTldDbHAyT/EG6uO8nRrK/t7oqgtqyOho5RVS+exOMdTN1FVXcWTH5axrCcKOHUdhNPlpqali8279tHW56atz82hsmqamqqxRcTR4Q6mticShy0FdnuOCba4SQ2xcFayncvPWsSy5DC6qkuwWgTogM4OCooP4naNUthv3IQ4Klgb/AGrOp8lfWA7sXHl0ALErITlPyS/bTkP5zsBSGPn6b+5Q96rmNhY5tvasTkaiU6eN+E9mJWaStqjeQZramrixc1dxCalcqSyfcxt6+PTsnx1DKFVRSdsj01JJ3nhcnocLhr7LWyp6qfggyM0dvXT2NHv+bfT89PS48Cc0BwgBpsEE+1yE2EdIKytkGRXIxuWpzMnqAd381EyY6PJXPMPbFifQV5eHv/97NtDhrU2HN6+lazseaQ5DFHuKhaH7mB+Qy+80QJt+1jl7GZVJPT1R1EdtJZ32i8ka8m1rD7/EwD05+UBFcMDO23xaVkkJiYS5+jAdGu7BzX9aVKYQZzOY08GtbV1OHqaSV9+FknpWbR1mTG1rTdAv7FS09ZLda8VJzk8UJhE32GhvhvKmi6kPSiEHe8f8R4RyofNXcAhbBaICbaQEhtOBH2kJ1iIDg6lp6WOJkcQ87NzCLNbefeJ37AstpbLLvgoNnp5rfgFkmPhsqyN2E03vaFVWDp34N6+ldLiIGJ7G/mPHAfx0TZCTCchph3rVZ5v+bR7wwgHR+8cCFsK82/jcGskzxZHEZz9MYxYKa3YweqiVvqD8gBPkZXbFY7FOtaqZKVmB00KM0hVdRVb9rcTm5RKcUPXScfvN8ZQ2dLL1up+CjuCOLi/lpYeBy1BKzEuC7t3VQEhYFlNcY2LtGhICocseyu1DhvZi5cRbrdxZPd7zO38kI3L0wkVJ4WHqyk7HEGnMcTPiyIuqB2aCrhsfiKLJYKI/jpuvaiQuOAewjtfxoKbq6/xBtW71/NvGLhDodthxeEKpcNl6OgPoty+BFv0evokhqLiMnqs8cxZcikdlnT2lzTyqXULfE9AjXl51LgqyBTPh/7QYjE4NoFOYmLiab/PTqeT2ppimmsrsYS04uppIyXJNWqZqFLTiSaFGSZ2lPH7B4yFhj4rzQ4LP/2wg4o33qStZ8C7NYhoVz9x4XaC2isJC7KyfMUKOqpLia3/G7dftsRTp2DcvPDiNrY4Qjk32RDubqQy4nky51QzPyaCMOnmI8tbiAnuJWjoF/BscLmF7oFkuizJlHbFUNkVTeK8JTgknLffy8dlCeWsi67GYQmnYO9+tuTXsWBhJudcsJHCXZuprO/CuvgzZKavAmBr/dNYw2JYZ/fMUeuijVMZWix2qgl0Tqa6uoGQ1oMsDgexh1BX2UebLYHYU/RnUGo60KQwQxmEtuB0tvfm8Pq+BI52pOHGgmDIsLi5YlkqK9JjkNYKthZVkpuT7vmQ736H+HAnuSG7cMccIjaskHnNf4e8Xhho5YY4FzfEcazYZgn0OG247G76JIqjrUE0VvcTHJ9FVNpyei0xvL5pG7t7V3HFZ/4ZgFde+AVLYuu5LOcKAN4rqyIq0s58m2c46H53MGbY926Xy0XDkA/yU01MMl6DTwGD1xjtSSA1NYHYcIMlOBxjnbwhB5SaapoUAtjpDkHR77ZS1B3P34vj2BN2PY7wEOwOB6tDazknroSUMBdz7Q2sT+knKdIBLTU4Oir49JxWLC3eStLButwe6AqOpBMrLksixGaDPZZdhfW8V5dG2lnX0m1J4tm/PM78qGYuu8rzAf9G6WZ6GkpZujqXRUGrAOhw7MGN5Yzei8bGdsK6N5OW6hlq4lQTk4CnyejQ2a0sIa2U7t+BmNE/8AefAtJSO3CGl+uTgJp1NCkEsLEMQeFyGz4oaeTPO5robk9nQUglV4Rs5RuZJWQFVZAc2okMK+we6I4BWxaEptHqzCC/PoigxFy6LInkbdtHhzWNRefewJHCg4RWvcytH13ha5JasmcTea3JnBO0FoB+19T9CaWmJvhGkhzLxCSNVUdJHCgkY17qsQ/4rQcIC+akRT+D1zHdNfokoGYdTQqT7EwHnBtxCApXHxWHt3Lw4Ls46reTbSvkobgKbPGeiTlcBFHbHUFFSxBHI9cQmrSM4qo+Sizn0uSIIjfKTW5mLg6Hg799+DdKe0JJXeDth9CejD0qFrecXkcuf3M6B3yD0gEcPnyYpqom1pyVdMIHfGSoaNGPUqPQpDDJho9eOp7JY4JMNykDe8gOfon4Q4dwHD5MhjjJADojYuiLXE2N9SLKK+upkrmEpa7kja0feotxVrJo7mrKHUdotKdSW1XMQV/ntGK6971M1qKFpDm6qCivoa0pnMSoOb5ru93O4yaKr6+vx7hOv8XORBs+W1Xhjvd5p/wIa7KE9LQk6stLMI39tDXncqxMTCl1KpoUpsBpTyDjaIPGLWQ0P8E3oreR0XQUKy6cERYKehfwRMs1VLsWUl5n5fPXXceGc84hLy+P6iN/oTMonDA5efn90FY4zo704+aLPdTUfty+3W2tbD7Y65sofk9JJW1h/h+yfPhsVc7wctzBTaSn5XpbLBk6B868c5pSs40mhQk0UlFRQUEBi+NGL/zu7+9n3873ierLJ7x7ByEd24mXSkQMicbKXsciXm35BHv6V3C4NZZIm42LL7qIUCDEsocTKgxGMbxtfdmhPWMe83/oRPHBYfljnk92sg2drcp011Df0uOXOJSaSTQpjGI8dQEjzTV8eM92InKSYMg02+Luh7q3oO4tBg6/yFl9h7CIwWlsFHSk8lT3tXzQt5787mzCxc3G5QtZHB9Ox6ZnsAbF+M4zfPC67vp6QlLmjRjb8Lb1aQ7ruMb8P535ZJVS048mhVGMty5geFFRQVE5ghs6iqFtL0t6txJZ/gMoc4DYcNmXstN1Je86zufPR3LoddtJsHWSHGZnqaOIkLBI5iesGvFawwevC6+uZGlE3KgDsg1tW79o8fxxj/l/OvPJKqWmF00KJ3FGk8n3NUDLLj4S9zqpwdWwxzNnss2SQl3kDTgyPs7jRzN5YlcTnQ7DnDDDVXObiR0ow+7optq+ltqmUxftHF8/EDO+WL2cTiftTfVYQiJOq3hJKTVzaFKYKC4HUb27iOl/BXb+D/RUAhAfFEmlYyELVl1No305v/7AwftH0qjY7MJubWZNUhAXRpfypYvTKS1p4P2SHtoc/rmF6uoGkkwpWaGMu3hJKTW9aVI4Ez3VUPM3qHkN6t5kqbMLN1YIz8WVcAnlXQk8vKmC+pBMGhpXs7c1AjdCPK1cHudkcVgPjdVlRLnbKC3poqKiAuMKxul0H1cpPLwX7vDlifxWnxgf4WuNpFNKKjX7aFI4HcYJDR94kkDNa9C2z7M+bC5kfpaizvm0dzlYvWwlT2yt4tH9hiO2jbjdQYQ3t7OMPdgbDkDLUQZCocBmp7bZQUd6DN0hUVQUlhCdPI/q6u7jB1wb3gt32HKgf6sf3qfAny2WlFInp0nhVAY6oGU3C/veJqb8P+BoJ4gNEs6DVfdD6pUQvQyn27D5tS1sO1rGrj1Cc28GIZYB0l1lZFNOdlgj1pBwynqtRCZm+yp8Q6s9PWyT0rNora/2XXb4gGvDe+EOXQ70b/XaYkmp6UOTwnDGQNt+UlsfIabnddhaAbiJIpwjzhWQejXtYetxWSJwtBq6nTG8XbSPNw/V09YzgI1oNiR2siKuhAjp5EBRLZGhMtbuBDOWtlhSanrQpADQUwP1b3t+6v4OPZVkAF2WNMj4FMSt5c8vlVJYVMSiFa2U9O+kuC+Bor45DNBCmE2YH9KD+/AWPjKvkcVhaVQU7kWSR+4zoJRSgWp2JgVHG9S/4+lAVv82dBzyrLfHQdLFsPyH7KpNZKClkA2Zi6nsgBeb3ZTFbuCFes+4QOHSw3zXYTJNGZ/bMJeqynIe6dyJ3ZJ9QlGQUkpNF7MyKQyU/5WgHZ/HJaF0hKyiI+6rNNlW0h20EHtQCDRCQeGx4Sn6XVBkWUScq4qPZllZHNuHq+EQJbvyyUyLZ07kXHoihDlxUX6+M6WUOjMBlxRE5Arg14AV+KMx5r6JvsaehhTqGj9BYuZZGLGBA158ZRNhwa9z2YXrgeOHp1gQA/8w8DhNHe1clHY9AIXN9ZTUtOCw2gktaaKisITevr6JDlUppaZUQCUFEbECvwUuA6qAHSLykjHm4ERex2WNJiFrPeuHDUcREYKvB3NBUfmQuMDGiX0AQsKjiIxLOFZcVKGjciqlpreASgrAOqDUGHMEQESeBK4DJjQpwLGxjAYdLq8nLNiQt7twxOXK2maqGxvZ+v4WAOrLS6mrbsE4HaMui7cfQpgdokI56fJ4jhlcbm7p4UhJKc6uBsR2aNTl8RwzdBmgtraBjjab730YvjyeY4Yvn+q9Hc8x+vvQ38dM+X10tjSSHB1B5tlMCjHGTM6Zx0FEbgSuMMZ80bv8OWC9MearQ/a5Hbjdu5gDFE15oKObAzT5O4hT0BgnxnSIEaZHnBrjxDidGOcZYxJG2hBoTwqnZIx5EHjQ33GMRER2GmPW+juOk9EYJ8Z0iBGmR5wa48SYqBhPPkXX1KsG5g5ZTveuU0opNQUCLSnsALJFJEtE7MBNwEt+jkkppWaNgCo+MsY4ReSrwCY8TVL/ZIw54OewTkdAFmsNozFOjOkQI0yPODXGiTEhMQZURbNSSin/CrTiI6WUUn6kSUEppZSPJoVxEJG5IvKOiBwUkQMi8nXv+jgReVNESrz/xvo5zhAR2S4ie71x/si7PktEtolIqYg85a3U92ecVhHJF5FXAjE+b0xlIrJfRPaIyE7vukD7fceIyLMiUigih0TknECKUURyvO/f4E+HiHwjkGL0xnmX9/9LgYg84f1/FFB/kyLydW98B0TkG951E/I+alIYHyfwLWPMUmADcIeILAXuBt4yxmQDb3mX/akfuMQYsxJYBVwhIhuAnwG/NMYsBFqB2/wXIgBfBw4NWQ60+AZdbIxZNaQteKD9vn8NvG6MWQysxPOeBkyMxpgi7/u3CjgL6AFeCKQYRSQNuBNYa4zJxdPg5SYC6G9SRHKBL+EZAWIlcI2ILGSi3kdjjP6c4Q/wIp7xmoqAFO+6FKDI37ENiTEM2A2sx9Pr0eZdfw6wyY9xpXv/gC8BXgEkkOIbEmcZMGfYuoD5fQPRwFG8jUcCMcZhcV0ObAm0GIE0oBKIw9M68xXgo4H0Nwl8EnhoyPK/At+dqPdRnxTOkIhkAquBbUCSMabWu6kOSPJXXIO8RTN7gAbgTeAw0GaMcXp3qcLzH8FffoXnD3pw0uZ4Aiu+QQZ4Q0R2eYdagcD6fWcBjcCfvUVxfxSRcAIrxqFuAp7wvg6YGI0x1cDPgQqgFmgHdhFYf5MFwPkiEi8iYcBVeDr9Tsj7qEnhDIhIBPAc8A1jTMfQbcaTrv3e3tcY4zKex/V0PI+bi09+xNQRkWuABmPMLn/HMgbnGWPWAFfiKS68YOjGAPh924A1wAPGmNVAN8OKDwIgRgC85fHXAs8M3+bvGL3l8NfhSbKpQDhwhb/iGYkx5hCe4qw3gNeBPXD8MM5n8j5qUhgnEQnCkxAeM8Y8711dLyIp3u0peL6dBwRjTBvwDp5H3xgRGey46M+hRDYC14pIGfAkniKkXxM48fl4v0FijGnAUw6+jsD6fVcBVcaYbd7lZ/EkiUCKcdCVwG5jTL13OZBi/Ahw1BjTaIwZAJ7H83caUH+TxpiHjDFnGWMuwFPHUcwEvY+aFMZBRAR4CDhkjPl/Qza9BNzifX0LnroGvxGRBBGJ8b4OxVPvcQhPcrjRu5vf4jTG3GOMSTfGZOIpTnjbGPPZQIlvkIiEi0jk4Gs85eEFBNDv2xhTB1SKSI531aV4hpwPmBiHuJljRUcQWDFWABtEJMz7/3zwfQy0v8lE778ZwMeBx5mo99FflSXT+Qc4D8+j2T48j2578JTrxeOpNC0B/g7E+TnOFUC+N84C4N+86+cD24FSPI/wwQHwnl4EvBKI8Xnj2ev9OQD8i3d9oP2+VwE7vb/vvwKxARhjONAMRA9ZF2gx/ggo9P6f+T8gOAD/Jj/Ak6z2ApdO5Puow1wopZTy0eIjpZRSPpoUlFJK+WhSUEop5aNJQSmllI8mBaWUUj6aFJRSSvloUlBKKeWjSUGpcRKRv3oHyDswOEieiNwmIsXeeSz+V0R+412fICLPicgO789G/0av1Mi085pS4yQiccaYFu8QIjvwDLG8Bc+YQ53A28BeY8xXReRx4HfGmM3eoQk2GWOW+C14pUZhO/UuSqlR3CkiN3hfzwU+B7xnjGkBEJFngEXe7R8BlnqG0wEgSkQijDFdUxmwUqeiSUGpcRCRi/B80J9jjOkRkXfxjJcz2rd/C7DBGNM3JQEqNU5ap6DU+EQDrd6EsBjPtKzhwIUiEusdZvkTQ/Z/A/ja4IKIrJrKYJUaK00KSo3P64BNRA4B9wF5eMbY/yme0TS34JnCs927/53AWhHZJyIHgS9PecRKjYFWNCs1gQbrCbxPCi8AfzLGvODvuJQaK31SUGpi3eudE7sAOIpnXgOlpg19UlBKKeWjTwpKKaV8NCkopZTy0aSglFLKR5OCUkopH00KSimlfP4/eG9i1R8IJqgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Generate n ages for a class\"\"\"\n",
    "print(\"Generating: \", 2010, \" ages for unit type: [0., 1., 0.]\")\n",
    "\n",
    "age_one_hot_labels = tf.repeat([[0., 1., 0.]],2010, axis=0)\n",
    "\n",
    "input_noise = tf.random.normal((2010, cgan.noise_dim), 0, 1)\n",
    "random_vector_labels = tf.concat([input_noise, age_one_hot_labels], axis=1)\n",
    "\n",
    "ages = cgan.generator(random_vector_labels)\n",
    "\n",
    "inv_gen_ages = [(val * (max_age_filtered-min_age_filtered)) + min_age_filtered for val in ages.numpy().flatten()]\n",
    "\n",
    "print(\"Generated Ages:\")\n",
    "print(\"min: \", np.min(inv_gen_ages))\n",
    "print(\"mean: \", np.mean(inv_gen_ages))\n",
    "print(\"max: \", np.max(inv_gen_ages))\n",
    "print(\"stdv: \", np.std(inv_gen_ages))\n",
    "\n",
    "df_ages_class = final_df.query(\"ethnicity == 'Caucasian'\")\n",
    "\n",
    "print(\"True Ages:\")\n",
    "print(\"min: \", np.min(df_ages_class.age))\n",
    "print(\"mean: \", np.mean(df_ages_class.age))\n",
    "print(\"max: \", np.max(df_ages_class.age))\n",
    "print(\"stdv: \", np.std(df_ages_class.age))\n",
    "\n",
    "\n",
    "sns.histplot(inv_gen_ages, bins=70, label='GAN', kde=True,)\n",
    "sns.histplot(df_ages_class.age, bins=70, color='orange', label='Truth', alpha=0.3, kde=True,)\n",
    "plt.title('Caucasian Ages')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "df_temp = pd.DataFrame(columns = ['age', 'ethnicity'])\n",
    "\n",
    "df_temp['age'] = inv_gen_ages\n",
    "df_temp['ethnicity'] = 'Caucasian'\n",
    "\n",
    "df_age_eth = df_age_eth.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "S1WvBkzMuC73",
    "outputId": "cdd165f8-af2a-4f5a-813e-f1b21a6b0d63"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethnicity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>African American</th>\n",
       "      <td>231.0</td>\n",
       "      <td>56.151515</td>\n",
       "      <td>16.861546</td>\n",
       "      <td>19.0</td>\n",
       "      <td>46.50</td>\n",
       "      <td>58.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caucasian</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>64.443781</td>\n",
       "      <td>17.419489</td>\n",
       "      <td>15.0</td>\n",
       "      <td>55.00</td>\n",
       "      <td>67.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Native American</th>\n",
       "      <td>12.0</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>20.331346</td>\n",
       "      <td>19.0</td>\n",
       "      <td>39.25</td>\n",
       "      <td>48.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     age                                                     \n",
       "                   count       mean        std   min    25%   50%   75%   max\n",
       "ethnicity                                                                    \n",
       "African American   231.0  56.151515  16.861546  19.0  46.50  58.0  69.0  90.0\n",
       "Caucasian         2010.0  64.443781  17.419489  15.0  55.00  67.0  78.0  89.0\n",
       "Native American     12.0  50.500000  20.331346  19.0  39.25  48.0  66.0  88.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ages.groupby('ethnicity').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "Z6odZ8N7uC73",
    "outputId": "dc8769b7-527f-4c12-fa25-51e05db3d3a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating:  12  ages for unit type: [0., 0., 1]\n",
      "Generated Ages:\n",
      "min:  32.07838091254234\n",
      "mean:  56.794130059580006\n",
      "max:  88.15487504005432\n",
      "stdv:  18.61921223570077\n",
      "True Ages:\n",
      "min:  19\n",
      "mean:  50.5\n",
      "max:  88\n",
      "stdv:  19.465781943365815\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Generate n ages for a class\"\"\"\n",
    "print(\"Generating: \", 12, \" ages for unit type: [0., 0., 1]\")\n",
    "\n",
    "age_one_hot_labels = tf.repeat([[0., 0., 1.]],12, axis=0)\n",
    "\n",
    "input_noise = tf.random.normal((12, cgan.noise_dim), 0, 1)\n",
    "random_vector_labels = tf.concat([input_noise, age_one_hot_labels], axis=1)\n",
    "\n",
    "ages = cgan.generator(random_vector_labels)\n",
    "\n",
    "inv_gen_ages = [(val * (max_age_filtered-min_age_filtered)) + min_age_filtered for val in ages.numpy().flatten()]\n",
    "\n",
    "print(\"Generated Ages:\")\n",
    "print(\"min: \", np.min(inv_gen_ages))\n",
    "print(\"mean: \", np.mean(inv_gen_ages))\n",
    "print(\"max: \", np.max(inv_gen_ages))\n",
    "print(\"stdv: \", np.std(inv_gen_ages))\n",
    "\n",
    "df_ages_class = final_df.query(\"ethnicity == 'Native American'\")\n",
    "\n",
    "print(\"True Ages:\")\n",
    "print(\"min: \", np.min(df_ages_class.age))\n",
    "print(\"mean: \", np.mean(df_ages_class.age))\n",
    "print(\"max: \", np.max(df_ages_class.age))\n",
    "print(\"stdv: \", np.std(df_ages_class.age))\n",
    "\n",
    "\n",
    "sns.histplot(inv_gen_ages, bins=70, label='GAN', kde=True,)\n",
    "sns.histplot(df_ages_class.age, bins=70, color='orange', label='Truth', alpha=0.3, kde=True,)\n",
    "plt.title('Native American Ages')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "df_temp = pd.DataFrame(columns = ['age', 'ethnicity'])\n",
    "\n",
    "df_temp['age'] = inv_gen_ages\n",
    "df_temp['ethnicity'] = 'Native American'\n",
    "\n",
    "df_age_eth = df_age_eth.append(df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQZFsKT8Azeb"
   },
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "o_RCa2NLuC73"
   },
   "outputs": [],
   "source": [
    "df_age_eth['data'] = 'GAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ORAQsVqANUe",
    "outputId": "a6a6ef0b-9083-4734-a4fa-41e9e633d012"
   },
   "outputs": [],
   "source": [
    "#!pip install table_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1MPPPuqfBtr-"
   },
   "outputs": [],
   "source": [
    "#https://pypi.org/project/table-evaluator/\n",
    "from table_evaluator import load_data, TableEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QV_bsxVsLrZ7",
    "outputId": "c08e8855-6c9c-4b0b-fc16-76dbd8af43fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>African American</th>\n",
       "      <th>Caucasian</th>\n",
       "      <th>Native American</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  ethnicity  African American  Caucasian  Native American\n",
       "0   87  Caucasian               0.0        1.0              0.0\n",
       "1   87  Caucasian               0.0        1.0              0.0\n",
       "2   76  Caucasian               0.0        1.0              0.0\n",
       "3   34  Caucasian               0.0        1.0              0.0\n",
       "4   61  Caucasian               0.0        1.0              0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "FLYDOPL0OfjT"
   },
   "outputs": [],
   "source": [
    "final_df['data'] = 'Truth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9nCeC_lDCXVD"
   },
   "outputs": [],
   "source": [
    "df_true = final_df[['age', 'ethnicity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "A2WD47BeCf4X"
   },
   "outputs": [],
   "source": [
    "table_evaluator = TableEvaluator(df_true, df_age_eth[['age', 'ethnicity']], cat_cols=['ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_rJ_8-0Dzbo",
    "outputId": "0fdc65de-92b4-40fc-f13a-4fee6bbe98a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dianam/.local/lib/python3.8/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier F1-scores and their Jaccard similarities::\n",
      "                             f1_real  f1_fake  jaccard_similarity\n",
      "index                                                            \n",
      "DecisionTreeClassifier_fake   0.8780   0.7938              0.7686\n",
      "DecisionTreeClassifier_real   0.8958   0.7738              0.7346\n",
      "LogisticRegression_fake       0.8847   0.8847              1.0000\n",
      "LogisticRegression_real       0.9047   0.9047              1.0000\n",
      "MLPClassifier_fake            0.8847   0.8847              1.0000\n",
      "MLPClassifier_real            0.9047   0.9047              1.0000\n",
      "RandomForestClassifier_fake   0.8780   0.7916              0.7721\n",
      "RandomForestClassifier_real   0.8980   0.7583              0.7019\n",
      "\n",
      "Privacy results:\n",
      "                                            result\n",
      "Duplicate rows between sets (real/fake)  (2232, 0)\n",
      "nearest neighbor mean                       0.0051\n",
      "nearest neighbor std                        0.0222\n",
      "\n",
      "Miscellaneous results:\n",
      "                                  Result\n",
      "Column Correlation Distance RMSE  0.0778\n",
      "Column Correlation distance MAE   0.0550\n",
      "\n",
      "Results:\n",
      "                                                result\n",
      "Basic statistics                                1.0000\n",
      "Correlation column correlations                    NaN\n",
      "Mean Correlation between fake and real columns  0.9986\n",
      "1 - MAPE Estimator results                      0.9392\n",
      "Similarity Score                                   NaN\n"
     ]
    }
   ],
   "source": [
    "table_evaluator.evaluate(target_col='ethnicity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHeuWOl2WK3Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "kR4ckeSWGmsI"
   },
   "outputs": [],
   "source": [
    "df_true.append(df_age_eth[['age', 'ethnicity']]).to_csv('age_eth_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcvPuMDXQD1r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MU4X0gtSDre7",
    "outputId": "1682fbda-5267-4834-d492-8cb33493b3a1"
   },
   "outputs": [],
   "source": [
    "#table_evaluator.visual_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qinl6TVMXvco"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_YE-sHpX932"
   },
   "source": [
    "**Distribute**\n",
    "https://www.tensorflow.org/tutorials/distribute/custom_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3R_tLpGX_SS",
    "outputId": "fcf1e70f-f0d5-4c8c-9f12-02c8faf0f59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "8HRYGzS9YrfI"
   },
   "outputs": [],
   "source": [
    "# Create a checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrswI7AVX_og",
    "outputId": "818555a7-79b7-40ed-e022-c2be86dc3916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator input dim:  53\n",
      "Dicrimination input dim:  4\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CommunicationImplementation.AUTO\n",
      "Number of devices: 2\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,545\n",
      "Trainable params: 4,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CommunicationImplementation.AUTO\n",
      "Number of devices: 2\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 53)]              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                3456      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 7,937\n",
      "Trainable params: 7,809\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#with mirrored_strategy.scope():\n",
    "dist_gan = ConditionalGAN(noise_dim=50,\n",
    "                 data_shape=1,\n",
    "                 num_classes=3, \n",
    "                 d_learning_rate=1e-6, \n",
    "                 g_learning_rate=1e-6, \n",
    "                 batch_size=32, \n",
    "                 start_epoch=0,\n",
    "                 verbose = True, distribute = True)\n",
    "\n",
    "  #loss_obj = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "  #loss = tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)\n",
    "\n",
    "dist_gan.compile(loss_fn=keras.losses.BinaryCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dnE59ztTX_og",
    "outputId": "fdff160f-1a91-4d0a-938f-02855df680b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "age_one_hot_labels1: Tensor(\"strided_slice_1:0\", shape=(None, 1, 3), dtype=float32)\n",
      "age_one_hot_labels2: Tensor(\"strided_slice_2:0\", shape=(None,), dtype=float32)\n",
      "age_one_hot_labels3: Tensor(\"strided_slice_3:0\", shape=(None, 3), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dianam/.local/lib/python3.8/site-packages/keras/backend.py:4993: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_one_hot_labels1: Tensor(\"strided_slice_1:0\", shape=(None, 1, 3), dtype=float32)\n",
      "age_one_hot_labels2: Tensor(\"strided_slice_2:0\", shape=(None,), dtype=float32)\n",
      "age_one_hot_labels3: Tensor(\"strided_slice_3:0\", shape=(None, 3), dtype=float32)\n",
      "36/36 [==============================] - 1s 3ms/step - g_loss: 0.6944 - d_loss: 0.6981\n",
      "Epoch 2/10000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6944 - d_loss: 0.6979\n",
      "Epoch 3/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6941 - d_loss: 0.6981\n",
      "Epoch 4/10000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6939 - d_loss: 0.6981\n",
      "Epoch 5/10000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6928 - d_loss: 0.6979\n",
      "Epoch 6/10000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6931 - d_loss: 0.6979\n",
      "Epoch 7/10000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6926 - d_loss: 0.6978\n",
      "Epoch 8/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6922 - d_loss: 0.6979\n",
      "Epoch 9/10000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6917 - d_loss: 0.6978\n",
      "Epoch 10/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6914 - d_loss: 0.6977\n",
      "Epoch 11/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6915 - d_loss: 0.6976\n",
      "Epoch 12/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6910 - d_loss: 0.6976\n",
      "Epoch 13/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6909 - d_loss: 0.6976\n",
      "Epoch 14/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6907 - d_loss: 0.6973\n",
      "Epoch 15/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6903 - d_loss: 0.6973\n",
      "Epoch 16/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6904 - d_loss: 0.6973\n",
      "Epoch 17/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6903 - d_loss: 0.6972\n",
      "Epoch 18/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6898 - d_loss: 0.6972\n",
      "Epoch 19/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6896 - d_loss: 0.6970\n",
      "Epoch 20/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6895 - d_loss: 0.6971\n",
      "Epoch 21/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6894 - d_loss: 0.6969\n",
      "Epoch 22/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6890 - d_loss: 0.6967\n",
      "Epoch 23/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6891 - d_loss: 0.6967\n",
      "Epoch 24/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6889 - d_loss: 0.6967\n",
      "Epoch 25/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6886 - d_loss: 0.6966\n",
      "Epoch 26/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6885 - d_loss: 0.6965\n",
      "Epoch 27/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6883 - d_loss: 0.6964\n",
      "Epoch 28/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6883 - d_loss: 0.6964\n",
      "Epoch 29/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6883 - d_loss: 0.6963\n",
      "Epoch 30/10000\n",
      "36/36 [==============================] - 0s 3ms/step - g_loss: 0.6881 - d_loss: 0.6962\n",
      "Epoch 31/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6878 - d_loss: 0.6960\n",
      "Epoch 32/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6879 - d_loss: 0.6960\n",
      "Epoch 33/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6877 - d_loss: 0.6958\n",
      "Epoch 34/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6877 - d_loss: 0.6958\n",
      "Epoch 35/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6877 - d_loss: 0.6958\n",
      "Epoch 36/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6876 - d_loss: 0.6956\n",
      "Epoch 37/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6876 - d_loss: 0.6955\n",
      "Epoch 38/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6875 - d_loss: 0.6953\n",
      "Epoch 39/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6874 - d_loss: 0.6954\n",
      "Epoch 40/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6875 - d_loss: 0.6952\n",
      "Epoch 41/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6874 - d_loss: 0.6951\n",
      "Epoch 42/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6873 - d_loss: 0.6950\n",
      "Epoch 43/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6873 - d_loss: 0.6949\n",
      "Epoch 44/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6873 - d_loss: 0.6948\n",
      "Epoch 45/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6873 - d_loss: 0.6947\n",
      "Epoch 46/10000\n",
      "36/36 [==============================] - 0s 4ms/step - g_loss: 0.6874 - d_loss: 0.6946\n",
      "Epoch 47/10000\n",
      "36/36 [==============================] - 0s 5ms/step - g_loss: 0.6873 - d_loss: 0.6945\n",
      "Epoch 48/10000\n",
      "12/36 [=========>....................] - ETA: 0s - g_loss: 0.6881 - d_loss: 0.6944"
     ]
    }
   ],
   "source": [
    "dist_gan.fit(dataset, epochs=10000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIGzgl7VX_og"
   },
   "outputs": [],
   "source": [
    "dist_age_eth = pd.DataFrame(\n",
    "    columns = ['age', 'ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65I9E3fmX_og"
   },
   "outputs": [],
   "source": [
    "dist_age_eth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uf8k3MAKX_og"
   },
   "outputs": [],
   "source": [
    "dist_age_eth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S441_DRCX_og"
   },
   "outputs": [],
   "source": [
    "\"\"\"Generate n ages for a class\"\"\"\n",
    "print(\"Generating: \", 231, \" ages for unit type: [1., 0., 0.]\")\n",
    "\n",
    "age_one_hot_labels = tf.repeat([[1., 0., 0.]],231, axis=0)\n",
    "\n",
    "input_noise = tf.random.normal((231, dist_gan.noise_dim), 0, 1)\n",
    "random_vector_labels = tf.concat([input_noise, age_one_hot_labels], axis=1)\n",
    "\n",
    "ages = dist_gan.generator(random_vector_labels)\n",
    "\n",
    "inv_gen_ages = [(val * (max_age_filtered-min_age_filtered)) + min_age_filtered for val in ages.numpy().flatten()]\n",
    "\n",
    "print(\"Generated Ages:\")\n",
    "print(\"min: \", np.min(inv_gen_ages))\n",
    "print(\"mean: \", np.mean(inv_gen_ages))\n",
    "print(\"max: \", np.max(inv_gen_ages))\n",
    "print(\"stdv: \", np.std(inv_gen_ages))\n",
    "\n",
    "df_ages_class = final_df.query(\"ethnicity == 'African American'\")\n",
    "\n",
    "print(\"True Ages:\")\n",
    "print(\"min: \", np.min(df_ages_class.age))\n",
    "print(\"mean: \", np.mean(df_ages_class.age))\n",
    "print(\"max: \", np.max(df_ages_class.age))\n",
    "print(\"stdv: \", np.std(df_ages_class.age))\n",
    "\n",
    "\n",
    "sns.histplot(inv_gen_ages, bins=70, label='GAN', kde=True,)\n",
    "sns.histplot(df_ages_class.age, bins=70, color='orange', label='Truth', alpha=0.3, kde=True,)\n",
    "plt.title('African American Ages')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "df_temp = pd.DataFrame(columns = ['age', 'ethnicity'])\n",
    "\n",
    "df_temp['age'] = inv_gen_ages\n",
    "df_temp['ethnicity'] = 'African American'\n",
    "\n",
    "dist_age_eth = dist_age_eth.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wt0QFiQrX_og"
   },
   "outputs": [],
   "source": [
    "\"\"\"Generate n ages for a class\"\"\"\n",
    "print(\"Generating: \", 2010, \" ages for unit type: [0., 1., 0.]\")\n",
    "\n",
    "age_one_hot_labels = tf.repeat([[0., 1., 0.]],2010, axis=0)\n",
    "\n",
    "input_noise = tf.random.normal((2010, dist_gan.noise_dim), 0, 1)\n",
    "random_vector_labels = tf.concat([input_noise, age_one_hot_labels], axis=1)\n",
    "\n",
    "ages = dist_gan.generator(random_vector_labels)\n",
    "\n",
    "inv_gen_ages = [(val * (max_age_filtered-min_age_filtered)) + min_age_filtered for val in ages.numpy().flatten()]\n",
    "\n",
    "print(\"Generated Ages:\")\n",
    "print(\"min: \", np.min(inv_gen_ages))\n",
    "print(\"mean: \", np.mean(inv_gen_ages))\n",
    "print(\"max: \", np.max(inv_gen_ages))\n",
    "print(\"stdv: \", np.std(inv_gen_ages))\n",
    "\n",
    "df_ages_class = final_df.query(\"ethnicity == 'Caucasian'\")\n",
    "\n",
    "print(\"True Ages:\")\n",
    "print(\"min: \", np.min(df_ages_class.age))\n",
    "print(\"mean: \", np.mean(df_ages_class.age))\n",
    "print(\"max: \", np.max(df_ages_class.age))\n",
    "print(\"stdv: \", np.std(df_ages_class.age))\n",
    "\n",
    "\n",
    "sns.histplot(inv_gen_ages, bins=70, label='GAN', kde=True,)\n",
    "sns.histplot(df_ages_class.age, bins=70, color='orange', label='Truth', alpha=0.3, kde=True,)\n",
    "plt.title('Caucasian Ages')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "df_temp = pd.DataFrame(columns = ['age', 'ethnicity'])\n",
    "\n",
    "df_temp['age'] = inv_gen_ages\n",
    "df_temp['ethnicity'] = 'Caucasian'\n",
    "\n",
    "dist_age_eth = dist_age_eth.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZTj1k1zX_og"
   },
   "outputs": [],
   "source": [
    "df_ages.groupby('ethnicity').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmpSbhAeX_og"
   },
   "outputs": [],
   "source": [
    "\"\"\"Generate n ages for a class\"\"\"\n",
    "print(\"Generating: \", 12, \" ages for unit type: [0., 0., 1]\")\n",
    "\n",
    "age_one_hot_labels = tf.repeat([[0., 0., 1.]],12, axis=0)\n",
    "\n",
    "input_noise = tf.random.normal((12, dist_gan.noise_dim), 0, 1)\n",
    "random_vector_labels = tf.concat([input_noise, age_one_hot_labels], axis=1)\n",
    "\n",
    "ages = dist_gan.generator(random_vector_labels)\n",
    "\n",
    "inv_gen_ages = [(val * (max_age_filtered-min_age_filtered)) + min_age_filtered for val in ages.numpy().flatten()]\n",
    "\n",
    "print(\"Generated Ages:\")\n",
    "print(\"min: \", np.min(inv_gen_ages))\n",
    "print(\"mean: \", np.mean(inv_gen_ages))\n",
    "print(\"max: \", np.max(inv_gen_ages))\n",
    "print(\"stdv: \", np.std(inv_gen_ages))\n",
    "\n",
    "df_ages_class = final_df.query(\"ethnicity == 'Native American'\")\n",
    "\n",
    "print(\"True Ages:\")\n",
    "print(\"min: \", np.min(df_ages_class.age))\n",
    "print(\"mean: \", np.mean(df_ages_class.age))\n",
    "print(\"max: \", np.max(df_ages_class.age))\n",
    "print(\"stdv: \", np.std(df_ages_class.age))\n",
    "\n",
    "\n",
    "sns.histplot(inv_gen_ages, bins=70, label='GAN', kde=True,)\n",
    "sns.histplot(df_ages_class.age, bins=70, color='orange', label='Truth', alpha=0.3, kde=True,)\n",
    "plt.title('Native American Ages')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "df_temp = pd.DataFrame(columns = ['age', 'ethnicity'])\n",
    "\n",
    "df_temp['age'] = inv_gen_ages\n",
    "df_temp['ethnicity'] = 'Native American'\n",
    "\n",
    "dist_age_eth = dist_age_eth.append(df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_my3XYUTX_og"
   },
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVARAPJWX_og"
   },
   "outputs": [],
   "source": [
    "dist_age_eth['data'] = 'GAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtjQKBkjX_og"
   },
   "outputs": [],
   "source": [
    "!pip install table_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnBPATxiX_og"
   },
   "outputs": [],
   "source": [
    "#https://pypi.org/project/table-evaluator/\n",
    "from table_evaluator import load_data, TableEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrXR-R5qX_og"
   },
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Zo_XwhtX_oh"
   },
   "outputs": [],
   "source": [
    "final_df['data'] = 'Truth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1CiCtFnX_oh"
   },
   "outputs": [],
   "source": [
    "df_true = final_df[['age', 'ethnicity']].reset_index().drop('index', axis = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_age_eth.reset_index(level = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_age_eth[['age', 'ethnicity']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PslivqmPX_oh"
   },
   "outputs": [],
   "source": [
    "table_evaluator = TableEvaluator(df_true, dist_age_eth[['age', 'ethnicity']], cat_cols=['ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_rMRfKMX_oh"
   },
   "outputs": [],
   "source": [
    "table_evaluator.evaluate(target_col='ethnicity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OG8rqpLX_oh"
   },
   "outputs": [],
   "source": [
    "dist_age_eth[['age', 'ethnicity']].reset_index().drop('index', axis = True).index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqflnNEIX_oh"
   },
   "outputs": [],
   "source": [
    "df_true.append(dist_age_eth[['age', 'ethnicity']]).to_csv('dist_age_eth_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGytAGNOX_oh"
   },
   "outputs": [],
   "source": [
    "#dist_age_eth[['age', 'ethnicity']].reset_index(level = 0, inplace = True)#.drop('index', axis = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_age_eth[['age', 'ethnicity']][dist_age_eth[['age', 'ethnicity']].index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true[df_true.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_age_eth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03o0aJCWX_oh"
   },
   "outputs": [],
   "source": [
    "#table_evaluator.visual_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "age_ethnicity_gan_DF.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "34b722ad9db4b3bab2fc22ea7ad4e4dafa3a71a0a38143cc4f1aba484bf1fe4b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
